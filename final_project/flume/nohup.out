Warning: No configuration directory set! Use --conf <dir> to override.
Info: Including Hadoop libraries found via (/usr/bin/hadoop) for HDFS access
Info: Excluding /usr/lib/hadoop/lib/slf4j-api-1.7.5.jar from classpath
Info: Excluding /usr/lib/hadoop/lib/slf4j-log4j12.jar from classpath
Info: Including HBASE libraries found via (/usr/bin/hbase) for HBASE access
Info: Excluding /usr/lib/hbase/bin/../lib/slf4j-api-1.7.5.jar from classpath
Info: Excluding /usr/lib/hbase/bin/../lib/slf4j-log4j12.jar from classpath
Info: Excluding /usr/lib/hadoop/lib/slf4j-api-1.7.5.jar from classpath
Info: Excluding /usr/lib/hadoop/lib/slf4j-log4j12.jar from classpath
Info: Excluding /usr/lib/hadoop/lib/slf4j-api-1.7.5.jar from classpath
Info: Excluding /usr/lib/hadoop/lib/slf4j-log4j12.jar from classpath
Info: Excluding /usr/lib/zookeeper/lib/slf4j-api-1.7.5.jar from classpath
Info: Excluding /usr/lib/zookeeper/lib/slf4j-log4j12-1.7.5.jar from classpath
Info: Excluding /usr/lib/zookeeper/lib/slf4j-log4j12.jar from classpath
Info: Including Hive libraries found via () for Hive access
+ exec /usr/java/jdk1.7.0_67-cloudera/bin/java -Xmx20m -cp '/usr/lib/flume-ng/lib/*:flume-sources-1.0-SNAPSHOT.jar:/etc/hadoop/conf:/usr/lib/hadoop/lib/activation-1.1.jar:/usr/lib/hadoop/lib/apacheds-i18n-2.0.0-M15.jar:/usr/lib/hadoop/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/lib/hadoop/lib/api-asn1-api-1.0.0-M20.jar:/usr/lib/hadoop/lib/api-util-1.0.0-M20.jar:/usr/lib/hadoop/lib/asm-3.2.jar:/usr/lib/hadoop/lib/avro.jar:/usr/lib/hadoop/lib/aws-java-sdk-core-1.10.6.jar:/usr/lib/hadoop/lib/aws-java-sdk-kms-1.10.6.jar:/usr/lib/hadoop/lib/aws-java-sdk-s3-1.10.6.jar:/usr/lib/hadoop/lib/commons-beanutils-1.7.0.jar:/usr/lib/hadoop/lib/commons-beanutils-core-1.8.0.jar:/usr/lib/hadoop/lib/commons-cli-1.2.jar:/usr/lib/hadoop/lib/commons-codec-1.4.jar:/usr/lib/hadoop/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop/lib/commons-compress-1.4.1.jar:/usr/lib/hadoop/lib/commons-configuration-1.6.jar:/usr/lib/hadoop/lib/commons-digester-1.8.jar:/usr/lib/hadoop/lib/commons-el-1.0.jar:/usr/lib/hadoop/lib/commons-httpclient-3.1.jar:/usr/lib/hadoop/lib/commons-io-2.4.jar:/usr/lib/hadoop/lib/commons-lang-2.6.jar:/usr/lib/hadoop/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop/lib/commons-net-3.1.jar:/usr/lib/hadoop/lib/curator-client-2.7.1.jar:/usr/lib/hadoop/lib/curator-framework-2.7.1.jar:/usr/lib/hadoop/lib/curator-recipes-2.7.1.jar:/usr/lib/hadoop/lib/gson-2.2.4.jar:/usr/lib/hadoop/lib/guava-11.0.2.jar:/usr/lib/hadoop/lib/hamcrest-core-1.3.jar:/usr/lib/hadoop/lib/htrace-core4-4.0.1-incubating.jar:/usr/lib/hadoop/lib/httpclient-4.2.5.jar:/usr/lib/hadoop/lib/httpcore-4.2.5.jar:/usr/lib/hadoop/lib/hue-plugins-3.9.0-cdh5.8.0.jar:/usr/lib/hadoop/lib/jackson-jaxrs-1.8.8.jar:/usr/lib/hadoop/lib/jackson-xc-1.8.8.jar:/usr/lib/hadoop/lib/jasper-compiler-5.5.23.jar:/usr/lib/hadoop/lib/jasper-runtime-5.5.23.jar:/usr/lib/hadoop/lib/java-xmlbuilder-0.4.jar:/usr/lib/hadoop/lib/jaxb-api-2.2.2.jar:/usr/lib/hadoop/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop/lib/jersey-core-1.9.jar:/usr/lib/hadoop/lib/jersey-json-1.9.jar:/usr/lib/hadoop/lib/jersey-server-1.9.jar:/usr/lib/hadoop/lib/jets3t-0.9.0.jar:/usr/lib/hadoop/lib/jettison-1.1.jar:/usr/lib/hadoop/lib/jetty-6.1.26.cloudera.4.jar:/usr/lib/hadoop/lib/jetty-util-6.1.26.cloudera.4.jar:/usr/lib/hadoop/lib/jsch-0.1.42.jar:/usr/lib/hadoop/lib/jsp-api-2.1.jar:/usr/lib/hadoop/lib/jsr305-3.0.0.jar:/usr/lib/hadoop/lib/junit-4.11.jar:/usr/lib/hadoop/lib/log4j-1.2.17.jar:/usr/lib/hadoop/lib/logredactor-1.0.3.jar:/usr/lib/hadoop/lib/mockito-all-1.8.5.jar:/usr/lib/hadoop/lib/native:/usr/lib/hadoop/lib/netty-3.6.2.Final.jar:/usr/lib/hadoop/lib/paranamer-2.3.jar:/usr/lib/hadoop/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop/lib/servlet-api-2.5.jar:/usr/lib/hadoop/lib/snappy-java-1.0.4.1.jar:/usr/lib/hadoop/lib/stax-api-1.0-2.jar:/usr/lib/hadoop/lib/xmlenc-0.52.jar:/usr/lib/hadoop/lib/xz-1.0.jar:/usr/lib/hadoop/lib/zookeeper.jar:/usr/lib/hadoop/.//bin:/usr/lib/hadoop/.//client:/usr/lib/hadoop/.//client-0.20:/usr/lib/hadoop/.//cloudera:/usr/lib/hadoop/.//etc:/usr/lib/hadoop/.//hadoop-annotations-2.6.0-cdh5.8.0.jar:/usr/lib/hadoop/.//hadoop-annotations.jar:/usr/lib/hadoop/.//hadoop-auth-2.6.0-cdh5.8.0.jar:/usr/lib/hadoop/.//hadoop-auth.jar:/usr/lib/hadoop/.//hadoop-aws-2.6.0-cdh5.8.0.jar:/usr/lib/hadoop/.//hadoop-aws.jar:/usr/lib/hadoop/.//hadoop-common-2.6.0-cdh5.8.0.jar:/usr/lib/hadoop/.//hadoop-common-2.6.0-cdh5.8.0-tests.jar:/usr/lib/hadoop/.//hadoop-common.jar:/usr/lib/hadoop/.//hadoop-common-tests.jar:/usr/lib/hadoop/.//hadoop-nfs-2.6.0-cdh5.8.0.jar:/usr/lib/hadoop/.//hadoop-nfs.jar:/usr/lib/hadoop/.//lib:/usr/lib/hadoop/.//libexec:/usr/lib/hadoop/.//LICENSE.txt:/usr/lib/hadoop/.//NOTICE.txt:/usr/lib/hadoop/.//parquet-avro.jar:/usr/lib/hadoop/.//parquet-cascading.jar:/usr/lib/hadoop/.//parquet-column.jar:/usr/lib/hadoop/.//parquet-common.jar:/usr/lib/hadoop/.//parquet-encoding.jar:/usr/lib/hadoop/.//parquet-format.jar:/usr/lib/hadoop/.//parquet-format-javadoc.jar:/usr/lib/hadoop/.//parquet-format-sources.jar:/usr/lib/hadoop/.//parquet-generator.jar:/usr/lib/hadoop/.//parquet-hadoop-bundle.jar:/usr/lib/hadoop/.//parquet-hadoop.jar:/usr/lib/hadoop/.//parquet-jackson.jar:/usr/lib/hadoop/.//parquet-pig-bundle.jar:/usr/lib/hadoop/.//parquet-pig.jar:/usr/lib/hadoop/.//parquet-protobuf.jar:/usr/lib/hadoop/.//parquet-scala_2.10.jar:/usr/lib/hadoop/.//parquet-scrooge_2.10.jar:/usr/lib/hadoop/.//parquet-test-hadoop2.jar:/usr/lib/hadoop/.//parquet-thrift.jar:/usr/lib/hadoop/.//parquet-tools.jar:/usr/lib/hadoop/.//sbin:/usr/lib/hadoop-hdfs/./:/usr/lib/hadoop-hdfs/lib/asm-3.2.jar:/usr/lib/hadoop-hdfs/lib/commons-cli-1.2.jar:/usr/lib/hadoop-hdfs/lib/commons-codec-1.4.jar:/usr/lib/hadoop-hdfs/lib/commons-daemon-1.0.13.jar:/usr/lib/hadoop-hdfs/lib/commons-el-1.0.jar:/usr/lib/hadoop-hdfs/lib/commons-io-2.4.jar:/usr/lib/hadoop-hdfs/lib/commons-lang-2.6.jar:/usr/lib/hadoop-hdfs/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-hdfs/lib/guava-11.0.2.jar:/usr/lib/hadoop-hdfs/lib/htrace-core4-4.0.1-incubating.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-asl-1.8.8.jar:/usr/lib/hadoop-hdfs/lib/jackson-mapper-asl-1.8.8.jar:/usr/lib/hadoop-hdfs/lib/jasper-runtime-5.5.23.jar:/usr/lib/hadoop-hdfs/lib/jersey-core-1.9.jar:/usr/lib/hadoop-hdfs/lib/jersey-server-1.9.jar:/usr/lib/hadoop-hdfs/lib/jetty-6.1.26.cloudera.4.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-6.1.26.cloudera.4.jar:/usr/lib/hadoop-hdfs/lib/jsp-api-2.1.jar:/usr/lib/hadoop-hdfs/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-hdfs/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-hdfs/lib/log4j-1.2.17.jar:/usr/lib/hadoop-hdfs/lib/netty-3.6.2.Final.jar:/usr/lib/hadoop-hdfs/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-hdfs/lib/servlet-api-2.5.jar:/usr/lib/hadoop-hdfs/lib/xercesImpl-2.9.1.jar:/usr/lib/hadoop-hdfs/lib/xml-apis-1.3.04.jar:/usr/lib/hadoop-hdfs/lib/xmlenc-0.52.jar:/usr/lib/hadoop-hdfs/.//bin:/usr/lib/hadoop-hdfs/.//cloudera:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-2.6.0-cdh5.8.0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-2.6.0-cdh5.8.0-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs-2.6.0-cdh5.8.0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-tests.jar:/usr/lib/hadoop-hdfs/.//lib:/usr/lib/hadoop-hdfs/.//LICENSE.txt:/usr/lib/hadoop-hdfs/.//NOTICE.txt:/usr/lib/hadoop-hdfs/.//sbin:/usr/lib/hadoop-hdfs/.//webapps:/usr/lib/hadoop-yarn/lib/activation-1.1.jar:/usr/lib/hadoop-yarn/lib/aopalliance-1.0.jar:/usr/lib/hadoop-yarn/lib/asm-3.2.jar:/usr/lib/hadoop-yarn/lib/commons-cli-1.2.jar:/usr/lib/hadoop-yarn/lib/commons-codec-1.4.jar:/usr/lib/hadoop-yarn/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop-yarn/lib/commons-compress-1.4.1.jar:/usr/lib/hadoop-yarn/lib/commons-io-2.4.jar:/usr/lib/hadoop-yarn/lib/commons-lang-2.6.jar:/usr/lib/hadoop-yarn/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-yarn/lib/guava-11.0.2.jar:/usr/lib/hadoop-yarn/lib/guice-3.0.jar:/usr/lib/hadoop-yarn/lib/guice-servlet-3.0.jar:/usr/lib/hadoop-yarn/lib/jackson-core-asl-1.8.8.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-1.8.8.jar:/usr/lib/hadoop-yarn/lib/jackson-mapper-asl-1.8.8.jar:/usr/lib/hadoop-yarn/lib/jackson-xc-1.8.8.jar:/usr/lib/hadoop-yarn/lib/javax.inject-1.jar:/usr/lib/hadoop-yarn/lib/jaxb-api-2.2.2.jar:/usr/lib/hadoop-yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-yarn/lib/jersey-client-1.9.jar:/usr/lib/hadoop-yarn/lib/jersey-core-1.9.jar:/usr/lib/hadoop-yarn/lib/jersey-guice-1.9.jar:/usr/lib/hadoop-yarn/lib/jersey-json-1.9.jar:/usr/lib/hadoop-yarn/lib/jersey-server-1.9.jar:/usr/lib/hadoop-yarn/lib/jettison-1.1.jar:/usr/lib/hadoop-yarn/lib/jetty-6.1.26.cloudera.4.jar:/usr/lib/hadoop-yarn/lib/jetty-util-6.1.26.cloudera.4.jar:/usr/lib/hadoop-yarn/lib/jline-2.11.jar:/usr/lib/hadoop-yarn/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-yarn/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-yarn/lib/log4j-1.2.17.jar:/usr/lib/hadoop-yarn/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-yarn/lib/servlet-api-2.5.jar:/usr/lib/hadoop-yarn/lib/spark-1.6.0-cdh5.8.0-yarn-shuffle.jar:/usr/lib/hadoop-yarn/lib/spark-yarn-shuffle.jar:/usr/lib/hadoop-yarn/lib/stax-api-1.0-2.jar:/usr/lib/hadoop-yarn/lib/xz-1.0.jar:/usr/lib/hadoop-yarn/lib/zookeeper.jar:/usr/lib/hadoop-yarn/.//bin:/usr/lib/hadoop-yarn/.//cloudera:/usr/lib/hadoop-yarn/.//etc:/usr/lib/hadoop-yarn/.//hadoop-yarn-api-2.6.0-cdh5.8.0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell-2.6.0-cdh5.8.0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher-2.6.0-cdh5.8.0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client-2.6.0-cdh5.8.0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common-2.6.0-cdh5.8.0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry-2.6.0-cdh5.8.0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice-2.6.0-cdh5.8.0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common-2.6.0-cdh5.8.0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager-2.6.0-cdh5.8.0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager-2.6.0-cdh5.8.0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests-2.6.0-cdh5.8.0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy-2.6.0-cdh5.8.0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-yarn/.//lib:/usr/lib/hadoop-yarn/.//LICENSE.txt:/usr/lib/hadoop-yarn/.//NOTICE.txt:/usr/lib/hadoop-yarn/.//sbin:/usr/lib/hadoop-mapreduce/lib/aopalliance-1.0.jar:/usr/lib/hadoop-mapreduce/lib/asm-3.2.jar:/usr/lib/hadoop-mapreduce/lib/avro.jar:/usr/lib/hadoop-mapreduce/lib/commons-compress-1.4.1.jar:/usr/lib/hadoop-mapreduce/lib/commons-io-2.4.jar:/usr/lib/hadoop-mapreduce/lib/guice-3.0.jar:/usr/lib/hadoop-mapreduce/lib/guice-servlet-3.0.jar:/usr/lib/hadoop-mapreduce/lib/hamcrest-core-1.3.jar:/usr/lib/hadoop-mapreduce/lib/jackson-core-asl-1.8.8.jar:/usr/lib/hadoop-mapreduce/lib/jackson-mapper-asl-1.8.8.jar:/usr/lib/hadoop-mapreduce/lib/javax.inject-1.jar:/usr/lib/hadoop-mapreduce/lib/jersey-core-1.9.jar:/usr/lib/hadoop-mapreduce/lib/jersey-guice-1.9.jar:/usr/lib/hadoop-mapreduce/lib/jersey-server-1.9.jar:/usr/lib/hadoop-mapreduce/lib/junit-4.11.jar:/usr/lib/hadoop-mapreduce/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-mapreduce/lib/log4j-1.2.17.jar:/usr/lib/hadoop-mapreduce/lib/netty-3.6.2.Final.jar:/usr/lib/hadoop-mapreduce/lib/paranamer-2.3.jar:/usr/lib/hadoop-mapreduce/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/lib/hadoop-mapreduce/lib/xz-1.0.jar:/usr/lib/hadoop-mapreduce/.//activation-1.1.jar:/usr/lib/hadoop-mapreduce/.//apacheds-i18n-2.0.0-M15.jar:/usr/lib/hadoop-mapreduce/.//apacheds-kerberos-codec-2.0.0-M15.jar:/usr/lib/hadoop-mapreduce/.//api-asn1-api-1.0.0-M20.jar:/usr/lib/hadoop-mapreduce/.//api-util-1.0.0-M20.jar:/usr/lib/hadoop-mapreduce/.//asm-3.2.jar:/usr/lib/hadoop-mapreduce/.//avro.jar:/usr/lib/hadoop-mapreduce/.//bin:/usr/lib/hadoop-mapreduce/.//cloudera:/usr/lib/hadoop-mapreduce/.//commons-beanutils-1.7.0.jar:/usr/lib/hadoop-mapreduce/.//commons-beanutils-core-1.8.0.jar:/usr/lib/hadoop-mapreduce/.//commons-cli-1.2.jar:/usr/lib/hadoop-mapreduce/.//commons-codec-1.4.jar:/usr/lib/hadoop-mapreduce/.//commons-collections-3.2.2.jar:/usr/lib/hadoop-mapreduce/.//commons-compress-1.4.1.jar:/usr/lib/hadoop-mapreduce/.//commons-configuration-1.6.jar:/usr/lib/hadoop-mapreduce/.//commons-digester-1.8.jar:/usr/lib/hadoop-mapreduce/.//commons-el-1.0.jar:/usr/lib/hadoop-mapreduce/.//commons-httpclient-3.1.jar:/usr/lib/hadoop-mapreduce/.//commons-io-2.4.jar:/usr/lib/hadoop-mapreduce/.//commons-lang-2.6.jar:/usr/lib/hadoop-mapreduce/.//commons-logging-1.1.3.jar:/usr/lib/hadoop-mapreduce/.//commons-math3-3.1.1.jar:/usr/lib/hadoop-mapreduce/.//commons-net-3.1.jar:/usr/lib/hadoop-mapreduce/.//curator-client-2.7.1.jar:/usr/lib/hadoop-mapreduce/.//curator-framework-2.7.1.jar:/usr/lib/hadoop-mapreduce/.//curator-recipes-2.7.1.jar:/usr/lib/hadoop-mapreduce/.//gson-2.2.4.jar:/usr/lib/hadoop-mapreduce/.//guava-11.0.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-ant-2.6.0-cdh5.8.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-ant.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs-2.6.0-cdh5.8.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives-2.6.0-cdh5.8.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives.jar:/usr/lib/hadoop-mapreduce/.//hadoop-auth-2.6.0-cdh5.8.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-auth.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-2.6.0-cdh5.8.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin-2.6.0-cdh5.8.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp-2.6.0-cdh5.8.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras-2.6.0-cdh5.8.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix-2.6.0-cdh5.8.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app-2.6.0-cdh5.8.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common-2.6.0-cdh5.8.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core-2.6.0-cdh5.8.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-2.6.0-cdh5.8.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins-2.6.0-cdh5.8.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-2.6.0-cdh5.8.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-2.6.0-cdh5.8.0-tests.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-tests.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask-2.6.0-cdh5.8.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle-2.6.0-cdh5.8.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples-2.6.0-cdh5.8.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack-2.6.0-cdh5.8.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen-2.6.0-cdh5.8.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls-2.6.0-cdh5.8.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming-2.6.0-cdh5.8.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming.jar:/usr/lib/hadoop-mapreduce/.//hamcrest-core-1.3.jar:/usr/lib/hadoop-mapreduce/.//htrace-core4-4.0.1-incubating.jar:/usr/lib/hadoop-mapreduce/.//httpclient-4.2.5.jar:/usr/lib/hadoop-mapreduce/.//httpcore-4.2.5.jar:/usr/lib/hadoop-mapreduce/.//jackson-annotations-2.2.3.jar:/usr/lib/hadoop-mapreduce/.//jackson-core-2.2.3.jar:/usr/lib/hadoop-mapreduce/.//jackson-core-asl-1.8.8.jar:/usr/lib/hadoop-mapreduce/.//jackson-databind-2.2.3.jar:/usr/lib/hadoop-mapreduce/.//jackson-jaxrs-1.8.8.jar:/usr/lib/hadoop-mapreduce/.//jackson-mapper-asl-1.8.8.jar:/usr/lib/hadoop-mapreduce/.//jackson-xc-1.8.8.jar:/usr/lib/hadoop-mapreduce/.//jasper-compiler-5.5.23.jar:/usr/lib/hadoop-mapreduce/.//jasper-runtime-5.5.23.jar:/usr/lib/hadoop-mapreduce/.//java-xmlbuilder-0.4.jar:/usr/lib/hadoop-mapreduce/.//jaxb-api-2.2.2.jar:/usr/lib/hadoop-mapreduce/.//jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-mapreduce/.//jersey-core-1.9.jar:/usr/lib/hadoop-mapreduce/.//jersey-json-1.9.jar:/usr/lib/hadoop-mapreduce/.//jersey-server-1.9.jar:/usr/lib/hadoop-mapreduce/.//jets3t-0.9.0.jar:/usr/lib/hadoop-mapreduce/.//jettison-1.1.jar:/usr/lib/hadoop-mapreduce/.//jetty-6.1.26.cloudera.4.jar:/usr/lib/hadoop-mapreduce/.//jetty-util-6.1.26.cloudera.4.jar:/usr/lib/hadoop-mapreduce/.//jsch-0.1.42.jar:/usr/lib/hadoop-mapreduce/.//jsp-api-2.1.jar:/usr/lib/hadoop-mapreduce/.//jsr305-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//junit-4.11.jar:/usr/lib/hadoop-mapreduce/.//lib:/usr/lib/hadoop-mapreduce/.//LICENSE.txt:/usr/lib/hadoop-mapreduce/.//log4j-1.2.17.jar:/usr/lib/hadoop-mapreduce/.//logs:/usr/lib/hadoop-mapreduce/.//metrics-core-3.0.2.jar:/usr/lib/hadoop-mapreduce/.//microsoft-windowsazure-storage-sdk-0.6.0.jar:/usr/lib/hadoop-mapreduce/.//mockito-all-1.8.5.jar:/usr/lib/hadoop-mapreduce/.//NOTICE.txt:/usr/lib/hadoop-mapreduce/.//paranamer-2.3.jar:/usr/lib/hadoop-mapreduce/.//protobuf-java-2.5.0.jar:/usr/lib/hadoop-mapreduce/.//sbin:/usr/lib/hadoop-mapreduce/.//servlet-api-2.5.jar:/usr/lib/hadoop-mapreduce/.//snappy-java-1.0.4.1.jar:/usr/lib/hadoop-mapreduce/.//stax-api-1.0-2.jar:/usr/lib/hadoop-mapreduce/.//xmlenc-0.52.jar:/usr/lib/hadoop-mapreduce/.//xz-1.0.jar:/usr/lib/hadoop-mapreduce/.//zookeeper.jar:/usr/lib/hbase/bin/../conf:/usr/java/jdk1.7.0_67-cloudera/lib/tools.jar:/usr/lib/hbase/bin/..:/usr/lib/hbase/bin/../lib/activation-1.1.jar:/usr/lib/hbase/bin/../lib/antisamy-1.4.3.jar:/usr/lib/hbase/bin/../lib/apacheds-i18n-2.0.0-M15.jar:/usr/lib/hbase/bin/../lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/lib/hbase/bin/../lib/api-asn1-api-1.0.0-M20.jar:/usr/lib/hbase/bin/../lib/api-util-1.0.0-M20.jar:/usr/lib/hbase/bin/../lib/asm-3.2.jar:/usr/lib/hbase/bin/../lib/avro.jar:/usr/lib/hbase/bin/../lib/aws-java-sdk-core-1.10.6.jar:/usr/lib/hbase/bin/../lib/aws-java-sdk-kms-1.10.6.jar:/usr/lib/hbase/bin/../lib/aws-java-sdk-s3-1.10.6.jar:/usr/lib/hbase/bin/../lib/batik-css-1.7.jar:/usr/lib/hbase/bin/../lib/batik-ext-1.7.jar:/usr/lib/hbase/bin/../lib/batik-util-1.7.jar:/usr/lib/hbase/bin/../lib/bsh-core-2.0b4.jar:/usr/lib/hbase/bin/../lib/commons-beanutils-1.7.0.jar:/usr/lib/hbase/bin/../lib/commons-beanutils-core-1.7.0.jar:/usr/lib/hbase/bin/../lib/commons-cli-1.2.jar:/usr/lib/hbase/bin/../lib/commons-codec-1.9.jar:/usr/lib/hbase/bin/../lib/commons-collections-3.2.2.jar:/usr/lib/hbase/bin/../lib/commons-compress-1.4.1.jar:/usr/lib/hbase/bin/../lib/commons-configuration-1.6.jar:/usr/lib/hbase/bin/../lib/commons-daemon-1.0.3.jar:/usr/lib/hbase/bin/../lib/commons-digester-1.8.jar:/usr/lib/hbase/bin/../lib/commons-el-1.0.jar:/usr/lib/hbase/bin/../lib/commons-fileupload-1.2.jar:/usr/lib/hbase/bin/../lib/commons-httpclient-3.1.jar:/usr/lib/hbase/bin/../lib/commons-io-2.4.jar:/usr/lib/hbase/bin/../lib/commons-lang-2.6.jar:/usr/lib/hbase/bin/../lib/commons-logging-1.2.jar:/usr/lib/hbase/bin/../lib/commons-math-2.1.jar:/usr/lib/hbase/bin/../lib/commons-math3-3.1.1.jar:/usr/lib/hbase/bin/../lib/commons-net-3.1.jar:/usr/lib/hbase/bin/../lib/core-3.1.1.jar:/usr/lib/hbase/bin/../lib/curator-client-2.7.1.jar:/usr/lib/hbase/bin/../lib/curator-framework-2.7.1.jar:/usr/lib/hbase/bin/../lib/curator-recipes-2.7.1.jar:/usr/lib/hbase/bin/../lib/disruptor-3.3.0.jar:/usr/lib/hbase/bin/../lib/esapi-2.1.0.jar:/usr/lib/hbase/bin/../lib/findbugs-annotations-1.3.9-1.jar:/usr/lib/hbase/bin/../lib/gson-2.2.4.jar:/usr/lib/hbase/bin/../lib/guava-12.0.1.jar:/usr/lib/hbase/bin/../lib/hamcrest-core-1.3.jar:/usr/lib/hbase/bin/../lib/hbase-annotations-1.2.0-cdh5.8.0.jar:/usr/lib/hbase/bin/../lib/hbase-annotations-1.2.0-cdh5.8.0-tests.jar:/usr/lib/hbase/bin/../lib/hbase-client-1.2.0-cdh5.8.0.jar:/usr/lib/hbase/bin/../lib/hbase-common-1.2.0-cdh5.8.0.jar:/usr/lib/hbase/bin/../lib/hbase-common-1.2.0-cdh5.8.0-tests.jar:/usr/lib/hbase/bin/../lib/hbase-examples-1.2.0-cdh5.8.0.jar:/usr/lib/hbase/bin/../lib/hbase-external-blockcache-1.2.0-cdh5.8.0.jar:/usr/lib/hbase/bin/../lib/hbase-hadoop2-compat-1.2.0-cdh5.8.0.jar:/usr/lib/hbase/bin/../lib/hbase-hadoop2-compat-1.2.0-cdh5.8.0-tests.jar:/usr/lib/hbase/bin/../lib/hbase-hadoop-compat-1.2.0-cdh5.8.0.jar:/usr/lib/hbase/bin/../lib/hbase-hadoop-compat-1.2.0-cdh5.8.0-tests.jar:/usr/lib/hbase/bin/../lib/hbase-it-1.2.0-cdh5.8.0.jar:/usr/lib/hbase/bin/../lib/hbase-it-1.2.0-cdh5.8.0-tests.jar:/usr/lib/hbase/bin/../lib/hbase-prefix-tree-1.2.0-cdh5.8.0.jar:/usr/lib/hbase/bin/../lib/hbase-procedure-1.2.0-cdh5.8.0.jar:/usr/lib/hbase/bin/../lib/hbase-protocol-1.2.0-cdh5.8.0.jar:/usr/lib/hbase/bin/../lib/hbase-resource-bundle-1.2.0-cdh5.8.0.jar:/usr/lib/hbase/bin/../lib/hbase-rest-1.2.0-cdh5.8.0.jar:/usr/lib/hbase/bin/../lib/hbase-server-1.2.0-cdh5.8.0.jar:/usr/lib/hbase/bin/../lib/hbase-server-1.2.0-cdh5.8.0-tests.jar:/usr/lib/hbase/bin/../lib/hbase-shell-1.2.0-cdh5.8.0.jar:/usr/lib/hbase/bin/../lib/hbase-spark-1.2.0-cdh5.8.0.jar:/usr/lib/hbase/bin/../lib/hbase-thrift-1.2.0-cdh5.8.0.jar:/usr/lib/hbase/bin/../lib/high-scale-lib-1.1.1.jar:/usr/lib/hbase/bin/../lib/hsqldb-1.8.0.10.jar:/usr/lib/hbase/bin/../lib/htrace-core-3.2.0-incubating.jar:/usr/lib/hbase/bin/../lib/htrace-core4-4.0.1-incubating.jar:/usr/lib/hbase/bin/../lib/htrace-core.jar:/usr/lib/hbase/bin/../lib/httpclient-4.2.5.jar:/usr/lib/hbase/bin/../lib/httpcore-4.2.5.jar:/usr/lib/hbase/bin/../lib/jackson-annotations-2.2.3.jar:/usr/lib/hbase/bin/../lib/jackson-core-2.2.3.jar:/usr/lib/hbase/bin/../lib/jackson-core-asl-1.8.8.jar:/usr/lib/hbase/bin/../lib/jackson-databind-2.2.3.jar:/usr/lib/hbase/bin/../lib/jackson-jaxrs-1.8.8.jar:/usr/lib/hbase/bin/../lib/jackson-mapper-asl-1.8.8.jar:/usr/lib/hbase/bin/../lib/jackson-xc-1.8.8.jar:/usr/lib/hbase/bin/../lib/jamon-runtime-2.4.1.jar:/usr/lib/hbase/bin/../lib/jasper-compiler-5.5.23.jar:/usr/lib/hbase/bin/../lib/jasper-runtime-5.5.23.jar:/usr/lib/hbase/bin/../lib/java-xmlbuilder-0.4.jar:/usr/lib/hbase/bin/../lib/jaxb-api-2.1.jar:/usr/lib/hbase/bin/../lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hbase/bin/../lib/jcodings-1.0.8.jar:/usr/lib/hbase/bin/../lib/jersey-client-1.9.jar:/usr/lib/hbase/bin/../lib/jersey-core-1.9.jar:/usr/lib/hbase/bin/../lib/jersey-json-1.9.jar:/usr/lib/hbase/bin/../lib/jersey-server-1.9.jar:/usr/lib/hbase/bin/../lib/jets3t-0.9.0.jar:/usr/lib/hbase/bin/../lib/jettison-1.3.3.jar:/usr/lib/hbase/bin/../lib/jetty-6.1.26.cloudera.4.jar:/usr/lib/hbase/bin/../lib/jetty-sslengine-6.1.26.cloudera.4.jar:/usr/lib/hbase/bin/../lib/jetty-util-6.1.26.cloudera.4.jar:/usr/lib/hbase/bin/../lib/joni-2.1.2.jar:/usr/lib/hbase/bin/../lib/jruby-cloudera-1.0.0.jar:/usr/lib/hbase/bin/../lib/jsch-0.1.42.jar:/usr/lib/hbase/bin/../lib/jsp-2.1-6.1.14.jar:/usr/lib/hbase/bin/../lib/jsp-api-2.1-6.1.14.jar:/usr/lib/hbase/bin/../lib/jsp-api-2.1.jar:/usr/lib/hbase/bin/../lib/jsr305-1.3.9.jar:/usr/lib/hbase/bin/../lib/junit-4.12.jar:/usr/lib/hbase/bin/../lib/leveldbjni-all-1.8.jar:/usr/lib/hbase/bin/../lib/libthrift-0.9.3.jar:/usr/lib/hbase/bin/../lib/log4j-1.2.17.jar:/usr/lib/hbase/bin/../lib/metrics-core-2.2.0.jar:/usr/lib/hbase/bin/../lib/nekohtml-1.9.12.jar:/usr/lib/hbase/bin/../lib/netty-all-4.0.23.Final.jar:/usr/lib/hbase/bin/../lib/paranamer-2.3.jar:/usr/lib/hbase/bin/../lib/protobuf-java-2.5.0.jar:/usr/lib/hbase/bin/../lib/servlet-api-2.5-6.1.14.jar:/usr/lib/hbase/bin/../lib/servlet-api-2.5.jar:/usr/lib/hbase/bin/../lib/snappy-java-1.0.4.1.jar:/usr/lib/hbase/bin/../lib/spymemcached-2.11.6.jar:/usr/lib/hbase/bin/../lib/xalan-2.7.0.jar:/usr/lib/hbase/bin/../lib/xercesImpl-2.9.1.jar:/usr/lib/hbase/bin/../lib/xml-apis-1.3.03.jar:/usr/lib/hbase/bin/../lib/xml-apis-ext-1.3.04.jar:/usr/lib/hbase/bin/../lib/xmlenc-0.52.jar:/usr/lib/hbase/bin/../lib/xom-1.2.5.jar:/usr/lib/hbase/bin/../lib/xz-1.0.jar:/usr/lib/hbase/bin/../lib/zookeeper.jar:/etc/hadoop/conf:/usr/lib/hadoop/lib/activation-1.1.jar:/usr/lib/hadoop/lib/apacheds-i18n-2.0.0-M15.jar:/usr/lib/hadoop/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/lib/hadoop/lib/api-asn1-api-1.0.0-M20.jar:/usr/lib/hadoop/lib/api-util-1.0.0-M20.jar:/usr/lib/hadoop/lib/asm-3.2.jar:/usr/lib/hadoop/lib/avro.jar:/usr/lib/hadoop/lib/aws-java-sdk-core-1.10.6.jar:/usr/lib/hadoop/lib/aws-java-sdk-kms-1.10.6.jar:/usr/lib/hadoop/lib/aws-java-sdk-s3-1.10.6.jar:/usr/lib/hadoop/lib/commons-beanutils-1.7.0.jar:/usr/lib/hadoop/lib/commons-beanutils-core-1.8.0.jar:/usr/lib/hadoop/lib/commons-cli-1.2.jar:/usr/lib/hadoop/lib/commons-codec-1.4.jar:/usr/lib/hadoop/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop/lib/commons-compress-1.4.1.jar:/usr/lib/hadoop/lib/commons-configuration-1.6.jar:/usr/lib/hadoop/lib/commons-digester-1.8.jar:/usr/lib/hadoop/lib/commons-el-1.0.jar:/usr/lib/hadoop/lib/commons-httpclient-3.1.jar:/usr/lib/hadoop/lib/commons-io-2.4.jar:/usr/lib/hadoop/lib/commons-lang-2.6.jar:/usr/lib/hadoop/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop/lib/commons-net-3.1.jar:/usr/lib/hadoop/lib/curator-client-2.7.1.jar:/usr/lib/hadoop/lib/curator-framework-2.7.1.jar:/usr/lib/hadoop/lib/curator-recipes-2.7.1.jar:/usr/lib/hadoop/lib/gson-2.2.4.jar:/usr/lib/hadoop/lib/guava-11.0.2.jar:/usr/lib/hadoop/lib/hamcrest-core-1.3.jar:/usr/lib/hadoop/lib/htrace-core4-4.0.1-incubating.jar:/usr/lib/hadoop/lib/httpclient-4.2.5.jar:/usr/lib/hadoop/lib/httpcore-4.2.5.jar:/usr/lib/hadoop/lib/hue-plugins-3.9.0-cdh5.8.0.jar:/usr/lib/hadoop/lib/jackson-jaxrs-1.8.8.jar:/usr/lib/hadoop/lib/jackson-xc-1.8.8.jar:/usr/lib/hadoop/lib/jasper-compiler-5.5.23.jar:/usr/lib/hadoop/lib/jasper-runtime-5.5.23.jar:/usr/lib/hadoop/lib/java-xmlbuilder-0.4.jar:/usr/lib/hadoop/lib/jaxb-api-2.2.2.jar:/usr/lib/hadoop/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop/lib/jersey-core-1.9.jar:/usr/lib/hadoop/lib/jersey-json-1.9.jar:/usr/lib/hadoop/lib/jersey-server-1.9.jar:/usr/lib/hadoop/lib/jets3t-0.9.0.jar:/usr/lib/hadoop/lib/jettison-1.1.jar:/usr/lib/hadoop/lib/jetty-6.1.26.cloudera.4.jar:/usr/lib/hadoop/lib/jetty-util-6.1.26.cloudera.4.jar:/usr/lib/hadoop/lib/jsch-0.1.42.jar:/usr/lib/hadoop/lib/jsp-api-2.1.jar:/usr/lib/hadoop/lib/jsr305-3.0.0.jar:/usr/lib/hadoop/lib/junit-4.11.jar:/usr/lib/hadoop/lib/log4j-1.2.17.jar:/usr/lib/hadoop/lib/logredactor-1.0.3.jar:/usr/lib/hadoop/lib/mockito-all-1.8.5.jar:/usr/lib/hadoop/lib/native:/usr/lib/hadoop/lib/netty-3.6.2.Final.jar:/usr/lib/hadoop/lib/paranamer-2.3.jar:/usr/lib/hadoop/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop/lib/servlet-api-2.5.jar:/usr/lib/hadoop/lib/snappy-java-1.0.4.1.jar:/usr/lib/hadoop/lib/stax-api-1.0-2.jar:/usr/lib/hadoop/lib/xmlenc-0.52.jar:/usr/lib/hadoop/lib/xz-1.0.jar:/usr/lib/hadoop/lib/zookeeper.jar:/usr/lib/hadoop/.//bin:/usr/lib/hadoop/.//client:/usr/lib/hadoop/.//client-0.20:/usr/lib/hadoop/.//cloudera:/usr/lib/hadoop/.//etc:/usr/lib/hadoop/.//hadoop-annotations-2.6.0-cdh5.8.0.jar:/usr/lib/hadoop/.//hadoop-annotations.jar:/usr/lib/hadoop/.//hadoop-auth-2.6.0-cdh5.8.0.jar:/usr/lib/hadoop/.//hadoop-auth.jar:/usr/lib/hadoop/.//hadoop-aws-2.6.0-cdh5.8.0.jar:/usr/lib/hadoop/.//hadoop-aws.jar:/usr/lib/hadoop/.//hadoop-common-2.6.0-cdh5.8.0.jar:/usr/lib/hadoop/.//hadoop-common-2.6.0-cdh5.8.0-tests.jar:/usr/lib/hadoop/.//hadoop-common.jar:/usr/lib/hadoop/.//hadoop-common-tests.jar:/usr/lib/hadoop/.//hadoop-nfs-2.6.0-cdh5.8.0.jar:/usr/lib/hadoop/.//hadoop-nfs.jar:/usr/lib/hadoop/.//lib:/usr/lib/hadoop/.//libexec:/usr/lib/hadoop/.//LICENSE.txt:/usr/lib/hadoop/.//NOTICE.txt:/usr/lib/hadoop/.//parquet-avro.jar:/usr/lib/hadoop/.//parquet-cascading.jar:/usr/lib/hadoop/.//parquet-column.jar:/usr/lib/hadoop/.//parquet-common.jar:/usr/lib/hadoop/.//parquet-encoding.jar:/usr/lib/hadoop/.//parquet-format.jar:/usr/lib/hadoop/.//parquet-format-javadoc.jar:/usr/lib/hadoop/.//parquet-format-sources.jar:/usr/lib/hadoop/.//parquet-generator.jar:/usr/lib/hadoop/.//parquet-hadoop-bundle.jar:/usr/lib/hadoop/.//parquet-hadoop.jar:/usr/lib/hadoop/.//parquet-jackson.jar:/usr/lib/hadoop/.//parquet-pig-bundle.jar:/usr/lib/hadoop/.//parquet-pig.jar:/usr/lib/hadoop/.//parquet-protobuf.jar:/usr/lib/hadoop/.//parquet-scala_2.10.jar:/usr/lib/hadoop/.//parquet-scrooge_2.10.jar:/usr/lib/hadoop/.//parquet-test-hadoop2.jar:/usr/lib/hadoop/.//parquet-thrift.jar:/usr/lib/hadoop/.//parquet-tools.jar:/usr/lib/hadoop/.//sbin:/usr/lib/hadoop-hdfs/./:/usr/lib/hadoop-hdfs/lib/asm-3.2.jar:/usr/lib/hadoop-hdfs/lib/commons-cli-1.2.jar:/usr/lib/hadoop-hdfs/lib/commons-codec-1.4.jar:/usr/lib/hadoop-hdfs/lib/commons-daemon-1.0.13.jar:/usr/lib/hadoop-hdfs/lib/commons-el-1.0.jar:/usr/lib/hadoop-hdfs/lib/commons-io-2.4.jar:/usr/lib/hadoop-hdfs/lib/commons-lang-2.6.jar:/usr/lib/hadoop-hdfs/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-hdfs/lib/guava-11.0.2.jar:/usr/lib/hadoop-hdfs/lib/htrace-core4-4.0.1-incubating.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-asl-1.8.8.jar:/usr/lib/hadoop-hdfs/lib/jackson-mapper-asl-1.8.8.jar:/usr/lib/hadoop-hdfs/lib/jasper-runtime-5.5.23.jar:/usr/lib/hadoop-hdfs/lib/jersey-core-1.9.jar:/usr/lib/hadoop-hdfs/lib/jersey-server-1.9.jar:/usr/lib/hadoop-hdfs/lib/jetty-6.1.26.cloudera.4.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-6.1.26.cloudera.4.jar:/usr/lib/hadoop-hdfs/lib/jsp-api-2.1.jar:/usr/lib/hadoop-hdfs/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-hdfs/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-hdfs/lib/log4j-1.2.17.jar:/usr/lib/hadoop-hdfs/lib/netty-3.6.2.Final.jar:/usr/lib/hadoop-hdfs/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-hdfs/lib/servlet-api-2.5.jar:/usr/lib/hadoop-hdfs/lib/xercesImpl-2.9.1.jar:/usr/lib/hadoop-hdfs/lib/xml-apis-1.3.04.jar:/usr/lib/hadoop-hdfs/lib/xmlenc-0.52.jar:/usr/lib/hadoop-hdfs/.//bin:/usr/lib/hadoop-hdfs/.//cloudera:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-2.6.0-cdh5.8.0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-2.6.0-cdh5.8.0-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs-2.6.0-cdh5.8.0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-tests.jar:/usr/lib/hadoop-hdfs/.//lib:/usr/lib/hadoop-hdfs/.//LICENSE.txt:/usr/lib/hadoop-hdfs/.//NOTICE.txt:/usr/lib/hadoop-hdfs/.//sbin:/usr/lib/hadoop-hdfs/.//webapps:/usr/lib/hadoop-yarn/lib/activation-1.1.jar:/usr/lib/hadoop-yarn/lib/aopalliance-1.0.jar:/usr/lib/hadoop-yarn/lib/asm-3.2.jar:/usr/lib/hadoop-yarn/lib/commons-cli-1.2.jar:/usr/lib/hadoop-yarn/lib/commons-codec-1.4.jar:/usr/lib/hadoop-yarn/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop-yarn/lib/commons-compress-1.4.1.jar:/usr/lib/hadoop-yarn/lib/commons-io-2.4.jar:/usr/lib/hadoop-yarn/lib/commons-lang-2.6.jar:/usr/lib/hadoop-yarn/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-yarn/lib/guava-11.0.2.jar:/usr/lib/hadoop-yarn/lib/guice-3.0.jar:/usr/lib/hadoop-yarn/lib/guice-servlet-3.0.jar:/usr/lib/hadoop-yarn/lib/jackson-core-asl-1.8.8.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-1.8.8.jar:/usr/lib/hadoop-yarn/lib/jackson-mapper-asl-1.8.8.jar:/usr/lib/hadoop-yarn/lib/jackson-xc-1.8.8.jar:/usr/lib/hadoop-yarn/lib/javax.inject-1.jar:/usr/lib/hadoop-yarn/lib/jaxb-api-2.2.2.jar:/usr/lib/hadoop-yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-yarn/lib/jersey-client-1.9.jar:/usr/lib/hadoop-yarn/lib/jersey-core-1.9.jar:/usr/lib/hadoop-yarn/lib/jersey-guice-1.9.jar:/usr/lib/hadoop-yarn/lib/jersey-json-1.9.jar:/usr/lib/hadoop-yarn/lib/jersey-server-1.9.jar:/usr/lib/hadoop-yarn/lib/jettison-1.1.jar:/usr/lib/hadoop-yarn/lib/jetty-6.1.26.cloudera.4.jar:/usr/lib/hadoop-yarn/lib/jetty-util-6.1.26.cloudera.4.jar:/usr/lib/hadoop-yarn/lib/jline-2.11.jar:/usr/lib/hadoop-yarn/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-yarn/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-yarn/lib/log4j-1.2.17.jar:/usr/lib/hadoop-yarn/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-yarn/lib/servlet-api-2.5.jar:/usr/lib/hadoop-yarn/lib/spark-1.6.0-cdh5.8.0-yarn-shuffle.jar:/usr/lib/hadoop-yarn/lib/spark-yarn-shuffle.jar:/usr/lib/hadoop-yarn/lib/stax-api-1.0-2.jar:/usr/lib/hadoop-yarn/lib/xz-1.0.jar:/usr/lib/hadoop-yarn/lib/zookeeper.jar:/usr/lib/hadoop-yarn/.//bin:/usr/lib/hadoop-yarn/.//cloudera:/usr/lib/hadoop-yarn/.//etc:/usr/lib/hadoop-yarn/.//hadoop-yarn-api-2.6.0-cdh5.8.0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell-2.6.0-cdh5.8.0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher-2.6.0-cdh5.8.0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client-2.6.0-cdh5.8.0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common-2.6.0-cdh5.8.0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry-2.6.0-cdh5.8.0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice-2.6.0-cdh5.8.0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common-2.6.0-cdh5.8.0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager-2.6.0-cdh5.8.0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager-2.6.0-cdh5.8.0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests-2.6.0-cdh5.8.0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy-2.6.0-cdh5.8.0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-yarn/.//lib:/usr/lib/hadoop-yarn/.//LICENSE.txt:/usr/lib/hadoop-yarn/.//NOTICE.txt:/usr/lib/hadoop-yarn/.//sbin:/usr/lib/hadoop-mapreduce/lib/aopalliance-1.0.jar:/usr/lib/hadoop-mapreduce/lib/asm-3.2.jar:/usr/lib/hadoop-mapreduce/lib/avro.jar:/usr/lib/hadoop-mapreduce/lib/commons-compress-1.4.1.jar:/usr/lib/hadoop-mapreduce/lib/commons-io-2.4.jar:/usr/lib/hadoop-mapreduce/lib/guice-3.0.jar:/usr/lib/hadoop-mapreduce/lib/guice-servlet-3.0.jar:/usr/lib/hadoop-mapreduce/lib/hamcrest-core-1.3.jar:/usr/lib/hadoop-mapreduce/lib/jackson-core-asl-1.8.8.jar:/usr/lib/hadoop-mapreduce/lib/jackson-mapper-asl-1.8.8.jar:/usr/lib/hadoop-mapreduce/lib/javax.inject-1.jar:/usr/lib/hadoop-mapreduce/lib/jersey-core-1.9.jar:/usr/lib/hadoop-mapreduce/lib/jersey-guice-1.9.jar:/usr/lib/hadoop-mapreduce/lib/jersey-server-1.9.jar:/usr/lib/hadoop-mapreduce/lib/junit-4.11.jar:/usr/lib/hadoop-mapreduce/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-mapreduce/lib/log4j-1.2.17.jar:/usr/lib/hadoop-mapreduce/lib/netty-3.6.2.Final.jar:/usr/lib/hadoop-mapreduce/lib/paranamer-2.3.jar:/usr/lib/hadoop-mapreduce/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/lib/hadoop-mapreduce/lib/xz-1.0.jar:/usr/lib/hadoop-mapreduce/.//activation-1.1.jar:/usr/lib/hadoop-mapreduce/.//apacheds-i18n-2.0.0-M15.jar:/usr/lib/hadoop-mapreduce/.//apacheds-kerberos-codec-2.0.0-M15.jar:/usr/lib/hadoop-mapreduce/.//api-asn1-api-1.0.0-M20.jar:/usr/lib/hadoop-mapreduce/.//api-util-1.0.0-M20.jar:/usr/lib/hadoop-mapreduce/.//asm-3.2.jar:/usr/lib/hadoop-mapreduce/.//avro.jar:/usr/lib/hadoop-mapreduce/.//bin:/usr/lib/hadoop-mapreduce/.//cloudera:/usr/lib/hadoop-mapreduce/.//commons-beanutils-1.7.0.jar:/usr/lib/hadoop-mapreduce/.//commons-beanutils-core-1.8.0.jar:/usr/lib/hadoop-mapreduce/.//commons-cli-1.2.jar:/usr/lib/hadoop-mapreduce/.//commons-codec-1.4.jar:/usr/lib/hadoop-mapreduce/.//commons-collections-3.2.2.jar:/usr/lib/hadoop-mapreduce/.//commons-compress-1.4.1.jar:/usr/lib/hadoop-mapreduce/.//commons-configuration-1.6.jar:/usr/lib/hadoop-mapreduce/.//commons-digester-1.8.jar:/usr/lib/hadoop-mapreduce/.//commons-el-1.0.jar:/usr/lib/hadoop-mapreduce/.//commons-httpclient-3.1.jar:/usr/lib/hadoop-mapreduce/.//commons-io-2.4.jar:/usr/lib/hadoop-mapreduce/.//commons-lang-2.6.jar:/usr/lib/hadoop-mapreduce/.//commons-logging-1.1.3.jar:/usr/lib/hadoop-mapreduce/.//commons-math3-3.1.1.jar:/usr/lib/hadoop-mapreduce/.//commons-net-3.1.jar:/usr/lib/hadoop-mapreduce/.//curator-client-2.7.1.jar:/usr/lib/hadoop-mapreduce/.//curator-framework-2.7.1.jar:/usr/lib/hadoop-mapreduce/.//curator-recipes-2.7.1.jar:/usr/lib/hadoop-mapreduce/.//gson-2.2.4.jar:/usr/lib/hadoop-mapreduce/.//guava-11.0.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-ant-2.6.0-cdh5.8.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-ant.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs-2.6.0-cdh5.8.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives-2.6.0-cdh5.8.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives.jar:/usr/lib/hadoop-mapreduce/.//hadoop-auth-2.6.0-cdh5.8.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-auth.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-2.6.0-cdh5.8.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin-2.6.0-cdh5.8.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp-2.6.0-cdh5.8.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras-2.6.0-cdh5.8.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix-2.6.0-cdh5.8.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app-2.6.0-cdh5.8.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common-2.6.0-cdh5.8.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core-2.6.0-cdh5.8.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-2.6.0-cdh5.8.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins-2.6.0-cdh5.8.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-2.6.0-cdh5.8.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-2.6.0-cdh5.8.0-tests.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-tests.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask-2.6.0-cdh5.8.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle-2.6.0-cdh5.8.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples-2.6.0-cdh5.8.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack-2.6.0-cdh5.8.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen-2.6.0-cdh5.8.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls-2.6.0-cdh5.8.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming-2.6.0-cdh5.8.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming.jar:/usr/lib/hadoop-mapreduce/.//hamcrest-core-1.3.jar:/usr/lib/hadoop-mapreduce/.//htrace-core4-4.0.1-incubating.jar:/usr/lib/hadoop-mapreduce/.//httpclient-4.2.5.jar:/usr/lib/hadoop-mapreduce/.//httpcore-4.2.5.jar:/usr/lib/hadoop-mapreduce/.//jackson-annotations-2.2.3.jar:/usr/lib/hadoop-mapreduce/.//jackson-core-2.2.3.jar:/usr/lib/hadoop-mapreduce/.//jackson-core-asl-1.8.8.jar:/usr/lib/hadoop-mapreduce/.//jackson-databind-2.2.3.jar:/usr/lib/hadoop-mapreduce/.//jackson-jaxrs-1.8.8.jar:/usr/lib/hadoop-mapreduce/.//jackson-mapper-asl-1.8.8.jar:/usr/lib/hadoop-mapreduce/.//jackson-xc-1.8.8.jar:/usr/lib/hadoop-mapreduce/.//jasper-compiler-5.5.23.jar:/usr/lib/hadoop-mapreduce/.//jasper-runtime-5.5.23.jar:/usr/lib/hadoop-mapreduce/.//java-xmlbuilder-0.4.jar:/usr/lib/hadoop-mapreduce/.//jaxb-api-2.2.2.jar:/usr/lib/hadoop-mapreduce/.//jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-mapreduce/.//jersey-core-1.9.jar:/usr/lib/hadoop-mapreduce/.//jersey-json-1.9.jar:/usr/lib/hadoop-mapreduce/.//jersey-server-1.9.jar:/usr/lib/hadoop-mapreduce/.//jets3t-0.9.0.jar:/usr/lib/hadoop-mapreduce/.//jettison-1.1.jar:/usr/lib/hadoop-mapreduce/.//jetty-6.1.26.cloudera.4.jar:/usr/lib/hadoop-mapreduce/.//jetty-util-6.1.26.cloudera.4.jar:/usr/lib/hadoop-mapreduce/.//jsch-0.1.42.jar:/usr/lib/hadoop-mapreduce/.//jsp-api-2.1.jar:/usr/lib/hadoop-mapreduce/.//jsr305-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//junit-4.11.jar:/usr/lib/hadoop-mapreduce/.//lib:/usr/lib/hadoop-mapreduce/.//LICENSE.txt:/usr/lib/hadoop-mapreduce/.//log4j-1.2.17.jar:/usr/lib/hadoop-mapreduce/.//logs:/usr/lib/hadoop-mapreduce/.//metrics-core-3.0.2.jar:/usr/lib/hadoop-mapreduce/.//microsoft-windowsazure-storage-sdk-0.6.0.jar:/usr/lib/hadoop-mapreduce/.//mockito-all-1.8.5.jar:/usr/lib/hadoop-mapreduce/.//NOTICE.txt:/usr/lib/hadoop-mapreduce/.//paranamer-2.3.jar:/usr/lib/hadoop-mapreduce/.//protobuf-java-2.5.0.jar:/usr/lib/hadoop-mapreduce/.//sbin:/usr/lib/hadoop-mapreduce/.//servlet-api-2.5.jar:/usr/lib/hadoop-mapreduce/.//snappy-java-1.0.4.1.jar:/usr/lib/hadoop-mapreduce/.//stax-api-1.0-2.jar:/usr/lib/hadoop-mapreduce/.//xmlenc-0.52.jar:/usr/lib/hadoop-mapreduce/.//xz-1.0.jar:/usr/lib/hadoop-mapreduce/.//zookeeper.jar:/etc/hadoop/conf:/usr/lib/hadoop/bin:/usr/lib/hadoop/client:/usr/lib/hadoop/client-0.20:/usr/lib/hadoop/cloudera:/usr/lib/hadoop/etc:/usr/lib/hadoop/hadoop-annotations-2.6.0-cdh5.8.0.jar:/usr/lib/hadoop/hadoop-annotations.jar:/usr/lib/hadoop/hadoop-auth-2.6.0-cdh5.8.0.jar:/usr/lib/hadoop/hadoop-auth.jar:/usr/lib/hadoop/hadoop-aws-2.6.0-cdh5.8.0.jar:/usr/lib/hadoop/hadoop-aws.jar:/usr/lib/hadoop/hadoop-common-2.6.0-cdh5.8.0.jar:/usr/lib/hadoop/hadoop-common-2.6.0-cdh5.8.0-tests.jar:/usr/lib/hadoop/hadoop-common.jar:/usr/lib/hadoop/hadoop-common-tests.jar:/usr/lib/hadoop/hadoop-nfs-2.6.0-cdh5.8.0.jar:/usr/lib/hadoop/hadoop-nfs.jar:/usr/lib/hadoop/lib:/usr/lib/hadoop/libexec:/usr/lib/hadoop/LICENSE.txt:/usr/lib/hadoop/NOTICE.txt:/usr/lib/hadoop/parquet-avro.jar:/usr/lib/hadoop/parquet-cascading.jar:/usr/lib/hadoop/parquet-column.jar:/usr/lib/hadoop/parquet-common.jar:/usr/lib/hadoop/parquet-encoding.jar:/usr/lib/hadoop/parquet-format.jar:/usr/lib/hadoop/parquet-format-javadoc.jar:/usr/lib/hadoop/parquet-format-sources.jar:/usr/lib/hadoop/parquet-generator.jar:/usr/lib/hadoop/parquet-hadoop-bundle.jar:/usr/lib/hadoop/parquet-hadoop.jar:/usr/lib/hadoop/parquet-jackson.jar:/usr/lib/hadoop/parquet-pig-bundle.jar:/usr/lib/hadoop/parquet-pig.jar:/usr/lib/hadoop/parquet-protobuf.jar:/usr/lib/hadoop/parquet-scala_2.10.jar:/usr/lib/hadoop/parquet-scrooge_2.10.jar:/usr/lib/hadoop/parquet-test-hadoop2.jar:/usr/lib/hadoop/parquet-thrift.jar:/usr/lib/hadoop/parquet-tools.jar:/usr/lib/hadoop/sbin:/usr/lib/hadoop/lib/activation-1.1.jar:/usr/lib/hadoop/lib/apacheds-i18n-2.0.0-M15.jar:/usr/lib/hadoop/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/lib/hadoop/lib/api-asn1-api-1.0.0-M20.jar:/usr/lib/hadoop/lib/api-util-1.0.0-M20.jar:/usr/lib/hadoop/lib/asm-3.2.jar:/usr/lib/hadoop/lib/avro.jar:/usr/lib/hadoop/lib/aws-java-sdk-core-1.10.6.jar:/usr/lib/hadoop/lib/aws-java-sdk-kms-1.10.6.jar:/usr/lib/hadoop/lib/aws-java-sdk-s3-1.10.6.jar:/usr/lib/hadoop/lib/commons-beanutils-1.7.0.jar:/usr/lib/hadoop/lib/commons-beanutils-core-1.8.0.jar:/usr/lib/hadoop/lib/commons-cli-1.2.jar:/usr/lib/hadoop/lib/commons-codec-1.4.jar:/usr/lib/hadoop/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop/lib/commons-compress-1.4.1.jar:/usr/lib/hadoop/lib/commons-configuration-1.6.jar:/usr/lib/hadoop/lib/commons-digester-1.8.jar:/usr/lib/hadoop/lib/commons-el-1.0.jar:/usr/lib/hadoop/lib/commons-httpclient-3.1.jar:/usr/lib/hadoop/lib/commons-io-2.4.jar:/usr/lib/hadoop/lib/commons-lang-2.6.jar:/usr/lib/hadoop/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop/lib/commons-net-3.1.jar:/usr/lib/hadoop/lib/curator-client-2.7.1.jar:/usr/lib/hadoop/lib/curator-framework-2.7.1.jar:/usr/lib/hadoop/lib/curator-recipes-2.7.1.jar:/usr/lib/hadoop/lib/gson-2.2.4.jar:/usr/lib/hadoop/lib/guava-11.0.2.jar:/usr/lib/hadoop/lib/hamcrest-core-1.3.jar:/usr/lib/hadoop/lib/htrace-core4-4.0.1-incubating.jar:/usr/lib/hadoop/lib/httpclient-4.2.5.jar:/usr/lib/hadoop/lib/httpcore-4.2.5.jar:/usr/lib/hadoop/lib/hue-plugins-3.9.0-cdh5.8.0.jar:/usr/lib/hadoop/lib/jackson-jaxrs-1.8.8.jar:/usr/lib/hadoop/lib/jackson-xc-1.8.8.jar:/usr/lib/hadoop/lib/jasper-compiler-5.5.23.jar:/usr/lib/hadoop/lib/jasper-runtime-5.5.23.jar:/usr/lib/hadoop/lib/java-xmlbuilder-0.4.jar:/usr/lib/hadoop/lib/jaxb-api-2.2.2.jar:/usr/lib/hadoop/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop/lib/jersey-core-1.9.jar:/usr/lib/hadoop/lib/jersey-json-1.9.jar:/usr/lib/hadoop/lib/jersey-server-1.9.jar:/usr/lib/hadoop/lib/jets3t-0.9.0.jar:/usr/lib/hadoop/lib/jettison-1.1.jar:/usr/lib/hadoop/lib/jetty-6.1.26.cloudera.4.jar:/usr/lib/hadoop/lib/jetty-util-6.1.26.cloudera.4.jar:/usr/lib/hadoop/lib/jsch-0.1.42.jar:/usr/lib/hadoop/lib/jsp-api-2.1.jar:/usr/lib/hadoop/lib/jsr305-3.0.0.jar:/usr/lib/hadoop/lib/junit-4.11.jar:/usr/lib/hadoop/lib/log4j-1.2.17.jar:/usr/lib/hadoop/lib/logredactor-1.0.3.jar:/usr/lib/hadoop/lib/mockito-all-1.8.5.jar:/usr/lib/hadoop/lib/native:/usr/lib/hadoop/lib/netty-3.6.2.Final.jar:/usr/lib/hadoop/lib/paranamer-2.3.jar:/usr/lib/hadoop/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop/lib/servlet-api-2.5.jar:/usr/lib/hadoop/lib/snappy-java-1.0.4.1.jar:/usr/lib/hadoop/lib/stax-api-1.0-2.jar:/usr/lib/hadoop/lib/xmlenc-0.52.jar:/usr/lib/hadoop/lib/xz-1.0.jar:/usr/lib/hadoop/lib/zookeeper.jar:/usr/lib/zookeeper/bin:/usr/lib/zookeeper/cloudera:/usr/lib/zookeeper/conf:/usr/lib/zookeeper/lib:/usr/lib/zookeeper/LICENSE.txt:/usr/lib/zookeeper/NOTICE.txt:/usr/lib/zookeeper/zookeeper-3.4.5-cdh5.8.0.jar:/usr/lib/zookeeper/zookeeper.jar:/usr/lib/zookeeper/lib/jline-2.11.jar:/usr/lib/zookeeper/lib/log4j-1.2.16.jar:/usr/lib/zookeeper/lib/netty-3.10.5.Final.jar:/conf:/lib/*:/usr/lib/flume-ng/../search/lib/antlr-2.7.7.jar:/usr/lib/flume-ng/../search/lib/antlr-runtime-3.4.jar:/usr/lib/flume-ng/../search/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/lib/flume-ng/../search/lib/apache-mime4j-core-0.7.2.jar:/usr/lib/flume-ng/../search/lib/apache-mime4j-dom-0.7.2.jar:/usr/lib/flume-ng/../search/lib/api-asn1-api-1.0.0-M20.jar:/usr/lib/flume-ng/../search/lib/api-util-1.0.0-M20.jar:/usr/lib/flume-ng/../search/lib/argparse4j-0.4.3.jar:/usr/lib/flume-ng/../search/lib/asm-3.2.jar:/usr/lib/flume-ng/../search/lib/asm-4.1.jar:/usr/lib/flume-ng/../search/lib/asm-commons-4.1.jar:/usr/lib/flume-ng/../search/lib/asm-debug-all-4.1.jar:/usr/lib/flume-ng/../search/lib/aspectjrt-1.6.5.jar:/usr/lib/flume-ng/../search/lib/avro.jar:/usr/lib/flume-ng/../search/lib/bcmail-jdk15-1.45.jar:/usr/lib/flume-ng/../search/lib/bcprov-jdk15-1.45.jar:/usr/lib/flume-ng/../search/lib/boilerpipe-1.1.0.jar:/usr/lib/flume-ng/../search/lib/commons-cli-1.2.jar:/usr/lib/flume-ng/../search/lib/commons-codec-1.4.jar:/usr/lib/flume-ng/../search/lib/commons-collections-3.2.2.jar:/usr/lib/flume-ng/../search/lib/commons-compress-1.4.1.jar:/usr/lib/flume-ng/../search/lib/commons-configuration-1.6.jar:/usr/lib/flume-ng/../search/lib/commons-el-1.0.jar:/usr/lib/flume-ng/../search/lib/commons-fileupload-1.3.1.jar:/usr/lib/flume-ng/../search/lib/commons-io-2.4.jar:/usr/lib/flume-ng/../search/lib/commons-lang-2.6.jar:/usr/lib/flume-ng/../search/lib/commons-logging-1.1.3.jar:/usr/lib/flume-ng/../search/lib/commons-math3-3.1.1.jar:/usr/lib/flume-ng/../search/lib/commons-net-3.1.jar:/usr/lib/flume-ng/../search/lib/concurrentlinkedhashmap-lru-1.2.jar:/usr/lib/flume-ng/../search/lib/config-1.0.2.jar:/usr/lib/flume-ng/../search/lib/curator-client-2.7.1.jar:/usr/lib/flume-ng/../search/lib/curator-framework-2.7.1.jar:/usr/lib/flume-ng/../search/lib/curator-recipes-2.7.1.jar:/usr/lib/flume-ng/../search/lib/dom4j-1.6.1.jar:/usr/lib/flume-ng/../search/lib/fastutil-6.3.jar:/usr/lib/flume-ng/../search/lib/fontbox-1.8.4.jar:/usr/lib/flume-ng/../search/lib/gson-2.2.4.jar:/usr/lib/flume-ng/../search/lib/guava-11.0.2.jar:/usr/lib/flume-ng/../search/lib/hadoop-annotations.jar:/usr/lib/flume-ng/../search/lib/hadoop-auth.jar:/usr/lib/flume-ng/../search/lib/hadoop-common.jar:/usr/lib/flume-ng/../search/lib/hadoop-hdfs.jar:/usr/lib/flume-ng/../search/lib/hppc-0.5.2.jar:/usr/lib/flume-ng/../search/lib/htrace-core-3.2.0-incubating.jar:/usr/lib/flume-ng/../search/lib/htrace-core4-4.0.1-incubating.jar:/usr/lib/flume-ng/../search/lib/httpclient-4.2.5.jar:/usr/lib/flume-ng/../search/lib/httpcore-4.2.5.jar:/usr/lib/flume-ng/../search/lib/httpmime-4.2.5.jar:/usr/lib/flume-ng/../search/lib/isoparser-1.0-RC-1.jar:/usr/lib/flume-ng/../search/lib/jackson-annotations-2.3.0.jar:/usr/lib/flume-ng/../search/lib/jackson-core-2.3.1.jar:/usr/lib/flume-ng/../search/lib/jackson-core-asl-1.8.8.jar:/usr/lib/flume-ng/../search/lib/jackson-databind-2.3.1.jar:/usr/lib/flume-ng/../search/lib/jackson-mapper-asl-1.8.8.jar:/usr/lib/flume-ng/../search/lib/javax.servlet-3.0.0.v201112011016.jar:/usr/lib/flume-ng/../search/lib/jcl-over-slf4j-1.7.5.jar:/usr/lib/flume-ng/../search/lib/jdom-1.0.jar:/usr/lib/flume-ng/../search/lib/jempbox-1.8.4.jar:/usr/lib/flume-ng/../search/lib/jersey-core-1.9.jar:/usr/lib/flume-ng/../search/lib/jersey-server-1.9.jar:/usr/lib/flume-ng/../search/lib/jetty-continuation-8.1.10.v20130312.jar:/usr/lib/flume-ng/../search/lib/jetty-deploy-8.1.10.v20130312.jar:/usr/lib/flume-ng/../search/lib/jetty-http-8.1.10.v20130312.jar:/usr/lib/flume-ng/../search/lib/jetty-io-8.1.10.v20130312.jar:/usr/lib/flume-ng/../search/lib/jetty-jmx-8.1.10.v20130312.jar:/usr/lib/flume-ng/../search/lib/jetty-security-8.1.10.v20130312.jar:/usr/lib/flume-ng/../search/lib/jetty-server-8.1.10.v20130312.jar:/usr/lib/flume-ng/../search/lib/jetty-servlet-8.1.10.v20130312.jar:/usr/lib/flume-ng/../search/lib/jetty-util-8.1.10.v20130312.jar:/usr/lib/flume-ng/../search/lib/jetty-webapp-8.1.10.v20130312.jar:/usr/lib/flume-ng/../search/lib/jetty-xml-8.1.10.v20130312.jar:/usr/lib/flume-ng/../search/lib/jhighlight-1.0.jar:/usr/lib/flume-ng/../search/lib/joda-time-1.6.jar:/usr/lib/flume-ng/../search/lib/jsch-0.1.42.jar:/usr/lib/flume-ng/../search/lib/jsr305-1.3.9.jar:/usr/lib/flume-ng/../search/lib/juniversalchardet-1.0.3.jar:/usr/lib/flume-ng/../search/lib/kite-hadoop-compatibility.jar:/usr/lib/flume-ng/../search/lib/kite-morphlines-avro.jar:/usr/lib/flume-ng/../search/lib/kite-morphlines-core.jar:/usr/lib/flume-ng/../search/lib/kite-morphlines-hadoop-core.jar:/usr/lib/flume-ng/../search/lib/kite-morphlines-hadoop-parquet-avro.jar:/usr/lib/flume-ng/../search/lib/kite-morphlines-hadoop-rcfile.jar:/usr/lib/flume-ng/../search/lib/kite-morphlines-hadoop-sequencefile.jar:/usr/lib/flume-ng/../search/lib/kite-morphlines-json.jar:/usr/lib/flume-ng/../search/lib/kite-morphlines-maxmind.jar:/usr/lib/flume-ng/../search/lib/kite-morphlines-metrics-servlets.jar:/usr/lib/flume-ng/../search/lib/kite-morphlines-saxon.jar:/usr/lib/flume-ng/../search/lib/kite-morphlines-solr-cell.jar:/usr/lib/flume-ng/../search/lib/kite-morphlines-solr-core.jar:/usr/lib/flume-ng/../search/lib/kite-morphlines-tika-core.jar:/usr/lib/flume-ng/../search/lib/kite-morphlines-tika-decompress.jar:/usr/lib/flume-ng/../search/lib/kite-morphlines-twitter.jar:/usr/lib/flume-ng/../search/lib/kite-morphlines-useragent.jar:/usr/lib/flume-ng/../search/lib/leveldbjni-all-1.8.jar:/usr/lib/flume-ng/../search/lib/log4j-1.2.17.jar:/usr/lib/flume-ng/../search/lib/lucene-analyzers-common.jar:/usr/lib/flume-ng/../search/lib/lucene-analyzers-kuromoji.jar:/usr/lib/flume-ng/../search/lib/lucene-analyzers-phonetic.jar:/usr/lib/flume-ng/../search/lib/lucene-codecs.jar:/usr/lib/flume-ng/../search/lib/lucene-core.jar:/usr/lib/flume-ng/../search/lib/lucene-expressions.jar:/usr/lib/flume-ng/../search/lib/lucene-grouping.jar:/usr/lib/flume-ng/../search/lib/lucene-highlighter.jar:/usr/lib/flume-ng/../search/lib/lucene-join.jar:/usr/lib/flume-ng/../search/lib/lucene-memory.jar:/usr/lib/flume-ng/../search/lib/lucene-misc.jar:/usr/lib/flume-ng/../search/lib/lucene-queries.jar:/usr/lib/flume-ng/../search/lib/lucene-queryparser.jar:/usr/lib/flume-ng/../search/lib/lucene-spatial.jar:/usr/lib/flume-ng/../search/lib/lucene-suggest.jar:/usr/lib/flume-ng/../search/lib/maxmind-db-1.0.0.jar:/usr/lib/flume-ng/../search/lib/metadata-extractor-2.6.2.jar:/usr/lib/flume-ng/../search/lib/metrics-core-3.0.2.jar:/usr/lib/flume-ng/../search/lib/metrics-healthchecks-3.0.2.jar:/usr/lib/flume-ng/../search/lib/metrics-json-3.0.2.jar:/usr/lib/flume-ng/../search/lib/metrics-jvm-3.0.2.jar:/usr/lib/flume-ng/../search/lib/metrics-servlets-3.0.2.jar:/usr/lib/flume-ng/../search/lib/netcdf-4.2-min.jar:/usr/lib/flume-ng/../search/lib/netty-3.6.2.Final.jar:/usr/lib/flume-ng/../search/lib/netty-all-4.0.23.Final.jar:/usr/lib/flume-ng/../search/lib/noggit-0.5.jar:/usr/lib/flume-ng/../search/lib/org.restlet-2.1.1.jar:/usr/lib/flume-ng/../search/lib/org.restlet.ext.servlet-2.1.1.jar:/usr/lib/flume-ng/../search/lib/paranamer-2.3.jar:/usr/lib/flume-ng/../search/lib/parquet-avro.jar:/usr/lib/flume-ng/../search/lib/parquet-column.jar:/usr/lib/flume-ng/../search/lib/parquet-common.jar:/usr/lib/flume-ng/../search/lib/parquet-encoding.jar:/usr/lib/flume-ng/../search/lib/parquet-format.jar:/usr/lib/flume-ng/../search/lib/parquet-hadoop.jar:/usr/lib/flume-ng/../search/lib/parquet-jackson.jar:/usr/lib/flume-ng/../search/lib/pdfbox-1.8.4.jar:/usr/lib/flume-ng/../search/lib/poi-3.10-beta2.jar:/usr/lib/flume-ng/../search/lib/poi-ooxml-3.10-beta2.jar:/usr/lib/flume-ng/../search/lib/poi-ooxml-schemas-3.10.1.jar:/usr/lib/flume-ng/../search/lib/poi-scratchpad-3.10-beta2.jar:/usr/lib/flume-ng/../search/lib/protobuf-java-2.5.0.jar:/usr/lib/flume-ng/../search/lib/rome-0.9.jar:/usr/lib/flume-ng/../search/lib/Saxon-HE-9.5.1-5.jar:/usr/lib/flume-ng/../search/lib/snakeyaml-1.10.jar:/usr/lib/flume-ng/../search/lib/snappy-java-1.0.4.1.jar:/usr/lib/flume-ng/../search/lib/solr-cell.jar:/usr/lib/flume-ng/../search/lib/solr-core.jar:/usr/lib/flume-ng/../search/lib/solr-solrj.jar:/usr/lib/flume-ng/../search/lib/spatial4j-0.4.1.jar:/usr/lib/flume-ng/../search/lib/tagsoup-1.2.1.jar:/usr/lib/flume-ng/../search/lib/tika-core-1.5.jar:/usr/lib/flume-ng/../search/lib/tika-parsers-1.5.jar:/usr/lib/flume-ng/../search/lib/tika-xmp-1.5.jar:/usr/lib/flume-ng/../search/lib/ua-parser-1.3.0.jar:/usr/lib/flume-ng/../search/lib/vorbis-java-core-0.1.jar:/usr/lib/flume-ng/../search/lib/vorbis-java-core-0.1-tests.jar:/usr/lib/flume-ng/../search/lib/vorbis-java-tika-0.1.jar:/usr/lib/flume-ng/../search/lib/wstx-asl-3.2.7.jar:/usr/lib/flume-ng/../search/lib/xercesImpl-2.9.1.jar:/usr/lib/flume-ng/../search/lib/xml-apis-1.3.04.jar:/usr/lib/flume-ng/../search/lib/xmlbeans-2.6.0.jar:/usr/lib/flume-ng/../search/lib/xmlenc-0.52.jar:/usr/lib/flume-ng/../search/lib/xmpcore-5.1.2.jar:/usr/lib/flume-ng/../search/lib/xz-1.0.jar:/usr/lib/flume-ng/../search/lib/zookeeper.jar' -Djava.library.path=:/usr/lib/hadoop/lib/native:/usr/lib/hadoop/lib/native:/usr/lib/hbase/bin/../lib/native/Linux-amd64-64 org.apache.flume.node.Application --name jobSearch --conf-file live_demo.properties
17/03/18 08:20:30 INFO node.PollingPropertiesFileConfigurationProvider: Configuration provider starting
17/03/18 08:20:30 INFO node.PollingPropertiesFileConfigurationProvider: Reloading configuration file:live_demo.properties
17/03/18 08:20:30 INFO conf.FlumeConfiguration: Processing:pSnk
17/03/18 08:20:30 INFO conf.FlumeConfiguration: Processing:pSnk
17/03/18 08:20:30 INFO conf.FlumeConfiguration: Processing:pSnk
17/03/18 08:20:30 INFO conf.FlumeConfiguration: Processing:pSnk
17/03/18 08:20:30 INFO conf.FlumeConfiguration: Processing:pSnk
17/03/18 08:20:30 INFO conf.FlumeConfiguration: Added sinks: pSnk Agent: jobSearch
17/03/18 08:20:30 INFO conf.FlumeConfiguration: Processing:pSnk
17/03/18 08:20:30 INFO conf.FlumeConfiguration: Processing:pSnk
17/03/18 08:20:30 INFO conf.FlumeConfiguration: Processing:pSnk
17/03/18 08:20:30 INFO conf.FlumeConfiguration: Processing:pSnk
17/03/18 08:20:30 INFO conf.FlumeConfiguration: Processing:pSnk
17/03/18 08:20:30 INFO conf.FlumeConfiguration: Post-validation flume configuration contains configuration for agents: [jobSearch]
17/03/18 08:20:30 INFO node.AbstractConfigurationProvider: Creating channels
17/03/18 08:20:30 INFO channel.DefaultChannelFactory: Creating instance of channel pChl type file
17/03/18 08:20:31 INFO node.AbstractConfigurationProvider: Created channel pChl
17/03/18 08:20:31 INFO source.DefaultSourceFactory: Creating instance of source pSrc, type com.cloudera.flume.source.TwitterSource
17/03/18 08:20:31 INFO sink.DefaultSinkFactory: Creating instance of sink: pSnk, type: hdfs
17/03/18 08:20:31 INFO node.AbstractConfigurationProvider: Channel pChl connected to [pSrc, pSnk]
17/03/18 08:20:32 INFO node.Application: Starting new configuration:{ sourceRunners:{pSrc=EventDrivenSourceRunner: { source:com.cloudera.flume.source.TwitterSource{name:pSrc,state:IDLE} }} sinkRunners:{pSnk=SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@476e8456 counterGroup:{ name:null counters:{} } }} channels:{pChl=FileChannel pChl { dataDirs: [/home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir] }} }
17/03/18 08:20:32 INFO node.Application: Starting Channel pChl
17/03/18 08:20:32 INFO file.FileChannel: Starting FileChannel pChl { dataDirs: [/home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir] }...
17/03/18 08:20:32 INFO file.Log: Encryption is not enabled
17/03/18 08:20:32 INFO file.Log: Replay started
17/03/18 08:20:32 INFO file.Log: Found NextFileID 1, from [/home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-1]
17/03/18 08:20:32 INFO file.EventQueueBackingStoreFileV3: Starting up with /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint and /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint.meta
17/03/18 08:20:32 INFO file.EventQueueBackingStoreFileV3: Reading checkpoint metadata from /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint.meta
17/03/18 08:20:33 INFO file.FlumeEventQueue: QueueSet population inserting 0 took 0
17/03/18 08:20:33 INFO file.Log: Last Checkpoint Sat Mar 18 08:19:26 PDT 2017, queue depth = 0
17/03/18 08:20:33 INFO file.Log: Replaying logs with v2 replay logic
17/03/18 08:20:33 INFO file.ReplayHandler: Starting replay of [/home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-1]
17/03/18 08:20:33 INFO file.ReplayHandler: Replaying /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-1
17/03/18 08:20:33 INFO tools.DirectMemoryUtils: Unable to get maxDirectMemory from VM: NoSuchMethodException: sun.misc.VM.maxDirectMemory(null)
17/03/18 08:20:33 INFO tools.DirectMemoryUtils: Direct Memory Allocation:  Allocation = 1048576, Allocated = 0, MaxDirectMemorySize = 18874368, Remaining = 18874368
17/03/18 08:20:33 INFO file.LogFile: fast-forward to checkpoint position: 66696
17/03/18 08:20:33 INFO file.LogFile: Encountered EOF at 66696 in /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-1
17/03/18 08:20:33 INFO file.ReplayHandler: read: 0, put: 0, take: 0, rollback: 0, commit: 0, skip: 0, eventCount:0
17/03/18 08:20:33 INFO file.FlumeEventQueue: Search Count = 0, Search Time = 0, Copy Count = 0, Copy Time = 0
17/03/18 08:20:33 INFO file.Log: Rolling /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir
17/03/18 08:20:33 INFO file.Log: Roll start /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir
17/03/18 08:20:33 INFO file.LogFile: Opened /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2
17/03/18 08:20:33 INFO file.Log: Roll end
17/03/18 08:20:33 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 0
17/03/18 08:20:33 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850432441, queueSize: 0, queueHead: 0
17/03/18 08:20:34 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 0 logWriteOrderID: 1489850432441
17/03/18 08:20:34 INFO file.FileChannel: Queue Size after replay: 0 [channel=pChl]
17/03/18 08:20:34 INFO instrumentation.MonitoredCounterGroup: Monitored counter group for type: CHANNEL, name: pChl: Successfully registered new MBean.
17/03/18 08:20:34 INFO instrumentation.MonitoredCounterGroup: Component type: CHANNEL, name: pChl started
17/03/18 08:20:34 INFO node.Application: Starting Sink pSnk
17/03/18 08:20:34 INFO node.Application: Starting Source pSrc
17/03/18 08:20:34 INFO instrumentation.MonitoredCounterGroup: Monitored counter group for type: SINK, name: pSnk: Successfully registered new MBean.
17/03/18 08:20:34 INFO instrumentation.MonitoredCounterGroup: Component type: SINK, name: pSnk started
17/03/18 08:20:34 INFO twitter4j.TwitterStreamImpl: Establishing connection.
17/03/18 08:20:54 INFO twitter4j.TwitterStreamImpl: Connection established.
17/03/18 08:20:54 INFO twitter4j.TwitterStreamImpl: Receiving status stream.
17/03/18 08:20:59 INFO hdfs.HDFSDataStream: Serializer = TEXT, UseRawLocalFileSystem = false
17/03/18 08:21:00 INFO hdfs.BucketWriter: Creating /user/cloudera/output/handson_train/flume/techJobTweet/2017/03/18/08/job_tweet.1489850459369.txt.tmp
17/03/18 08:21:02 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 08:21:02 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850432447, queueSize: 1, queueHead: 999999
17/03/18 08:21:02 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 11344 logWriteOrderID: 1489850432447
17/03/18 08:21:32 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 3
17/03/18 08:21:32 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850432487, queueSize: 0, queueHead: 1
17/03/18 08:21:32 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 62733 logWriteOrderID: 1489850432487
17/03/18 08:22:02 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 3
17/03/18 08:22:02 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850432510, queueSize: 0, queueHead: 2
17/03/18 08:22:02 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 101722 logWriteOrderID: 1489850432510
17/03/18 08:22:32 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 08:22:32 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850432566, queueSize: 0, queueHead: 2
17/03/18 08:22:32 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 192946 logWriteOrderID: 1489850432566
17/03/18 08:23:02 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 3
17/03/18 08:23:02 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850432592, queueSize: 2, queueHead: 1
17/03/18 08:23:02 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 255098 logWriteOrderID: 1489850432592
17/03/18 08:23:32 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 4
17/03/18 08:23:32 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850432618, queueSize: 0, queueHead: 5
17/03/18 08:23:32 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 286583 logWriteOrderID: 1489850432618
17/03/18 08:24:02 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 08:24:02 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850432653, queueSize: 0, queueHead: 5
17/03/18 08:24:02 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 335963 logWriteOrderID: 1489850432653
17/03/18 08:24:32 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 08:24:32 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850432661, queueSize: 0, queueHead: 5
17/03/18 08:24:32 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 344174 logWriteOrderID: 1489850432661
17/03/18 08:25:02 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 08:25:02 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850432674, queueSize: 0, queueHead: 5
17/03/18 08:25:02 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 359914 logWriteOrderID: 1489850432674
17/03/18 08:25:32 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 08:25:32 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850432692, queueSize: 1, queueHead: 4
17/03/18 08:25:32 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 378815 logWriteOrderID: 1489850432692
17/03/18 08:26:02 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 08:26:02 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850432707, queueSize: 0, queueHead: 5
17/03/18 08:26:02 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 397258 logWriteOrderID: 1489850432707
17/03/18 08:26:32 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 08:26:32 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850432727, queueSize: 0, queueHead: 5
17/03/18 08:26:32 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 424546 logWriteOrderID: 1489850432727
17/03/18 08:27:02 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 3
17/03/18 08:27:02 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850432749, queueSize: 0, queueHead: 6
17/03/18 08:27:02 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 459185 logWriteOrderID: 1489850432749
17/03/18 08:27:32 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 08:27:32 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850432765, queueSize: 0, queueHead: 6
17/03/18 08:27:32 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 476916 logWriteOrderID: 1489850432765
17/03/18 08:28:02 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 08:28:02 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850432774, queueSize: 0, queueHead: 6
17/03/18 08:28:02 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 491980 logWriteOrderID: 1489850432774
17/03/18 08:28:32 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 08:28:32 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850432793, queueSize: 0, queueHead: 6
17/03/18 08:28:32 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 523480 logWriteOrderID: 1489850432793
17/03/18 08:29:02 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 08:29:02 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850432809, queueSize: 0, queueHead: 6
17/03/18 08:29:02 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 540429 logWriteOrderID: 1489850432809
17/03/18 08:29:32 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 08:29:32 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850432821, queueSize: 0, queueHead: 6
17/03/18 08:29:32 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 559268 logWriteOrderID: 1489850432821
17/03/18 08:30:02 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 3
17/03/18 08:30:02 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850432836, queueSize: 0, queueHead: 7
17/03/18 08:30:02 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 577759 logWriteOrderID: 1489850432836
17/03/18 08:30:32 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 08:30:32 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850432858, queueSize: 1, queueHead: 6
17/03/18 08:30:33 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 601976 logWriteOrderID: 1489850432858
17/03/18 08:31:03 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 08:31:03 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850432869, queueSize: 0, queueHead: 7
17/03/18 08:31:03 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 614277 logWriteOrderID: 1489850432869
17/03/18 08:31:09 INFO hdfs.BucketWriter: Closing /user/cloudera/output/handson_train/flume/techJobTweet/2017/03/18/08/job_tweet.1489850459369.txt.tmp
17/03/18 08:31:10 INFO hdfs.BucketWriter: Renaming /user/cloudera/output/handson_train/flume/techJobTweet/2017/03/18/08/job_tweet.1489850459369.txt.tmp to /user/cloudera/output/handson_train/flume/techJobTweet/2017/03/18/08/job_tweet.1489850459369.txt
17/03/18 08:31:10 INFO hdfs.HDFSEventSink: Writer callback called.
17/03/18 08:31:12 INFO hdfs.HDFSDataStream: Serializer = TEXT, UseRawLocalFileSystem = false
17/03/18 08:31:12 INFO hdfs.BucketWriter: Creating /user/cloudera/output/handson_train/flume/techJobTweet/2017/03/18/08/job_tweet.1489851072384.txt.tmp
17/03/18 08:31:33 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 08:31:33 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850432910, queueSize: 0, queueHead: 7
17/03/18 08:31:33 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 668617 logWriteOrderID: 1489850432910
17/03/18 08:32:03 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 3
17/03/18 08:32:03 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850432933, queueSize: 0, queueHead: 8
17/03/18 08:32:03 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 698547 logWriteOrderID: 1489850432933
17/03/18 08:32:33 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 08:32:33 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850432953, queueSize: 0, queueHead: 8
17/03/18 08:32:33 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 734875 logWriteOrderID: 1489850432953
17/03/18 08:33:03 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 08:33:03 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850432958, queueSize: 0, queueHead: 8
17/03/18 08:33:03 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 737450 logWriteOrderID: 1489850432958
17/03/18 08:33:33 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 08:33:33 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850432967, queueSize: 0, queueHead: 8
17/03/18 08:33:33 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 744520 logWriteOrderID: 1489850432967
17/03/18 08:34:03 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 08:34:03 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850432980, queueSize: 0, queueHead: 8
17/03/18 08:34:03 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 760721 logWriteOrderID: 1489850432980
17/03/18 08:34:33 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 08:34:33 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850432985, queueSize: 0, queueHead: 8
17/03/18 08:34:33 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 766032 logWriteOrderID: 1489850432985
17/03/18 08:35:03 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 3
17/03/18 08:35:03 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850433007, queueSize: 2, queueHead: 7
17/03/18 08:35:03 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 817532 logWriteOrderID: 1489850433007
17/03/18 08:35:33 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 4
17/03/18 08:35:33 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850433060, queueSize: 1, queueHead: 10
17/03/18 08:35:33 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 887454 logWriteOrderID: 1489850433060
17/03/18 08:36:03 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 4
17/03/18 08:36:03 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850433109, queueSize: 0, queueHead: 13
17/03/18 08:36:03 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 957944 logWriteOrderID: 1489850433109
17/03/18 08:36:33 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 08:36:33 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850433125, queueSize: 0, queueHead: 13
17/03/18 08:36:33 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 979239 logWriteOrderID: 1489850433125
17/03/18 08:37:03 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 08:37:03 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850433134, queueSize: 0, queueHead: 13
17/03/18 08:37:03 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 989400 logWriteOrderID: 1489850433134
17/03/18 08:37:33 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 08:37:33 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850433137, queueSize: 1, queueHead: 12
17/03/18 08:37:33 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 996977 logWriteOrderID: 1489850433137
17/03/18 08:38:03 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 08:38:03 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850433147, queueSize: 0, queueHead: 13
17/03/18 08:38:03 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 1009612 logWriteOrderID: 1489850433147
17/03/18 08:38:33 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 08:38:33 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850433164, queueSize: 0, queueHead: 13
17/03/18 08:38:33 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 1033041 logWriteOrderID: 1489850433164
17/03/18 08:39:03 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 08:39:03 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850433187, queueSize: 1, queueHead: 12
17/03/18 08:39:03 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 1063647 logWriteOrderID: 1489850433187
17/03/18 08:39:33 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 4
17/03/18 08:39:33 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850433209, queueSize: 1, queueHead: 14
17/03/18 08:39:33 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 1086856 logWriteOrderID: 1489850433209
17/03/18 08:40:03 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 08:40:03 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850433236, queueSize: 2, queueHead: 13
17/03/18 08:40:03 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 1123794 logWriteOrderID: 1489850433236
17/03/18 08:40:33 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 08:40:33 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850433259, queueSize: 0, queueHead: 15
17/03/18 08:40:33 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 1149117 logWriteOrderID: 1489850433259
17/03/18 08:41:03 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 08:41:03 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850433264, queueSize: 0, queueHead: 15
17/03/18 08:41:03 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 1153644 logWriteOrderID: 1489850433264
17/03/18 08:41:12 INFO hdfs.BucketWriter: Closing /user/cloudera/output/handson_train/flume/techJobTweet/2017/03/18/08/job_tweet.1489851072384.txt.tmp
17/03/18 08:41:12 INFO hdfs.BucketWriter: Renaming /user/cloudera/output/handson_train/flume/techJobTweet/2017/03/18/08/job_tweet.1489851072384.txt.tmp to /user/cloudera/output/handson_train/flume/techJobTweet/2017/03/18/08/job_tweet.1489851072384.txt
17/03/18 08:41:12 INFO hdfs.HDFSEventSink: Writer callback called.
17/03/18 08:41:18 INFO hdfs.HDFSDataStream: Serializer = TEXT, UseRawLocalFileSystem = false
17/03/18 08:41:18 INFO hdfs.BucketWriter: Creating /user/cloudera/output/handson_train/flume/techJobTweet/2017/03/18/08/job_tweet.1489851678332.txt.tmp
17/03/18 08:41:33 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 08:41:33 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850433281, queueSize: 0, queueHead: 15
17/03/18 08:41:33 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 1176995 logWriteOrderID: 1489850433281
17/03/18 08:42:03 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 08:42:03 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850433304, queueSize: 0, queueHead: 15
17/03/18 08:42:03 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 1203204 logWriteOrderID: 1489850433304
17/03/18 08:42:33 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 08:42:33 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850433317, queueSize: 0, queueHead: 15
17/03/18 08:42:33 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 1217687 logWriteOrderID: 1489850433317
17/03/18 08:43:03 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 08:43:03 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850433331, queueSize: 1, queueHead: 14
17/03/18 08:43:03 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 1243096 logWriteOrderID: 1489850433331
17/03/18 08:43:33 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 08:43:33 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850433341, queueSize: 0, queueHead: 15
17/03/18 08:43:33 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 1254904 logWriteOrderID: 1489850433341
17/03/18 08:44:03 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 08:44:03 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850433350, queueSize: 0, queueHead: 15
17/03/18 08:44:04 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 1266599 logWriteOrderID: 1489850433350
17/03/18 08:44:34 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 08:44:34 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850433366, queueSize: 0, queueHead: 15
17/03/18 08:44:34 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 1293273 logWriteOrderID: 1489850433366
17/03/18 08:45:04 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 08:45:04 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850433373, queueSize: 1, queueHead: 14
17/03/18 08:45:04 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 1304621 logWriteOrderID: 1489850433373
17/03/18 08:45:34 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 3
17/03/18 08:45:34 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850433426, queueSize: 0, queueHead: 16
17/03/18 08:45:34 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 1366424 logWriteOrderID: 1489850433426
17/03/18 08:46:04 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 08:46:04 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850433453, queueSize: 1, queueHead: 15
17/03/18 08:46:04 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 1395676 logWriteOrderID: 1489850433453
17/03/18 08:46:34 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 08:46:34 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850433484, queueSize: 0, queueHead: 16
17/03/18 08:46:34 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 1419140 logWriteOrderID: 1489850433484
17/03/18 08:47:04 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 08:47:04 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850433489, queueSize: 2, queueHead: 14
17/03/18 08:47:04 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 1433615 logWriteOrderID: 1489850433489
17/03/18 08:47:34 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 08:47:34 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850433493, queueSize: 0, queueHead: 16
17/03/18 08:47:34 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 1433733 logWriteOrderID: 1489850433493
17/03/18 08:48:04 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 4
17/03/18 08:48:04 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850433525, queueSize: 0, queueHead: 18
17/03/18 08:48:04 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 1487530 logWriteOrderID: 1489850433525
17/03/18 08:48:34 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 08:48:34 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850433550, queueSize: 1, queueHead: 17
17/03/18 08:48:34 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 1512546 logWriteOrderID: 1489850433550
17/03/18 08:49:04 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 08:49:04 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850433557, queueSize: 0, queueHead: 18
17/03/18 08:49:04 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 1515623 logWriteOrderID: 1489850433557
17/03/18 08:49:34 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 08:49:34 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850433581, queueSize: 0, queueHead: 18
17/03/18 08:49:34 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 1554019 logWriteOrderID: 1489850433581
17/03/18 08:50:04 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 3
17/03/18 08:50:04 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850433604, queueSize: 0, queueHead: 19
17/03/18 08:50:04 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 1581582 logWriteOrderID: 1489850433604
17/03/18 08:50:34 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 08:50:34 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850433623, queueSize: 0, queueHead: 19
17/03/18 08:50:34 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 1599041 logWriteOrderID: 1489850433623
17/03/18 08:51:04 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 08:51:04 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850433635, queueSize: 0, queueHead: 19
17/03/18 08:51:04 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 1617103 logWriteOrderID: 1489850433635
17/03/18 08:51:18 INFO hdfs.BucketWriter: Closing /user/cloudera/output/handson_train/flume/techJobTweet/2017/03/18/08/job_tweet.1489851678332.txt.tmp
17/03/18 08:51:18 INFO hdfs.BucketWriter: Renaming /user/cloudera/output/handson_train/flume/techJobTweet/2017/03/18/08/job_tweet.1489851678332.txt.tmp to /user/cloudera/output/handson_train/flume/techJobTweet/2017/03/18/08/job_tweet.1489851678332.txt
17/03/18 08:51:18 INFO hdfs.HDFSEventSink: Writer callback called.
17/03/18 08:51:23 INFO hdfs.HDFSDataStream: Serializer = TEXT, UseRawLocalFileSystem = false
17/03/18 08:51:23 INFO hdfs.BucketWriter: Creating /user/cloudera/output/handson_train/flume/techJobTweet/2017/03/18/08/job_tweet.1489852283192.txt.tmp
17/03/18 08:51:34 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 08:51:34 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850433648, queueSize: 0, queueHead: 19
17/03/18 08:51:34 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 1624812 logWriteOrderID: 1489850433648
17/03/18 08:52:04 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 08:52:04 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850433680, queueSize: 0, queueHead: 19
17/03/18 08:52:04 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 1652825 logWriteOrderID: 1489850433680
17/03/18 08:52:34 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 08:52:34 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850433693, queueSize: 0, queueHead: 19
17/03/18 08:52:34 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 1666194 logWriteOrderID: 1489850433693
17/03/18 08:53:04 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 08:53:04 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850433711, queueSize: 1, queueHead: 18
17/03/18 08:53:04 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 1693699 logWriteOrderID: 1489850433711
17/03/18 08:53:34 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 08:53:34 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850433726, queueSize: 0, queueHead: 19
17/03/18 08:53:34 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 1702517 logWriteOrderID: 1489850433726
17/03/18 08:54:04 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 08:54:04 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850433735, queueSize: 0, queueHead: 19
17/03/18 08:54:04 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 1710898 logWriteOrderID: 1489850433735
17/03/18 08:54:34 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 08:54:34 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850433740, queueSize: 0, queueHead: 19
17/03/18 08:54:34 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 1713565 logWriteOrderID: 1489850433740
17/03/18 08:55:04 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 08:55:04 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850433745, queueSize: 2, queueHead: 17
17/03/18 08:55:04 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 1718799 logWriteOrderID: 1489850433745
17/03/18 08:55:34 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 4
17/03/18 08:55:34 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850433782, queueSize: 1, queueHead: 20
17/03/18 08:55:34 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 1758165 logWriteOrderID: 1489850433782
17/03/18 08:56:04 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 08:56:04 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850433809, queueSize: 0, queueHead: 21
17/03/18 08:56:04 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 1793169 logWriteOrderID: 1489850433809
17/03/18 08:56:34 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 3
17/03/18 08:56:34 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850433835, queueSize: 0, queueHead: 22
17/03/18 08:56:34 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 1822404 logWriteOrderID: 1489850433835
17/03/18 08:57:04 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 4
17/03/18 08:57:04 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850433860, queueSize: 0, queueHead: 24
17/03/18 08:57:05 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 1850451 logWriteOrderID: 1489850433860
17/03/18 08:57:35 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 08:57:35 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850433891, queueSize: 2, queueHead: 22
17/03/18 08:57:35 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 1894157 logWriteOrderID: 1489850433891
17/03/18 08:58:05 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 08:58:05 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850433899, queueSize: 0, queueHead: 24
17/03/18 08:58:05 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 1902019 logWriteOrderID: 1489850433899
17/03/18 08:58:35 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 3
17/03/18 08:58:35 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850433918, queueSize: 0, queueHead: 25
17/03/18 08:58:35 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 1928055 logWriteOrderID: 1489850433918
17/03/18 08:59:05 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 08:59:05 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850433927, queueSize: 0, queueHead: 25
17/03/18 08:59:05 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 1935117 logWriteOrderID: 1489850433927
17/03/18 08:59:35 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 08:59:35 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850433943, queueSize: 0, queueHead: 25
17/03/18 08:59:35 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 1950626 logWriteOrderID: 1489850433943
17/03/18 09:00:00 INFO hdfs.HDFSDataStream: Serializer = TEXT, UseRawLocalFileSystem = false
17/03/18 09:00:01 INFO hdfs.BucketWriter: Creating /user/cloudera/output/handson_train/flume/techJobTweet/2017/03/18/09/job_tweet.1489852800850.txt.tmp
17/03/18 09:00:05 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 09:00:05 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850433962, queueSize: 1, queueHead: 24
17/03/18 09:00:05 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 1967971 logWriteOrderID: 1489850433962
17/03/18 09:00:35 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 09:00:35 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850433980, queueSize: 0, queueHead: 25
17/03/18 09:00:35 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 1980945 logWriteOrderID: 1489850433980
17/03/18 09:01:05 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 09:01:05 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850433999, queueSize: 1, queueHead: 24
17/03/18 09:01:05 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 2005186 logWriteOrderID: 1489850433999
17/03/18 09:01:23 INFO hdfs.BucketWriter: Closing /user/cloudera/output/handson_train/flume/techJobTweet/2017/03/18/08/job_tweet.1489852283192.txt.tmp
17/03/18 09:01:23 INFO hdfs.BucketWriter: Renaming /user/cloudera/output/handson_train/flume/techJobTweet/2017/03/18/08/job_tweet.1489852283192.txt.tmp to /user/cloudera/output/handson_train/flume/techJobTweet/2017/03/18/08/job_tweet.1489852283192.txt
17/03/18 09:01:23 INFO hdfs.HDFSEventSink: Writer callback called.
17/03/18 09:01:35 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 3
17/03/18 09:01:35 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850434016, queueSize: 0, queueHead: 26
17/03/18 09:01:35 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 2022364 logWriteOrderID: 1489850434016
17/03/18 09:02:05 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 4
17/03/18 09:02:05 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850434041, queueSize: 0, queueHead: 28
17/03/18 09:02:05 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 2060457 logWriteOrderID: 1489850434041
17/03/18 09:02:35 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 09:02:35 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850434055, queueSize: 1, queueHead: 27
17/03/18 09:02:35 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 2079530 logWriteOrderID: 1489850434055
17/03/18 09:03:05 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 3
17/03/18 09:03:05 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850434082, queueSize: 1, queueHead: 28
17/03/18 09:03:05 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 2106833 logWriteOrderID: 1489850434082
17/03/18 09:03:35 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 09:03:35 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850434093, queueSize: 0, queueHead: 29
17/03/18 09:03:35 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 2114864 logWriteOrderID: 1489850434093
17/03/18 09:04:05 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 09:04:05 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850434105, queueSize: 0, queueHead: 29
17/03/18 09:04:05 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 2128947 logWriteOrderID: 1489850434105
17/03/18 09:04:35 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 09:04:35 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850434129, queueSize: 0, queueHead: 29
17/03/18 09:04:35 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 2160227 logWriteOrderID: 1489850434129
17/03/18 09:05:05 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 09:05:05 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850434134, queueSize: 0, queueHead: 29
17/03/18 09:05:05 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 2165049 logWriteOrderID: 1489850434134
17/03/18 09:05:35 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 09:05:35 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850434153, queueSize: 0, queueHead: 29
17/03/18 09:05:35 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 2181453 logWriteOrderID: 1489850434153
17/03/18 09:06:05 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 09:06:05 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850434173, queueSize: 0, queueHead: 29
17/03/18 09:06:05 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 2203379 logWriteOrderID: 1489850434173
17/03/18 09:06:35 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 09:06:35 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850434193, queueSize: 0, queueHead: 29
17/03/18 09:06:35 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 2234268 logWriteOrderID: 1489850434193
17/03/18 09:07:05 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 09:07:05 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850434217, queueSize: 0, queueHead: 29
17/03/18 09:07:05 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 2260878 logWriteOrderID: 1489850434217
17/03/18 09:07:35 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 09:07:35 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850434222, queueSize: 0, queueHead: 29
17/03/18 09:07:35 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 2263876 logWriteOrderID: 1489850434222
17/03/18 09:08:05 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 09:08:05 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850434243, queueSize: 0, queueHead: 29
17/03/18 09:08:05 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 2282545 logWriteOrderID: 1489850434243
17/03/18 09:08:35 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 09:08:35 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850434262, queueSize: 1, queueHead: 28
17/03/18 09:08:35 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 2316067 logWriteOrderID: 1489850434262
17/03/18 09:09:05 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 09:09:05 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850434277, queueSize: 0, queueHead: 29
17/03/18 09:09:05 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 2332325 logWriteOrderID: 1489850434277
17/03/18 09:09:35 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 3
17/03/18 09:09:35 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850434296, queueSize: 0, queueHead: 30
17/03/18 09:09:35 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 2353086 logWriteOrderID: 1489850434296
17/03/18 09:10:01 INFO hdfs.BucketWriter: Closing /user/cloudera/output/handson_train/flume/techJobTweet/2017/03/18/09/job_tweet.1489852800850.txt.tmp
17/03/18 09:10:01 INFO hdfs.BucketWriter: Renaming /user/cloudera/output/handson_train/flume/techJobTweet/2017/03/18/09/job_tweet.1489852800850.txt.tmp to /user/cloudera/output/handson_train/flume/techJobTweet/2017/03/18/09/job_tweet.1489852800850.txt
17/03/18 09:10:01 INFO hdfs.HDFSEventSink: Writer callback called.
17/03/18 09:10:03 INFO hdfs.HDFSDataStream: Serializer = TEXT, UseRawLocalFileSystem = false
17/03/18 09:10:03 INFO hdfs.BucketWriter: Creating /user/cloudera/output/handson_train/flume/techJobTweet/2017/03/18/09/job_tweet.1489853403699.txt.tmp
17/03/18 09:10:05 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 09:10:05 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850434320, queueSize: 0, queueHead: 30
17/03/18 09:10:06 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 2387725 logWriteOrderID: 1489850434320
17/03/18 09:10:36 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 09:10:36 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850434352, queueSize: 1, queueHead: 29
17/03/18 09:10:36 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 2429352 logWriteOrderID: 1489850434352
17/03/18 09:11:06 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 09:11:06 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850434378, queueSize: 0, queueHead: 30
17/03/18 09:11:06 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 2463399 logWriteOrderID: 1489850434378
17/03/18 09:11:36 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 09:11:36 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850434406, queueSize: 0, queueHead: 30
17/03/18 09:11:36 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 2506564 logWriteOrderID: 1489850434406
17/03/18 09:12:06 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 09:12:06 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850434421, queueSize: 0, queueHead: 30
17/03/18 09:12:06 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 2544538 logWriteOrderID: 1489850434421
17/03/18 09:12:36 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 09:12:36 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850434440, queueSize: 1, queueHead: 29
17/03/18 09:12:36 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 2564875 logWriteOrderID: 1489850434440
17/03/18 09:13:06 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 09:13:06 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850434453, queueSize: 1, queueHead: 29
17/03/18 09:13:06 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 2578875 logWriteOrderID: 1489850434453
17/03/18 09:13:36 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 09:13:36 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850434467, queueSize: 0, queueHead: 30
17/03/18 09:13:36 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 2598677 logWriteOrderID: 1489850434467
17/03/18 09:14:06 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 09:14:06 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850434488, queueSize: 0, queueHead: 30
17/03/18 09:14:06 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 2626686 logWriteOrderID: 1489850434488
17/03/18 09:14:36 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 09:14:36 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850434504, queueSize: 0, queueHead: 30
17/03/18 09:14:36 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 2644635 logWriteOrderID: 1489850434504
17/03/18 09:15:06 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 3
17/03/18 09:15:06 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850434523, queueSize: 0, queueHead: 31
17/03/18 09:15:06 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 2662443 logWriteOrderID: 1489850434523
17/03/18 09:15:36 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 09:15:36 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850434543, queueSize: 0, queueHead: 31
17/03/18 09:15:36 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 2678889 logWriteOrderID: 1489850434543
17/03/18 09:16:06 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 09:16:06 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850434560, queueSize: 0, queueHead: 31
17/03/18 09:16:06 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 2699190 logWriteOrderID: 1489850434560
17/03/18 09:17:06 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 09:17:06 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850434590, queueSize: 0, queueHead: 31
17/03/18 09:17:06 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 2730463 logWriteOrderID: 1489850434590
17/03/18 09:17:36 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 09:17:36 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850434618, queueSize: 0, queueHead: 31
17/03/18 09:17:36 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 2753276 logWriteOrderID: 1489850434618
17/03/18 09:18:06 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 09:18:06 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850434635, queueSize: 0, queueHead: 31
17/03/18 09:18:06 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 2776624 logWriteOrderID: 1489850434635
17/03/18 09:18:36 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 09:18:36 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850434652, queueSize: 0, queueHead: 31
17/03/18 09:18:36 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 2789721 logWriteOrderID: 1489850434652
17/03/18 09:19:06 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 09:19:06 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850434670, queueSize: 1, queueHead: 30
17/03/18 09:19:06 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 2814158 logWriteOrderID: 1489850434670
17/03/18 09:19:36 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 09:19:36 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850434699, queueSize: 0, queueHead: 31
17/03/18 09:19:36 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 2848110 logWriteOrderID: 1489850434699
17/03/18 09:20:04 INFO hdfs.BucketWriter: Closing /user/cloudera/output/handson_train/flume/techJobTweet/2017/03/18/09/job_tweet.1489853403699.txt.tmp
17/03/18 09:20:04 INFO hdfs.BucketWriter: Renaming /user/cloudera/output/handson_train/flume/techJobTweet/2017/03/18/09/job_tweet.1489853403699.txt.tmp to /user/cloudera/output/handson_train/flume/techJobTweet/2017/03/18/09/job_tweet.1489853403699.txt
17/03/18 09:20:04 INFO hdfs.HDFSEventSink: Writer callback called.
17/03/18 09:20:06 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 09:20:06 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850434717, queueSize: 1, queueHead: 30
17/03/18 09:20:06 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 2866395 logWriteOrderID: 1489850434717
17/03/18 09:20:06 INFO hdfs.HDFSDataStream: Serializer = TEXT, UseRawLocalFileSystem = false
17/03/18 09:20:07 INFO hdfs.BucketWriter: Creating /user/cloudera/output/handson_train/flume/techJobTweet/2017/03/18/09/job_tweet.1489854006814.txt.tmp
17/03/18 09:20:36 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 09:20:36 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850434724, queueSize: 0, queueHead: 31
17/03/18 09:20:36 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 2873004 logWriteOrderID: 1489850434724
17/03/18 09:21:06 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 09:21:06 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850434741, queueSize: 0, queueHead: 31
17/03/18 09:21:06 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 2885223 logWriteOrderID: 1489850434741
17/03/18 09:21:36 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 09:21:36 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850434750, queueSize: 0, queueHead: 31
17/03/18 09:21:36 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 2895677 logWriteOrderID: 1489850434750
17/03/18 09:22:06 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 09:22:06 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850434769, queueSize: 0, queueHead: 31
17/03/18 09:22:06 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 2919574 logWriteOrderID: 1489850434769
17/03/18 09:22:36 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 3
17/03/18 09:22:36 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850434792, queueSize: 0, queueHead: 32
17/03/18 09:22:36 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 2963093 logWriteOrderID: 1489850434792
17/03/18 09:23:06 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 09:23:06 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850434805, queueSize: 0, queueHead: 32
17/03/18 09:23:07 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 2978290 logWriteOrderID: 1489850434805
17/03/18 09:23:37 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 09:23:37 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850434810, queueSize: 0, queueHead: 32
17/03/18 09:23:37 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 2980403 logWriteOrderID: 1489850434810
17/03/18 09:24:07 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 09:24:07 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850434826, queueSize: 0, queueHead: 32
17/03/18 09:24:07 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 2995532 logWriteOrderID: 1489850434826
17/03/18 09:24:37 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 09:24:37 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850434846, queueSize: 0, queueHead: 32
17/03/18 09:24:37 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 3015769 logWriteOrderID: 1489850434846
17/03/18 09:25:07 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 09:25:07 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850434855, queueSize: 2, queueHead: 30
17/03/18 09:25:07 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 3023977 logWriteOrderID: 1489850434855
17/03/18 09:25:37 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 4
17/03/18 09:25:37 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850434873, queueSize: 0, queueHead: 34
17/03/18 09:25:37 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 3041199 logWriteOrderID: 1489850434873
17/03/18 09:26:07 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 09:26:07 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850434900, queueSize: 0, queueHead: 34
17/03/18 09:26:07 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 3079120 logWriteOrderID: 1489850434900
17/03/18 09:26:37 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 3
17/03/18 09:26:37 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850434938, queueSize: 0, queueHead: 35
17/03/18 09:26:37 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 3136032 logWriteOrderID: 1489850434938
17/03/18 09:27:07 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 09:27:07 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850434958, queueSize: 0, queueHead: 35
17/03/18 09:27:07 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 3171494 logWriteOrderID: 1489850434958
17/03/18 09:27:37 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 09:27:37 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850434971, queueSize: 2, queueHead: 33
17/03/18 09:27:37 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 3188340 logWriteOrderID: 1489850434971
17/03/18 09:28:07 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 09:28:07 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850434977, queueSize: 1, queueHead: 34
17/03/18 09:28:07 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 3194827 logWriteOrderID: 1489850434977
17/03/18 09:28:37 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 09:28:37 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850434987, queueSize: 0, queueHead: 35
17/03/18 09:28:37 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 3203378 logWriteOrderID: 1489850434987
17/03/18 09:29:07 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 09:29:07 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850435010, queueSize: 0, queueHead: 35
17/03/18 09:29:07 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 3231797 logWriteOrderID: 1489850435010
17/03/18 09:29:37 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 09:29:37 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850435015, queueSize: 0, queueHead: 35
17/03/18 09:29:37 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 3234282 logWriteOrderID: 1489850435015
17/03/18 09:30:07 INFO hdfs.BucketWriter: Closing /user/cloudera/output/handson_train/flume/techJobTweet/2017/03/18/09/job_tweet.1489854006814.txt.tmp
17/03/18 09:30:07 INFO hdfs.BucketWriter: Renaming /user/cloudera/output/handson_train/flume/techJobTweet/2017/03/18/09/job_tweet.1489854006814.txt.tmp to /user/cloudera/output/handson_train/flume/techJobTweet/2017/03/18/09/job_tweet.1489854006814.txt
17/03/18 09:30:07 INFO hdfs.HDFSEventSink: Writer callback called.
17/03/18 09:30:07 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 09:30:07 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850435028, queueSize: 0, queueHead: 35
17/03/18 09:30:07 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 3247042 logWriteOrderID: 1489850435028
17/03/18 09:30:18 INFO hdfs.HDFSDataStream: Serializer = TEXT, UseRawLocalFileSystem = false
17/03/18 09:30:18 INFO hdfs.BucketWriter: Creating /user/cloudera/output/handson_train/flume/techJobTweet/2017/03/18/09/job_tweet.1489854618410.txt.tmp
17/03/18 09:30:37 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 3
17/03/18 09:30:37 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850435064, queueSize: 1, queueHead: 35
17/03/18 09:30:37 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 3285477 logWriteOrderID: 1489850435064
17/03/18 09:31:07 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 09:31:07 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850435090, queueSize: 0, queueHead: 36
17/03/18 09:31:07 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 3305309 logWriteOrderID: 1489850435090
17/03/18 09:31:37 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 3
17/03/18 09:31:37 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850435105, queueSize: 0, queueHead: 37
17/03/18 09:31:37 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 3332140 logWriteOrderID: 1489850435105
17/03/18 09:32:07 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 09:32:07 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850435127, queueSize: 0, queueHead: 37
17/03/18 09:32:07 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 3360020 logWriteOrderID: 1489850435127
17/03/18 09:32:37 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 3
17/03/18 09:32:37 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850435146, queueSize: 0, queueHead: 38
17/03/18 09:32:37 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 3387507 logWriteOrderID: 1489850435146
17/03/18 09:33:07 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 09:33:07 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850435151, queueSize: 0, queueHead: 38
17/03/18 09:33:07 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 3390152 logWriteOrderID: 1489850435151
17/03/18 09:33:37 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 09:33:37 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850435179, queueSize: 0, queueHead: 38
17/03/18 09:33:37 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 3431642 logWriteOrderID: 1489850435179
17/03/18 09:34:07 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 3
17/03/18 09:34:07 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850435231, queueSize: 0, queueHead: 39
17/03/18 09:34:07 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 3502161 logWriteOrderID: 1489850435231
17/03/18 09:34:37 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 09:34:37 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850435240, queueSize: 0, queueHead: 39
17/03/18 09:34:37 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 3512147 logWriteOrderID: 1489850435240
17/03/18 09:35:07 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 09:35:07 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850435256, queueSize: 0, queueHead: 39
17/03/18 09:35:07 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 3538388 logWriteOrderID: 1489850435256
17/03/18 09:35:37 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 09:35:37 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850435282, queueSize: 1, queueHead: 38
17/03/18 09:35:37 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 3595152 logWriteOrderID: 1489850435282
17/03/18 09:36:07 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 09:36:08 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850435317, queueSize: 0, queueHead: 39
17/03/18 09:36:08 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 3647705 logWriteOrderID: 1489850435317
17/03/18 09:36:38 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 3
17/03/18 09:36:38 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850435347, queueSize: 0, queueHead: 40
17/03/18 09:36:38 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 3688916 logWriteOrderID: 1489850435347
17/03/18 09:37:08 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 6
17/03/18 09:37:08 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850435375, queueSize: 0, queueHead: 44
17/03/18 09:37:08 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 3726066 logWriteOrderID: 1489850435375
17/03/18 09:37:38 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 09:37:38 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850435388, queueSize: 0, queueHead: 44
17/03/18 09:37:38 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 3750147 logWriteOrderID: 1489850435388
17/03/18 09:38:08 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 09:38:08 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850435405, queueSize: 0, queueHead: 44
17/03/18 09:38:08 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 3771282 logWriteOrderID: 1489850435405
17/03/18 09:39:08 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 09:39:08 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850435414, queueSize: 0, queueHead: 44
17/03/18 09:39:08 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 3788194 logWriteOrderID: 1489850435414
17/03/18 09:40:08 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 3
17/03/18 09:40:08 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850435437, queueSize: 0, queueHead: 45
17/03/18 09:40:08 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 3829841 logWriteOrderID: 1489850435437
17/03/18 09:40:18 INFO hdfs.BucketWriter: Closing /user/cloudera/output/handson_train/flume/techJobTweet/2017/03/18/09/job_tweet.1489854618410.txt.tmp
17/03/18 09:40:18 INFO hdfs.BucketWriter: Renaming /user/cloudera/output/handson_train/flume/techJobTweet/2017/03/18/09/job_tweet.1489854618410.txt.tmp to /user/cloudera/output/handson_train/flume/techJobTweet/2017/03/18/09/job_tweet.1489854618410.txt
17/03/18 09:40:18 INFO hdfs.HDFSEventSink: Writer callback called.
17/03/18 09:40:22 INFO hdfs.HDFSDataStream: Serializer = TEXT, UseRawLocalFileSystem = false
17/03/18 09:40:22 INFO hdfs.BucketWriter: Creating /user/cloudera/output/handson_train/flume/techJobTweet/2017/03/18/09/job_tweet.1489855222308.txt.tmp
17/03/18 09:40:38 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 09:40:38 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850435450, queueSize: 0, queueHead: 45
17/03/18 09:40:38 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 3850697 logWriteOrderID: 1489850435450
17/03/18 09:41:08 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 09:41:08 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850435481, queueSize: 0, queueHead: 45
17/03/18 09:41:08 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 3893647 logWriteOrderID: 1489850435481
17/03/18 09:41:38 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 09:41:38 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850435494, queueSize: 0, queueHead: 45
17/03/18 09:41:38 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 3906758 logWriteOrderID: 1489850435494
17/03/18 09:42:08 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 09:42:08 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850435507, queueSize: 0, queueHead: 45
17/03/18 09:42:08 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 3929286 logWriteOrderID: 1489850435507
17/03/18 09:42:38 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 09:42:38 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850435524, queueSize: 0, queueHead: 45
17/03/18 09:42:38 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 3953596 logWriteOrderID: 1489850435524
17/03/18 09:43:08 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 09:43:08 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850435544, queueSize: 0, queueHead: 45
17/03/18 09:43:08 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 3981887 logWriteOrderID: 1489850435544
17/03/18 09:43:38 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 09:43:38 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850435572, queueSize: 0, queueHead: 45
17/03/18 09:43:38 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 4026406 logWriteOrderID: 1489850435572
17/03/18 09:44:08 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 09:44:08 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850435577, queueSize: 0, queueHead: 45
17/03/18 09:44:08 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 4029041 logWriteOrderID: 1489850435577
17/03/18 09:44:38 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 3
17/03/18 09:44:38 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850435598, queueSize: 1, queueHead: 45
17/03/18 09:44:38 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 4077533 logWriteOrderID: 1489850435598
17/03/18 09:45:08 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 3
17/03/18 09:45:08 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850435622, queueSize: 0, queueHead: 47
17/03/18 09:45:08 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 4106525 logWriteOrderID: 1489850435622
17/03/18 09:45:38 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 09:45:38 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850435648, queueSize: 1, queueHead: 46
17/03/18 09:45:38 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 4132123 logWriteOrderID: 1489850435648
17/03/18 09:46:08 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 09:46:08 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850435670, queueSize: 0, queueHead: 47
17/03/18 09:46:08 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 4154411 logWriteOrderID: 1489850435670
17/03/18 09:46:38 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 3
17/03/18 09:46:38 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850435689, queueSize: 0, queueHead: 48
17/03/18 09:46:38 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 4176954 logWriteOrderID: 1489850435689
17/03/18 09:47:08 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 09:47:08 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850435694, queueSize: 0, queueHead: 48
17/03/18 09:47:08 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 4184367 logWriteOrderID: 1489850435694
17/03/18 09:47:38 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 09:47:38 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850435699, queueSize: 0, queueHead: 48
17/03/18 09:47:38 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 4186419 logWriteOrderID: 1489850435699
17/03/18 09:48:08 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 09:48:08 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850435711, queueSize: 0, queueHead: 48
17/03/18 09:48:08 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 4208234 logWriteOrderID: 1489850435711
17/03/18 09:48:38 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 3
17/03/18 09:48:38 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850435762, queueSize: 0, queueHead: 49
17/03/18 09:48:38 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 4247863 logWriteOrderID: 1489850435762
17/03/18 09:49:08 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 6
17/03/18 09:49:08 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850435827, queueSize: 0, queueHead: 53
17/03/18 09:49:09 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 4300338 logWriteOrderID: 1489850435827
17/03/18 09:49:39 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 09:49:39 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850435851, queueSize: 0, queueHead: 53
17/03/18 09:49:39 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 4325501 logWriteOrderID: 1489850435851
17/03/18 09:50:09 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 09:50:09 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850435863, queueSize: 0, queueHead: 53
17/03/18 09:50:09 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 4339119 logWriteOrderID: 1489850435863
17/03/18 09:50:22 INFO hdfs.BucketWriter: Closing /user/cloudera/output/handson_train/flume/techJobTweet/2017/03/18/09/job_tweet.1489855222308.txt.tmp
17/03/18 09:50:22 INFO hdfs.BucketWriter: Renaming /user/cloudera/output/handson_train/flume/techJobTweet/2017/03/18/09/job_tweet.1489855222308.txt.tmp to /user/cloudera/output/handson_train/flume/techJobTweet/2017/03/18/09/job_tweet.1489855222308.txt
17/03/18 09:50:22 INFO hdfs.HDFSEventSink: Writer callback called.
17/03/18 09:50:29 INFO hdfs.HDFSDataStream: Serializer = TEXT, UseRawLocalFileSystem = false
17/03/18 09:50:29 INFO hdfs.BucketWriter: Creating /user/cloudera/output/handson_train/flume/techJobTweet/2017/03/18/09/job_tweet.1489855829401.txt.tmp
17/03/18 09:50:39 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 09:50:39 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850435883, queueSize: 0, queueHead: 53
17/03/18 09:50:39 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 4368592 logWriteOrderID: 1489850435883
17/03/18 09:51:09 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 09:51:09 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850435900, queueSize: 0, queueHead: 53
17/03/18 09:51:09 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 4378563 logWriteOrderID: 1489850435900
17/03/18 09:51:39 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 09:51:39 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850435907, queueSize: 1, queueHead: 52
17/03/18 09:51:39 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 4395607 logWriteOrderID: 1489850435907
17/03/18 09:52:09 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 09:52:09 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850435918, queueSize: 0, queueHead: 53
17/03/18 09:52:09 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 4403269 logWriteOrderID: 1489850435918
17/03/18 09:52:39 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 09:52:39 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850435927, queueSize: 0, queueHead: 53
17/03/18 09:52:39 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 4411162 logWriteOrderID: 1489850435927
17/03/18 09:53:09 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 09:53:09 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850435938, queueSize: 1, queueHead: 52
17/03/18 09:53:09 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 4422194 logWriteOrderID: 1489850435938
17/03/18 09:53:39 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 09:53:39 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850435956, queueSize: 0, queueHead: 53
17/03/18 09:53:39 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 4435414 logWriteOrderID: 1489850435956
17/03/18 09:54:09 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 09:54:09 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850435986, queueSize: 1, queueHead: 52
17/03/18 09:54:09 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 4466467 logWriteOrderID: 1489850435986
17/03/18 09:54:39 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 09:54:39 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850436001, queueSize: 0, queueHead: 53
17/03/18 09:54:39 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 4488601 logWriteOrderID: 1489850436001
17/03/18 09:55:09 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 09:55:09 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850436018, queueSize: 0, queueHead: 53
17/03/18 09:55:09 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 4510793 logWriteOrderID: 1489850436018
17/03/18 09:55:39 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 09:55:39 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850436031, queueSize: 0, queueHead: 53
17/03/18 09:55:39 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 4526879 logWriteOrderID: 1489850436031
17/03/18 09:56:09 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 09:56:09 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850436059, queueSize: 0, queueHead: 53
17/03/18 09:56:09 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 4555766 logWriteOrderID: 1489850436059
17/03/18 09:56:39 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 09:56:39 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850436064, queueSize: 0, queueHead: 53
17/03/18 09:56:39 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 4558636 logWriteOrderID: 1489850436064
17/03/18 09:57:09 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 09:57:09 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850436069, queueSize: 0, queueHead: 53
17/03/18 09:57:09 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 4560992 logWriteOrderID: 1489850436069
17/03/18 09:57:39 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 09:57:39 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850436086, queueSize: 0, queueHead: 53
17/03/18 09:57:39 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 4571615 logWriteOrderID: 1489850436086
17/03/18 09:58:09 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 09:58:09 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850436107, queueSize: 0, queueHead: 53
17/03/18 09:58:09 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 4594861 logWriteOrderID: 1489850436107
17/03/18 09:58:39 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 09:58:39 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850436120, queueSize: 0, queueHead: 53
17/03/18 09:58:39 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 4611122 logWriteOrderID: 1489850436120
17/03/18 09:59:09 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 09:59:09 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850436133, queueSize: 0, queueHead: 53
17/03/18 09:59:09 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 4626772 logWriteOrderID: 1489850436133
17/03/18 09:59:39 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 09:59:39 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850436149, queueSize: 0, queueHead: 53
17/03/18 09:59:39 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 4652790 logWriteOrderID: 1489850436149
17/03/18 10:00:01 INFO hdfs.HDFSDataStream: Serializer = TEXT, UseRawLocalFileSystem = false
17/03/18 10:00:02 INFO hdfs.BucketWriter: Creating /user/cloudera/output/handson_train/flume/techJobTweet/2017/03/18/10/job_tweet.1489856401893.txt.tmp
17/03/18 10:00:09 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 3
17/03/18 10:00:09 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850436183, queueSize: 0, queueHead: 54
17/03/18 10:00:09 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 4700324 logWriteOrderID: 1489850436183
17/03/18 10:00:29 INFO hdfs.BucketWriter: Closing /user/cloudera/output/handson_train/flume/techJobTweet/2017/03/18/09/job_tweet.1489855829401.txt.tmp
17/03/18 10:00:29 INFO hdfs.BucketWriter: Renaming /user/cloudera/output/handson_train/flume/techJobTweet/2017/03/18/09/job_tweet.1489855829401.txt.tmp to /user/cloudera/output/handson_train/flume/techJobTweet/2017/03/18/09/job_tweet.1489855829401.txt
17/03/18 10:00:29 INFO hdfs.HDFSEventSink: Writer callback called.
17/03/18 10:00:39 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 4
17/03/18 10:00:39 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850436218, queueSize: 1, queueHead: 55
17/03/18 10:00:39 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 4738820 logWriteOrderID: 1489850436218
17/03/18 10:01:09 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 10:01:09 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850436259, queueSize: 0, queueHead: 56
17/03/18 10:01:09 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 4770380 logWriteOrderID: 1489850436259
17/03/18 10:01:39 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 4
17/03/18 10:01:40 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850436300, queueSize: 0, queueHead: 58
17/03/18 10:01:40 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 4810153 logWriteOrderID: 1489850436300
17/03/18 10:02:10 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 10:02:10 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850436320, queueSize: 0, queueHead: 58
17/03/18 10:02:10 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 4839160 logWriteOrderID: 1489850436320
17/03/18 10:02:40 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 10:02:40 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850436344, queueSize: 0, queueHead: 58
17/03/18 10:02:40 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 4862476 logWriteOrderID: 1489850436344
17/03/18 10:03:10 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 10:03:10 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850436369, queueSize: 1, queueHead: 57
17/03/18 10:03:10 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 4895795 logWriteOrderID: 1489850436369
17/03/18 10:03:40 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 3
17/03/18 10:03:40 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850436394, queueSize: 0, queueHead: 59
17/03/18 10:03:40 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 4926346 logWriteOrderID: 1489850436394
17/03/18 10:04:10 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 10:04:10 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850436414, queueSize: 0, queueHead: 59
17/03/18 10:04:10 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 4941911 logWriteOrderID: 1489850436414
17/03/18 10:04:40 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 10:04:40 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850436427, queueSize: 0, queueHead: 59
17/03/18 10:04:40 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 4959183 logWriteOrderID: 1489850436427
17/03/18 10:05:10 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 4
17/03/18 10:05:10 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850436448, queueSize: 0, queueHead: 61
17/03/18 10:05:10 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 4995650 logWriteOrderID: 1489850436448
17/03/18 10:05:40 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 10:05:40 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850436453, queueSize: 0, queueHead: 61
17/03/18 10:05:40 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 4998004 logWriteOrderID: 1489850436453
17/03/18 10:06:10 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 10:06:10 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850436462, queueSize: 0, queueHead: 61
17/03/18 10:06:10 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 5007208 logWriteOrderID: 1489850436462
17/03/18 10:06:40 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 10:06:40 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850436484, queueSize: 1, queueHead: 60
17/03/18 10:06:40 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 5029807 logWriteOrderID: 1489850436484
17/03/18 10:07:10 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 10:07:10 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850436491, queueSize: 0, queueHead: 61
17/03/18 10:07:10 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 5035008 logWriteOrderID: 1489850436491
17/03/18 10:07:40 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 5
17/03/18 10:07:40 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850436530, queueSize: 0, queueHead: 64
17/03/18 10:07:40 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 5094838 logWriteOrderID: 1489850436530
17/03/18 10:08:10 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 10:08:10 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850436570, queueSize: 1, queueHead: 63
17/03/18 10:08:10 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 5163392 logWriteOrderID: 1489850436570
17/03/18 10:08:40 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 3
17/03/18 10:08:40 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850436620, queueSize: 0, queueHead: 65
17/03/18 10:08:40 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 5242280 logWriteOrderID: 1489850436620
17/03/18 10:09:10 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 10:09:10 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850436635, queueSize: 1, queueHead: 64
17/03/18 10:09:10 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 5264097 logWriteOrderID: 1489850436635
17/03/18 10:09:40 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 10:09:40 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850436655, queueSize: 1, queueHead: 64
17/03/18 10:09:40 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 5288828 logWriteOrderID: 1489850436655
17/03/18 10:10:02 INFO hdfs.BucketWriter: Closing /user/cloudera/output/handson_train/flume/techJobTweet/2017/03/18/10/job_tweet.1489856401893.txt.tmp
17/03/18 10:10:02 INFO hdfs.BucketWriter: Renaming /user/cloudera/output/handson_train/flume/techJobTweet/2017/03/18/10/job_tweet.1489856401893.txt.tmp to /user/cloudera/output/handson_train/flume/techJobTweet/2017/03/18/10/job_tweet.1489856401893.txt
17/03/18 10:10:02 INFO hdfs.HDFSEventSink: Writer callback called.
17/03/18 10:10:08 INFO hdfs.HDFSDataStream: Serializer = TEXT, UseRawLocalFileSystem = false
17/03/18 10:10:08 INFO hdfs.BucketWriter: Creating /user/cloudera/output/handson_train/flume/techJobTweet/2017/03/18/10/job_tweet.1489857008284.txt.tmp
17/03/18 10:10:10 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 10:10:10 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850436688, queueSize: 0, queueHead: 65
17/03/18 10:10:10 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 5332322 logWriteOrderID: 1489850436688
17/03/18 10:10:40 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 10:10:40 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850436707, queueSize: 0, queueHead: 65
17/03/18 10:10:40 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 5365224 logWriteOrderID: 1489850436707
17/03/18 10:11:10 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 3
17/03/18 10:11:10 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850436733, queueSize: 0, queueHead: 66
17/03/18 10:11:10 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 5403646 logWriteOrderID: 1489850436733
17/03/18 10:11:40 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 3
17/03/18 10:11:40 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850436752, queueSize: 0, queueHead: 67
17/03/18 10:11:40 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 5420026 logWriteOrderID: 1489850436752
17/03/18 10:12:10 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 10:12:10 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850436767, queueSize: 1, queueHead: 66
17/03/18 10:12:10 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 5435390 logWriteOrderID: 1489850436767
17/03/18 10:12:40 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 10:12:40 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850436793, queueSize: 0, queueHead: 67
17/03/18 10:12:40 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 5479271 logWriteOrderID: 1489850436793
17/03/18 10:13:10 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 10:13:10 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850436798, queueSize: 0, queueHead: 67
17/03/18 10:13:11 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 5490002 logWriteOrderID: 1489850436798
17/03/18 10:13:41 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 10:13:41 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850436812, queueSize: 1, queueHead: 66
17/03/18 10:13:41 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 5512676 logWriteOrderID: 1489850436812
17/03/18 10:14:11 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 10:14:11 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850436822, queueSize: 0, queueHead: 67
17/03/18 10:14:11 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 5519672 logWriteOrderID: 1489850436822
17/03/18 10:14:41 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 10:14:41 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850436846, queueSize: 0, queueHead: 67
17/03/18 10:14:41 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 5552057 logWriteOrderID: 1489850436846
17/03/18 10:15:11 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 3
17/03/18 10:15:11 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850436866, queueSize: 1, queueHead: 67
17/03/18 10:15:11 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 5587615 logWriteOrderID: 1489850436866
17/03/18 10:15:41 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 10:15:41 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850436882, queueSize: 1, queueHead: 67
17/03/18 10:15:41 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 5601941 logWriteOrderID: 1489850436882
17/03/18 10:16:11 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 3
17/03/18 10:16:11 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850436922, queueSize: 0, queueHead: 69
17/03/18 10:16:11 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 5653608 logWriteOrderID: 1489850436922
17/03/18 10:16:41 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 10:16:41 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850436946, queueSize: 0, queueHead: 69
17/03/18 10:16:41 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 5680008 logWriteOrderID: 1489850436946
17/03/18 10:17:11 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 10:17:11 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850436966, queueSize: 0, queueHead: 69
17/03/18 10:17:11 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 5695940 logWriteOrderID: 1489850436966
17/03/18 10:17:41 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 10:17:41 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850436983, queueSize: 0, queueHead: 69
17/03/18 10:17:41 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 5716714 logWriteOrderID: 1489850436983
17/03/18 10:18:11 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 3
17/03/18 10:18:11 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850437001, queueSize: 0, queueHead: 70
17/03/18 10:18:11 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 5732765 logWriteOrderID: 1489850437001
17/03/18 10:18:41 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 10:18:41 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850437014, queueSize: 0, queueHead: 70
17/03/18 10:18:41 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 5750555 logWriteOrderID: 1489850437014
17/03/18 10:19:11 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 10:19:11 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850437030, queueSize: 0, queueHead: 70
17/03/18 10:19:11 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 5776248 logWriteOrderID: 1489850437030
17/03/18 10:19:41 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 10:19:41 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850437043, queueSize: 0, queueHead: 70
17/03/18 10:19:41 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 5785795 logWriteOrderID: 1489850437043
17/03/18 10:20:08 INFO hdfs.BucketWriter: Closing /user/cloudera/output/handson_train/flume/techJobTweet/2017/03/18/10/job_tweet.1489857008284.txt.tmp
17/03/18 10:20:08 INFO hdfs.BucketWriter: Renaming /user/cloudera/output/handson_train/flume/techJobTweet/2017/03/18/10/job_tweet.1489857008284.txt.tmp to /user/cloudera/output/handson_train/flume/techJobTweet/2017/03/18/10/job_tweet.1489857008284.txt
17/03/18 10:20:08 INFO hdfs.HDFSEventSink: Writer callback called.
17/03/18 10:20:11 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 10:20:11 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850437063, queueSize: 0, queueHead: 70
17/03/18 10:20:11 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 5804069 logWriteOrderID: 1489850437063
17/03/18 10:20:41 INFO hdfs.HDFSDataStream: Serializer = TEXT, UseRawLocalFileSystem = false
17/03/18 10:20:41 INFO hdfs.BucketWriter: Creating /user/cloudera/output/handson_train/flume/techJobTweet/2017/03/18/10/job_tweet.1489857641172.txt.tmp
17/03/18 10:20:41 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 10:20:41 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850437071, queueSize: 0, queueHead: 70
17/03/18 10:20:41 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 5809411 logWriteOrderID: 1489850437071
17/03/18 10:21:11 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 10:21:11 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850437084, queueSize: 0, queueHead: 70
17/03/18 10:21:11 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 5822338 logWriteOrderID: 1489850437084
17/03/18 10:21:41 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 10:21:41 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850437096, queueSize: 0, queueHead: 70
17/03/18 10:21:41 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 5839650 logWriteOrderID: 1489850437096
17/03/18 10:22:11 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 10:22:11 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850437106, queueSize: 1, queueHead: 69
17/03/18 10:22:11 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 5849796 logWriteOrderID: 1489850437106
17/03/18 10:22:41 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 10:22:41 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850437131, queueSize: 0, queueHead: 70
17/03/18 10:22:41 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 5878297 logWriteOrderID: 1489850437131
17/03/18 10:23:11 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 10:23:11 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850437153, queueSize: 0, queueHead: 70
17/03/18 10:23:11 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 5902484 logWriteOrderID: 1489850437153
17/03/18 10:23:41 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 10:23:41 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850437172, queueSize: 0, queueHead: 70
17/03/18 10:23:41 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 5930443 logWriteOrderID: 1489850437172
17/03/18 10:24:11 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 10:24:11 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850437196, queueSize: 0, queueHead: 70
17/03/18 10:24:11 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 5950641 logWriteOrderID: 1489850437196
17/03/18 10:24:41 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 10:24:42 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850437209, queueSize: 0, queueHead: 70
17/03/18 10:24:42 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 5970154 logWriteOrderID: 1489850437209
17/03/18 10:25:12 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 10:25:12 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850437233, queueSize: 2, queueHead: 68
17/03/18 10:25:12 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 6013115 logWriteOrderID: 1489850437233
17/03/18 10:25:42 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 10:25:42 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850437249, queueSize: 0, queueHead: 70
17/03/18 10:25:42 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 6031481 logWriteOrderID: 1489850437249
17/03/18 10:26:12 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 10:26:12 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850437265, queueSize: 0, queueHead: 70
17/03/18 10:26:12 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 6058992 logWriteOrderID: 1489850437265
17/03/18 10:26:42 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 10:26:42 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850437283, queueSize: 1, queueHead: 69
17/03/18 10:26:42 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 6078803 logWriteOrderID: 1489850437283
17/03/18 10:27:12 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 10:27:12 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850437297, queueSize: 0, queueHead: 70
17/03/18 10:27:12 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 6097314 logWriteOrderID: 1489850437297
17/03/18 10:27:42 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 10:27:42 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850437310, queueSize: 0, queueHead: 70
17/03/18 10:27:42 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 6116759 logWriteOrderID: 1489850437310
17/03/18 10:28:12 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 10:28:12 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850437330, queueSize: 0, queueHead: 70
17/03/18 10:28:12 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 6141614 logWriteOrderID: 1489850437330
17/03/18 10:28:42 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 10:28:42 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850437342, queueSize: 0, queueHead: 70
17/03/18 10:28:42 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 6160282 logWriteOrderID: 1489850437342
17/03/18 10:29:12 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 10:29:12 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850437358, queueSize: 0, queueHead: 70
17/03/18 10:29:12 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 6189092 logWriteOrderID: 1489850437358
17/03/18 10:29:42 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 10:29:42 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850437367, queueSize: 0, queueHead: 70
17/03/18 10:29:42 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 6200753 logWriteOrderID: 1489850437367
17/03/18 10:30:12 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 4
17/03/18 10:30:12 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850437405, queueSize: 1, queueHead: 71
17/03/18 10:30:12 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 6246855 logWriteOrderID: 1489850437405
17/03/18 10:30:41 INFO hdfs.BucketWriter: Closing /user/cloudera/output/handson_train/flume/techJobTweet/2017/03/18/10/job_tweet.1489857641172.txt.tmp
17/03/18 10:30:41 INFO hdfs.BucketWriter: Renaming /user/cloudera/output/handson_train/flume/techJobTweet/2017/03/18/10/job_tweet.1489857641172.txt.tmp to /user/cloudera/output/handson_train/flume/techJobTweet/2017/03/18/10/job_tweet.1489857641172.txt
17/03/18 10:30:41 INFO hdfs.HDFSEventSink: Writer callback called.
17/03/18 10:30:42 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 10:30:42 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850437433, queueSize: 0, queueHead: 72
17/03/18 10:30:42 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 6285238 logWriteOrderID: 1489850437433
17/03/18 10:30:51 INFO hdfs.HDFSDataStream: Serializer = TEXT, UseRawLocalFileSystem = false
17/03/18 10:30:51 INFO hdfs.BucketWriter: Creating /user/cloudera/output/handson_train/flume/techJobTweet/2017/03/18/10/job_tweet.1489858251107.txt.tmp
17/03/18 10:31:12 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 10:31:12 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850437462, queueSize: 0, queueHead: 72
17/03/18 10:31:12 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 6324417 logWriteOrderID: 1489850437462
17/03/18 10:31:42 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 10:31:42 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850437476, queueSize: 1, queueHead: 71
17/03/18 10:31:42 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 6345249 logWriteOrderID: 1489850437476
17/03/18 10:32:12 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 3
17/03/18 10:32:12 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850437512, queueSize: 0, queueHead: 73
17/03/18 10:32:12 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 6398178 logWriteOrderID: 1489850437512
17/03/18 10:32:42 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 10:32:42 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850437517, queueSize: 0, queueHead: 73
17/03/18 10:32:42 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 6401210 logWriteOrderID: 1489850437517
17/03/18 10:33:12 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 3
17/03/18 10:33:12 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850437541, queueSize: 1, queueHead: 73
17/03/18 10:33:12 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 6440140 logWriteOrderID: 1489850437541
17/03/18 10:33:42 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 10:33:42 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850437569, queueSize: 0, queueHead: 74
17/03/18 10:33:42 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 6465116 logWriteOrderID: 1489850437569
17/03/18 10:34:12 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 10:34:12 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850437596, queueSize: 0, queueHead: 74
17/03/18 10:34:12 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 6502700 logWriteOrderID: 1489850437596
17/03/18 10:34:42 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 10:34:42 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850437621, queueSize: 0, queueHead: 74
17/03/18 10:34:42 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 6533020 logWriteOrderID: 1489850437621
17/03/18 10:35:12 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 10:35:12 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850437643, queueSize: 1, queueHead: 73
17/03/18 10:35:12 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 6555754 logWriteOrderID: 1489850437643
17/03/18 10:35:42 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 3
17/03/18 10:35:42 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850437658, queueSize: 1, queueHead: 74
17/03/18 10:35:42 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 6575486 logWriteOrderID: 1489850437658
17/03/18 10:36:12 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 3
17/03/18 10:36:12 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850437681, queueSize: 1, queueHead: 75
17/03/18 10:36:13 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 6601025 logWriteOrderID: 1489850437681
17/03/18 10:36:43 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 10:36:43 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850437696, queueSize: 0, queueHead: 76
17/03/18 10:36:43 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 6614838 logWriteOrderID: 1489850437696
17/03/18 10:37:13 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 3
17/03/18 10:37:13 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850437711, queueSize: 0, queueHead: 77
17/03/18 10:37:13 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 6637551 logWriteOrderID: 1489850437711
17/03/18 10:37:43 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 10:37:43 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850437724, queueSize: 0, queueHead: 77
17/03/18 10:37:43 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 6655365 logWriteOrderID: 1489850437724
17/03/18 10:38:13 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 10:38:13 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850437733, queueSize: 0, queueHead: 77
17/03/18 10:38:13 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 6666311 logWriteOrderID: 1489850437733
17/03/18 10:38:43 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 5
17/03/18 10:38:43 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850437766, queueSize: 0, queueHead: 80
17/03/18 10:38:43 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 6711949 logWriteOrderID: 1489850437766
17/03/18 10:39:13 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 10:39:13 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850437778, queueSize: 0, queueHead: 80
17/03/18 10:39:13 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 6726262 logWriteOrderID: 1489850437778
17/03/18 10:39:43 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 5
17/03/18 10:39:43 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850437828, queueSize: 0, queueHead: 83
17/03/18 10:39:43 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 6810404 logWriteOrderID: 1489850437828
17/03/18 10:40:13 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 3
17/03/18 10:40:13 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850437870, queueSize: 0, queueHead: 84
17/03/18 10:40:13 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 6869050 logWriteOrderID: 1489850437870
17/03/18 10:40:43 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 10:40:43 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850437897, queueSize: 0, queueHead: 84
17/03/18 10:40:43 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 6910019 logWriteOrderID: 1489850437897
17/03/18 10:40:51 INFO hdfs.BucketWriter: Closing /user/cloudera/output/handson_train/flume/techJobTweet/2017/03/18/10/job_tweet.1489858251107.txt.tmp
17/03/18 10:40:51 INFO hdfs.BucketWriter: Renaming /user/cloudera/output/handson_train/flume/techJobTweet/2017/03/18/10/job_tweet.1489858251107.txt.tmp to /user/cloudera/output/handson_train/flume/techJobTweet/2017/03/18/10/job_tweet.1489858251107.txt
17/03/18 10:40:51 INFO hdfs.HDFSEventSink: Writer callback called.
17/03/18 10:40:56 INFO hdfs.HDFSDataStream: Serializer = TEXT, UseRawLocalFileSystem = false
17/03/18 10:40:56 INFO hdfs.BucketWriter: Creating /user/cloudera/output/handson_train/flume/techJobTweet/2017/03/18/10/job_tweet.1489858856327.txt.tmp
17/03/18 10:41:13 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 10:41:13 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850437925, queueSize: 0, queueHead: 84
17/03/18 10:41:13 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 6951156 logWriteOrderID: 1489850437925
17/03/18 10:41:43 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 4
17/03/18 10:41:43 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850437971, queueSize: 0, queueHead: 86
17/03/18 10:41:43 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 7016175 logWriteOrderID: 1489850437971
17/03/18 10:42:13 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 10:42:13 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850437983, queueSize: 0, queueHead: 86
17/03/18 10:42:13 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 7027364 logWriteOrderID: 1489850437983
17/03/18 10:42:43 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 10:42:43 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850438012, queueSize: 0, queueHead: 86
17/03/18 10:42:43 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 7064542 logWriteOrderID: 1489850438012
17/03/18 10:43:13 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 3
17/03/18 10:43:13 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850438030, queueSize: 0, queueHead: 87
17/03/18 10:43:13 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 7096988 logWriteOrderID: 1489850438030
17/03/18 10:43:43 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 3
17/03/18 10:43:43 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850438049, queueSize: 0, queueHead: 88
17/03/18 10:43:43 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 7122441 logWriteOrderID: 1489850438049
17/03/18 10:44:13 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 3
17/03/18 10:44:13 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850438082, queueSize: 0, queueHead: 89
17/03/18 10:44:13 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 7179204 logWriteOrderID: 1489850438082
17/03/18 10:44:43 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 10:44:43 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850438097, queueSize: 0, queueHead: 89
17/03/18 10:44:43 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 7202938 logWriteOrderID: 1489850438097
17/03/18 10:45:13 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 10:45:13 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850438131, queueSize: 0, queueHead: 89
17/03/18 10:45:13 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 7241477 logWriteOrderID: 1489850438131
17/03/18 10:45:43 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 10:45:43 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850438152, queueSize: 0, queueHead: 89
17/03/18 10:45:43 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 7274835 logWriteOrderID: 1489850438152
17/03/18 10:46:13 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 10:46:13 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850438178, queueSize: 0, queueHead: 89
17/03/18 10:46:13 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 7313314 logWriteOrderID: 1489850438178
17/03/18 10:46:43 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 10:46:43 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850438205, queueSize: 0, queueHead: 89
17/03/18 10:46:43 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 7348813 logWriteOrderID: 1489850438205
17/03/18 10:47:13 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 10:47:13 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850438213, queueSize: 0, queueHead: 89
17/03/18 10:47:13 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 7361763 logWriteOrderID: 1489850438213
17/03/18 10:47:43 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 10:47:43 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850438226, queueSize: 0, queueHead: 89
17/03/18 10:47:43 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 7378832 logWriteOrderID: 1489850438226
17/03/18 10:48:13 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 10:48:13 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850438242, queueSize: 0, queueHead: 89
17/03/18 10:48:13 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 7400560 logWriteOrderID: 1489850438242
17/03/18 10:48:43 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 3
17/03/18 10:48:43 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850438269, queueSize: 0, queueHead: 90
17/03/18 10:48:43 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 7439814 logWriteOrderID: 1489850438269
17/03/18 10:49:13 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 10:49:13 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850438308, queueSize: 0, queueHead: 90
17/03/18 10:49:14 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 7494365 logWriteOrderID: 1489850438308
17/03/18 10:49:44 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 10:49:44 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850438330, queueSize: 1, queueHead: 89
17/03/18 10:49:44 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 7522285 logWriteOrderID: 1489850438330
17/03/18 10:50:14 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 10:50:14 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850438344, queueSize: 0, queueHead: 90
17/03/18 10:50:14 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 7532841 logWriteOrderID: 1489850438344
17/03/18 10:50:44 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 10:50:44 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850438364, queueSize: 0, queueHead: 90
17/03/18 10:50:44 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 7557559 logWriteOrderID: 1489850438364
17/03/18 10:50:56 INFO hdfs.BucketWriter: Closing /user/cloudera/output/handson_train/flume/techJobTweet/2017/03/18/10/job_tweet.1489858856327.txt.tmp
17/03/18 10:50:56 INFO hdfs.BucketWriter: Renaming /user/cloudera/output/handson_train/flume/techJobTweet/2017/03/18/10/job_tweet.1489858856327.txt.tmp to /user/cloudera/output/handson_train/flume/techJobTweet/2017/03/18/10/job_tweet.1489858856327.txt
17/03/18 10:50:56 INFO hdfs.HDFSEventSink: Writer callback called.
17/03/18 10:51:14 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 10:51:14 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850438367, queueSize: 1, queueHead: 89
17/03/18 10:51:14 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 7560047 logWriteOrderID: 1489850438367
17/03/18 10:51:14 INFO hdfs.HDFSDataStream: Serializer = TEXT, UseRawLocalFileSystem = false
17/03/18 10:51:14 INFO hdfs.BucketWriter: Creating /user/cloudera/output/handson_train/flume/techJobTweet/2017/03/18/10/job_tweet.1489859474408.txt.tmp
17/03/18 10:51:44 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 10:51:44 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850438372, queueSize: 1, queueHead: 89
17/03/18 10:51:44 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 7566860 logWriteOrderID: 1489850438372
17/03/18 10:52:14 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 10:52:14 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850438398, queueSize: 0, queueHead: 90
17/03/18 10:52:14 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 7590083 logWriteOrderID: 1489850438398
17/03/18 10:52:44 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 10:52:44 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850438418, queueSize: 0, queueHead: 90
17/03/18 10:52:44 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 7615584 logWriteOrderID: 1489850438418
17/03/18 10:53:14 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 10:53:14 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850438449, queueSize: 0, queueHead: 90
17/03/18 10:53:14 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 7657532 logWriteOrderID: 1489850438449
17/03/18 10:53:44 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 10:53:44 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850438454, queueSize: 0, queueHead: 90
17/03/18 10:53:44 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 7662348 logWriteOrderID: 1489850438454
17/03/18 10:54:14 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 3
17/03/18 10:54:14 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850438476, queueSize: 0, queueHead: 91
17/03/18 10:54:14 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 7695987 logWriteOrderID: 1489850438476
17/03/18 10:54:44 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 10:54:44 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850438492, queueSize: 0, queueHead: 91
17/03/18 10:54:44 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 7720800 logWriteOrderID: 1489850438492
17/03/18 10:55:14 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 4
17/03/18 10:55:14 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850438547, queueSize: 0, queueHead: 93
17/03/18 10:55:14 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 7792792 logWriteOrderID: 1489850438547
17/03/18 10:55:44 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 10:55:44 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850438567, queueSize: 0, queueHead: 93
17/03/18 10:55:44 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 7824044 logWriteOrderID: 1489850438567
17/03/18 10:56:14 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 10:56:14 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850438574, queueSize: 1, queueHead: 92
17/03/18 10:56:14 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 7833505 logWriteOrderID: 1489850438574
17/03/18 10:56:44 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 10:56:44 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850438593, queueSize: 0, queueHead: 93
17/03/18 10:56:44 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 7850896 logWriteOrderID: 1489850438593
17/03/18 10:57:14 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 10:57:14 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850438609, queueSize: 0, queueHead: 93
17/03/18 10:57:14 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 7870302 logWriteOrderID: 1489850438609
17/03/18 10:57:44 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 3
17/03/18 10:57:44 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850438632, queueSize: 0, queueHead: 94
17/03/18 10:57:44 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 7897837 logWriteOrderID: 1489850438632
17/03/18 10:58:14 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 10:58:14 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850438644, queueSize: 0, queueHead: 94
17/03/18 10:58:14 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 7910601 logWriteOrderID: 1489850438644
17/03/18 10:58:44 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 10:58:44 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850438673, queueSize: 1, queueHead: 93
17/03/18 10:58:44 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 7940204 logWriteOrderID: 1489850438673
17/03/18 10:59:14 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 10:59:14 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850438695, queueSize: 0, queueHead: 94
17/03/18 10:59:14 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 7963698 logWriteOrderID: 1489850438695
17/03/18 10:59:44 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 10:59:44 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850438707, queueSize: 0, queueHead: 94
17/03/18 10:59:44 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 7980071 logWriteOrderID: 1489850438707
17/03/18 11:00:03 INFO hdfs.HDFSDataStream: Serializer = TEXT, UseRawLocalFileSystem = false
17/03/18 11:00:03 INFO hdfs.BucketWriter: Creating /user/cloudera/output/handson_train/flume/techJobTweet/2017/03/18/11/job_tweet.1489860003098.txt.tmp
17/03/18 11:00:14 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 11:00:14 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850438731, queueSize: 0, queueHead: 94
17/03/18 11:00:14 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 8004432 logWriteOrderID: 1489850438731
17/03/18 11:00:44 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 11:00:44 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850438748, queueSize: 0, queueHead: 94
17/03/18 11:00:44 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 8021503 logWriteOrderID: 1489850438748
17/03/18 11:01:14 INFO hdfs.BucketWriter: Closing /user/cloudera/output/handson_train/flume/techJobTweet/2017/03/18/10/job_tweet.1489859474408.txt.tmp
17/03/18 11:01:14 INFO hdfs.BucketWriter: Renaming /user/cloudera/output/handson_train/flume/techJobTweet/2017/03/18/10/job_tweet.1489859474408.txt.tmp to /user/cloudera/output/handson_train/flume/techJobTweet/2017/03/18/10/job_tweet.1489859474408.txt
17/03/18 11:01:14 INFO hdfs.HDFSEventSink: Writer callback called.
17/03/18 11:01:14 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 3
17/03/18 11:01:14 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850438783, queueSize: 1, queueHead: 94
17/03/18 11:01:14 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 8064725 logWriteOrderID: 1489850438783
17/03/18 11:01:45 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 11:01:45 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850438801, queueSize: 0, queueHead: 95
17/03/18 11:01:45 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 8082592 logWriteOrderID: 1489850438801
17/03/18 11:02:15 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 3
17/03/18 11:02:15 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850438819, queueSize: 0, queueHead: 96
17/03/18 11:02:15 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 8099070 logWriteOrderID: 1489850438819
17/03/18 11:02:45 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 11:02:45 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850438839, queueSize: 0, queueHead: 96
17/03/18 11:02:45 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 8122474 logWriteOrderID: 1489850438839
17/03/18 11:03:15 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 11:03:15 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850438852, queueSize: 0, queueHead: 96
17/03/18 11:03:15 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 8139181 logWriteOrderID: 1489850438852
17/03/18 11:03:45 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 11:03:45 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850438871, queueSize: 0, queueHead: 96
17/03/18 11:03:45 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 8168037 logWriteOrderID: 1489850438871
17/03/18 11:04:15 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 11:04:15 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850438888, queueSize: 0, queueHead: 96
17/03/18 11:04:15 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 8192843 logWriteOrderID: 1489850438888
17/03/18 11:04:45 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 11:04:45 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850438904, queueSize: 0, queueHead: 96
17/03/18 11:04:45 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 8214047 logWriteOrderID: 1489850438904
17/03/18 11:05:15 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 11:05:15 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850438928, queueSize: 0, queueHead: 96
17/03/18 11:05:15 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 8255987 logWriteOrderID: 1489850438928
17/03/18 11:05:45 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 11:05:45 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850438961, queueSize: 0, queueHead: 96
17/03/18 11:05:45 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 8320067 logWriteOrderID: 1489850438961
17/03/18 11:06:15 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 3
17/03/18 11:06:15 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850438987, queueSize: 0, queueHead: 97
17/03/18 11:06:15 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 8355430 logWriteOrderID: 1489850438987
17/03/18 11:06:45 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 11:06:45 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850439001, queueSize: 1, queueHead: 96
17/03/18 11:06:45 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 8383470 logWriteOrderID: 1489850439001
17/03/18 11:07:15 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 11:07:15 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850439020, queueSize: 0, queueHead: 97
17/03/18 11:07:15 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 8400344 logWriteOrderID: 1489850439020
17/03/18 11:07:45 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 11:07:45 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850439036, queueSize: 0, queueHead: 97
17/03/18 11:07:45 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 8426005 logWriteOrderID: 1489850439036
17/03/18 11:08:15 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 11:08:15 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850439073, queueSize: 0, queueHead: 97
17/03/18 11:08:15 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 8477229 logWriteOrderID: 1489850439073
17/03/18 11:08:45 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 11:08:45 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850439089, queueSize: 0, queueHead: 97
17/03/18 11:08:45 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 8507294 logWriteOrderID: 1489850439089
17/03/18 11:09:15 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 11:09:15 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850439104, queueSize: 0, queueHead: 97
17/03/18 11:09:15 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 8532724 logWriteOrderID: 1489850439104
17/03/18 11:09:45 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 11:09:45 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850439119, queueSize: 1, queueHead: 96
17/03/18 11:09:45 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 8557300 logWriteOrderID: 1489850439119
17/03/18 11:10:03 INFO hdfs.BucketWriter: Closing /user/cloudera/output/handson_train/flume/techJobTweet/2017/03/18/11/job_tweet.1489860003098.txt.tmp
17/03/18 11:10:03 INFO hdfs.BucketWriter: Renaming /user/cloudera/output/handson_train/flume/techJobTweet/2017/03/18/11/job_tweet.1489860003098.txt.tmp to /user/cloudera/output/handson_train/flume/techJobTweet/2017/03/18/11/job_tweet.1489860003098.txt
17/03/18 11:10:03 INFO hdfs.HDFSEventSink: Writer callback called.
17/03/18 11:10:04 INFO hdfs.HDFSDataStream: Serializer = TEXT, UseRawLocalFileSystem = false
17/03/18 11:10:04 INFO hdfs.BucketWriter: Creating /user/cloudera/output/handson_train/flume/techJobTweet/2017/03/18/11/job_tweet.1489860604484.txt.tmp
17/03/18 11:10:15 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 5
17/03/18 11:10:15 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850439142, queueSize: 0, queueHead: 100
17/03/18 11:10:15 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 8610459 logWriteOrderID: 1489850439142
17/03/18 11:10:45 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 11:10:45 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850439155, queueSize: 0, queueHead: 100
17/03/18 11:10:45 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 8626010 logWriteOrderID: 1489850439155
17/03/18 11:11:15 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 11:11:15 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850439163, queueSize: 0, queueHead: 100
17/03/18 11:11:15 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 8637154 logWriteOrderID: 1489850439163
17/03/18 11:11:45 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 11:11:45 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850439191, queueSize: 0, queueHead: 100
17/03/18 11:11:45 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 8669552 logWriteOrderID: 1489850439191
17/03/18 11:12:15 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 11:12:15 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850439212, queueSize: 0, queueHead: 100
17/03/18 11:12:15 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 8688568 logWriteOrderID: 1489850439212
17/03/18 11:12:45 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 11:12:45 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850439225, queueSize: 0, queueHead: 100
17/03/18 11:12:45 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 8704470 logWriteOrderID: 1489850439225
17/03/18 11:13:15 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 11:13:15 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850439234, queueSize: 0, queueHead: 100
17/03/18 11:13:15 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 8713880 logWriteOrderID: 1489850439234
17/03/18 11:13:45 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 11:13:46 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850439239, queueSize: 0, queueHead: 100
17/03/18 11:13:46 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 8718948 logWriteOrderID: 1489850439239
17/03/18 11:14:16 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 11:14:16 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850439251, queueSize: 0, queueHead: 100
17/03/18 11:14:16 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 8731717 logWriteOrderID: 1489850439251
17/03/18 11:14:46 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 11:14:46 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850439265, queueSize: 1, queueHead: 99
17/03/18 11:14:46 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 8756824 logWriteOrderID: 1489850439265
17/03/18 11:15:16 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 11:15:16 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850439291, queueSize: 0, queueHead: 100
17/03/18 11:15:16 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 8778436 logWriteOrderID: 1489850439291
17/03/18 11:15:46 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 11:15:46 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850439302, queueSize: 1, queueHead: 99
17/03/18 11:15:46 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 8799080 logWriteOrderID: 1489850439302
17/03/18 11:16:16 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 11:16:16 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850439332, queueSize: 0, queueHead: 100
17/03/18 11:16:16 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 8841462 logWriteOrderID: 1489850439332
17/03/18 11:16:46 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 11:16:46 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850439348, queueSize: 0, queueHead: 100
17/03/18 11:16:46 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 8864388 logWriteOrderID: 1489850439348
17/03/18 11:17:16 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 11:17:16 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850439369, queueSize: 0, queueHead: 100
17/03/18 11:17:16 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 8893978 logWriteOrderID: 1489850439369
17/03/18 11:17:46 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 11:17:46 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850439378, queueSize: 0, queueHead: 100
17/03/18 11:17:46 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 8909624 logWriteOrderID: 1489850439378
17/03/18 11:18:16 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 11:18:16 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850439387, queueSize: 0, queueHead: 100
17/03/18 11:18:16 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 8919621 logWriteOrderID: 1489850439387
17/03/18 11:18:46 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 11:18:46 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850439404, queueSize: 0, queueHead: 100
17/03/18 11:18:46 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 8941570 logWriteOrderID: 1489850439404
17/03/18 11:19:46 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 11:19:46 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850439413, queueSize: 0, queueHead: 100
17/03/18 11:19:46 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 8951700 logWriteOrderID: 1489850439413
17/03/18 11:20:04 INFO hdfs.BucketWriter: Closing /user/cloudera/output/handson_train/flume/techJobTweet/2017/03/18/11/job_tweet.1489860604484.txt.tmp
17/03/18 11:20:04 INFO hdfs.BucketWriter: Renaming /user/cloudera/output/handson_train/flume/techJobTweet/2017/03/18/11/job_tweet.1489860604484.txt.tmp to /user/cloudera/output/handson_train/flume/techJobTweet/2017/03/18/11/job_tweet.1489860604484.txt
17/03/18 11:20:04 INFO hdfs.HDFSEventSink: Writer callback called.
17/03/18 11:20:05 INFO hdfs.HDFSDataStream: Serializer = TEXT, UseRawLocalFileSystem = false
17/03/18 11:20:06 INFO hdfs.BucketWriter: Creating /user/cloudera/output/handson_train/flume/techJobTweet/2017/03/18/11/job_tweet.1489861205973.txt.tmp
17/03/18 11:20:16 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 11:20:16 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850439426, queueSize: 0, queueHead: 100
17/03/18 11:20:16 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 8967529 logWriteOrderID: 1489850439426
17/03/18 11:20:46 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 11:20:46 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850439441, queueSize: 1, queueHead: 99
17/03/18 11:20:46 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 8991902 logWriteOrderID: 1489850439441
17/03/18 11:21:16 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 11:21:16 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850439460, queueSize: 2, queueHead: 98
17/03/18 11:21:16 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 9027584 logWriteOrderID: 1489850439460
17/03/18 11:21:46 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 11:21:46 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850439468, queueSize: 0, queueHead: 100
17/03/18 11:21:46 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 9030198 logWriteOrderID: 1489850439468
17/03/18 11:22:16 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 11:22:16 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850439485, queueSize: 2, queueHead: 98
17/03/18 11:22:16 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 9053767 logWriteOrderID: 1489850439485
17/03/18 11:22:46 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 11:22:46 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850439501, queueSize: 0, queueHead: 100
17/03/18 11:22:46 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 9069399 logWriteOrderID: 1489850439501
17/03/18 11:23:16 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 11:23:16 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850439517, queueSize: 0, queueHead: 100
17/03/18 11:23:16 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 9092684 logWriteOrderID: 1489850439517
17/03/18 11:23:46 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 11:23:46 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850439528, queueSize: 1, queueHead: 99
17/03/18 11:23:46 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 9100864 logWriteOrderID: 1489850439528
17/03/18 11:24:16 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 11:24:16 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850439554, queueSize: 0, queueHead: 100
17/03/18 11:24:16 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 9128932 logWriteOrderID: 1489850439554
17/03/18 11:24:46 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 11:24:46 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850439571, queueSize: 0, queueHead: 100
17/03/18 11:24:46 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 9153169 logWriteOrderID: 1489850439571
17/03/18 11:25:16 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 11:25:16 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850439603, queueSize: 0, queueHead: 100
17/03/18 11:25:16 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 9199503 logWriteOrderID: 1489850439603
17/03/18 11:25:46 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 11:25:46 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850439614, queueSize: 1, queueHead: 99
17/03/18 11:25:46 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 9220003 logWriteOrderID: 1489850439614
17/03/18 11:26:16 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 3
17/03/18 11:26:16 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850439627, queueSize: 0, queueHead: 101
17/03/18 11:26:16 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 9239584 logWriteOrderID: 1489850439627
17/03/18 11:26:46 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 11:26:46 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850439662, queueSize: 0, queueHead: 101
17/03/18 11:26:47 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 9278783 logWriteOrderID: 1489850439662
17/03/18 11:27:17 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 3
17/03/18 11:27:17 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850439692, queueSize: 0, queueHead: 102
17/03/18 11:27:17 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 9344665 logWriteOrderID: 1489850439692
17/03/18 11:27:47 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 11:27:47 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850439697, queueSize: 0, queueHead: 102
17/03/18 11:27:47 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 9349445 logWriteOrderID: 1489850439697
17/03/18 11:28:17 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 3
17/03/18 11:28:17 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850439733, queueSize: 0, queueHead: 103
17/03/18 11:28:17 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 9399591 logWriteOrderID: 1489850439733
17/03/18 11:28:47 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 11:28:47 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850439753, queueSize: 0, queueHead: 103
17/03/18 11:28:47 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 9422818 logWriteOrderID: 1489850439753
17/03/18 11:29:17 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 4
17/03/18 11:29:17 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850439775, queueSize: 0, queueHead: 105
17/03/18 11:29:17 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 9462611 logWriteOrderID: 1489850439775
17/03/18 11:29:47 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 11:29:47 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850439802, queueSize: 0, queueHead: 105
17/03/18 11:29:47 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 9511551 logWriteOrderID: 1489850439802
17/03/18 11:30:06 INFO hdfs.BucketWriter: Closing /user/cloudera/output/handson_train/flume/techJobTweet/2017/03/18/11/job_tweet.1489861205973.txt.tmp
17/03/18 11:30:06 INFO hdfs.BucketWriter: Renaming /user/cloudera/output/handson_train/flume/techJobTweet/2017/03/18/11/job_tweet.1489861205973.txt.tmp to /user/cloudera/output/handson_train/flume/techJobTweet/2017/03/18/11/job_tweet.1489861205973.txt
17/03/18 11:30:06 INFO hdfs.HDFSEventSink: Writer callback called.
17/03/18 11:30:06 INFO hdfs.HDFSDataStream: Serializer = TEXT, UseRawLocalFileSystem = false
17/03/18 11:30:07 INFO hdfs.BucketWriter: Creating /user/cloudera/output/handson_train/flume/techJobTweet/2017/03/18/11/job_tweet.1489861806951.txt.tmp
17/03/18 11:30:17 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 4
17/03/18 11:30:17 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850439836, queueSize: 0, queueHead: 107
17/03/18 11:30:17 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 9556426 logWriteOrderID: 1489850439836
17/03/18 11:30:47 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 3
17/03/18 11:30:47 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850439870, queueSize: 0, queueHead: 108
17/03/18 11:30:47 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 9594554 logWriteOrderID: 1489850439870
17/03/18 11:31:17 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 3
17/03/18 11:31:17 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850439892, queueSize: 0, queueHead: 109
17/03/18 11:31:17 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 9624917 logWriteOrderID: 1489850439892
17/03/18 11:31:47 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 11:31:47 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850439923, queueSize: 0, queueHead: 109
17/03/18 11:31:47 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 9658110 logWriteOrderID: 1489850439923
17/03/18 11:32:17 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 11:32:17 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850439928, queueSize: 0, queueHead: 109
17/03/18 11:32:17 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 9665156 logWriteOrderID: 1489850439928
17/03/18 11:32:47 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 11:32:47 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850439933, queueSize: 0, queueHead: 109
17/03/18 11:32:47 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 9669879 logWriteOrderID: 1489850439933
17/03/18 11:33:17 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 3
17/03/18 11:33:17 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850439951, queueSize: 0, queueHead: 110
17/03/18 11:33:17 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 9698746 logWriteOrderID: 1489850439951
17/03/18 11:33:47 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 11:33:47 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850439960, queueSize: 0, queueHead: 110
17/03/18 11:33:47 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 9712689 logWriteOrderID: 1489850439960
17/03/18 11:34:17 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 11:34:17 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850439988, queueSize: 0, queueHead: 110
17/03/18 11:34:17 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 9751272 logWriteOrderID: 1489850439988
17/03/18 11:34:47 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 11:34:47 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850439997, queueSize: 0, queueHead: 110
17/03/18 11:34:47 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 9764531 logWriteOrderID: 1489850439997
17/03/18 11:35:17 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 11:35:17 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850440009, queueSize: 0, queueHead: 110
17/03/18 11:35:17 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 9775715 logWriteOrderID: 1489850440009
17/03/18 11:35:47 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 11:35:47 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850440014, queueSize: 0, queueHead: 110
17/03/18 11:35:47 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 9781003 logWriteOrderID: 1489850440014
17/03/18 11:36:17 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 3
17/03/18 11:36:17 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850440035, queueSize: 1, queueHead: 110
17/03/18 11:36:17 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 9803676 logWriteOrderID: 1489850440035
17/03/18 11:36:47 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 11:36:47 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850440052, queueSize: 1, queueHead: 110
17/03/18 11:36:47 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 9824799 logWriteOrderID: 1489850440052
17/03/18 11:37:17 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 11:37:17 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850440067, queueSize: 0, queueHead: 111
17/03/18 11:37:17 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 9843655 logWriteOrderID: 1489850440067
17/03/18 11:37:47 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 11:37:47 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850440076, queueSize: 0, queueHead: 111
17/03/18 11:37:47 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 9851414 logWriteOrderID: 1489850440076
17/03/18 11:38:17 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 11:38:17 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850440093, queueSize: 0, queueHead: 111
17/03/18 11:38:17 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 9874162 logWriteOrderID: 1489850440093
17/03/18 11:38:47 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 11:38:47 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850440109, queueSize: 0, queueHead: 111
17/03/18 11:38:47 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 9896845 logWriteOrderID: 1489850440109
17/03/18 11:39:17 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 11:39:17 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850440126, queueSize: 0, queueHead: 111
17/03/18 11:39:17 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 9917985 logWriteOrderID: 1489850440126
17/03/18 11:39:47 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 11:39:47 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850440139, queueSize: 0, queueHead: 111
17/03/18 11:39:47 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 9943409 logWriteOrderID: 1489850440139
17/03/18 11:40:07 INFO hdfs.BucketWriter: Closing /user/cloudera/output/handson_train/flume/techJobTweet/2017/03/18/11/job_tweet.1489861806951.txt.tmp
17/03/18 11:40:07 INFO hdfs.BucketWriter: Renaming /user/cloudera/output/handson_train/flume/techJobTweet/2017/03/18/11/job_tweet.1489861806951.txt.tmp to /user/cloudera/output/handson_train/flume/techJobTweet/2017/03/18/11/job_tweet.1489861806951.txt
17/03/18 11:40:07 INFO hdfs.HDFSEventSink: Writer callback called.
17/03/18 11:40:17 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 11:40:17 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850440148, queueSize: 0, queueHead: 111
17/03/18 11:40:18 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 9962182 logWriteOrderID: 1489850440148
17/03/18 11:40:29 INFO hdfs.HDFSDataStream: Serializer = TEXT, UseRawLocalFileSystem = false
17/03/18 11:40:29 INFO hdfs.BucketWriter: Creating /user/cloudera/output/handson_train/flume/techJobTweet/2017/03/18/11/job_tweet.1489862429457.txt.tmp
17/03/18 11:40:48 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 11:40:48 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850440157, queueSize: 0, queueHead: 111
17/03/18 11:40:48 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 9978452 logWriteOrderID: 1489850440157
17/03/18 11:41:18 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 11:41:18 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850440177, queueSize: 0, queueHead: 111
17/03/18 11:41:18 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 10014128 logWriteOrderID: 1489850440177
17/03/18 11:41:48 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 11:41:48 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850440197, queueSize: 0, queueHead: 111
17/03/18 11:41:48 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 10036805 logWriteOrderID: 1489850440197
17/03/18 11:42:18 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 11:42:18 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850440206, queueSize: 0, queueHead: 111
17/03/18 11:42:18 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 10049305 logWriteOrderID: 1489850440206
17/03/18 11:42:48 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 11:42:48 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850440211, queueSize: 0, queueHead: 111
17/03/18 11:42:48 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 10057312 logWriteOrderID: 1489850440211
17/03/18 11:43:18 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 11:43:18 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850440216, queueSize: 0, queueHead: 111
17/03/18 11:43:18 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 10062620 logWriteOrderID: 1489850440216
17/03/18 11:43:48 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 11:43:48 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850440228, queueSize: 0, queueHead: 111
17/03/18 11:43:48 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 10078941 logWriteOrderID: 1489850440228
17/03/18 11:44:18 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 3
17/03/18 11:44:18 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850440247, queueSize: 0, queueHead: 112
17/03/18 11:44:18 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 10102011 logWriteOrderID: 1489850440247
17/03/18 11:44:48 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 11:44:48 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850440256, queueSize: 0, queueHead: 112
17/03/18 11:44:48 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 10111507 logWriteOrderID: 1489850440256
17/03/18 11:45:18 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 3
17/03/18 11:45:18 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850440284, queueSize: 1, queueHead: 112
17/03/18 11:45:18 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 10158509 logWriteOrderID: 1489850440284
17/03/18 11:45:48 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 11:45:48 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850440321, queueSize: 0, queueHead: 113
17/03/18 11:45:48 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 10204163 logWriteOrderID: 1489850440321
17/03/18 11:46:18 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 11:46:18 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850440326, queueSize: 0, queueHead: 113
17/03/18 11:46:18 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 10211749 logWriteOrderID: 1489850440326
17/03/18 11:46:48 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 11:46:48 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850440331, queueSize: 0, queueHead: 113
17/03/18 11:46:48 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 10216807 logWriteOrderID: 1489850440331
17/03/18 11:47:18 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 11:47:18 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850440352, queueSize: 0, queueHead: 113
17/03/18 11:47:18 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 10243403 logWriteOrderID: 1489850440352
17/03/18 11:47:48 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 11:47:48 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850440365, queueSize: 0, queueHead: 113
17/03/18 11:47:48 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 10258474 logWriteOrderID: 1489850440365
17/03/18 11:48:18 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 11:48:18 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850440368, queueSize: 1, queueHead: 112
17/03/18 11:48:18 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 10268947 logWriteOrderID: 1489850440368
17/03/18 11:48:48 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 11:48:48 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850440386, queueSize: 0, queueHead: 113
17/03/18 11:48:48 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 10283617 logWriteOrderID: 1489850440386
17/03/18 11:49:18 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 11:49:18 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850440395, queueSize: 0, queueHead: 113
17/03/18 11:49:18 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 10295075 logWriteOrderID: 1489850440395
17/03/18 11:49:48 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 11:49:48 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850440416, queueSize: 1, queueHead: 112
17/03/18 11:49:48 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 10322651 logWriteOrderID: 1489850440416
17/03/18 11:50:18 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 11:50:18 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850440446, queueSize: 0, queueHead: 113
17/03/18 11:50:18 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 10343881 logWriteOrderID: 1489850440446
17/03/18 11:50:29 INFO hdfs.BucketWriter: Closing /user/cloudera/output/handson_train/flume/techJobTweet/2017/03/18/11/job_tweet.1489862429457.txt.tmp
17/03/18 11:50:30 INFO hdfs.BucketWriter: Renaming /user/cloudera/output/handson_train/flume/techJobTweet/2017/03/18/11/job_tweet.1489862429457.txt.tmp to /user/cloudera/output/handson_train/flume/techJobTweet/2017/03/18/11/job_tweet.1489862429457.txt
17/03/18 11:50:30 INFO hdfs.HDFSEventSink: Writer callback called.
17/03/18 11:50:34 INFO hdfs.HDFSDataStream: Serializer = TEXT, UseRawLocalFileSystem = false
17/03/18 11:50:34 INFO hdfs.BucketWriter: Creating /user/cloudera/output/handson_train/flume/techJobTweet/2017/03/18/11/job_tweet.1489863034020.txt.tmp
17/03/18 11:50:48 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 11:50:48 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850440468, queueSize: 1, queueHead: 112
17/03/18 11:50:48 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 10375040 logWriteOrderID: 1489850440468
17/03/18 11:51:18 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 11:51:18 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850440487, queueSize: 0, queueHead: 113
17/03/18 11:51:18 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 10389658 logWriteOrderID: 1489850440487
17/03/18 11:51:48 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 11:51:48 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850440512, queueSize: 0, queueHead: 113
17/03/18 11:51:48 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 10416149 logWriteOrderID: 1489850440512
17/03/18 11:52:18 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 11:52:18 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850440523, queueSize: 1, queueHead: 112
17/03/18 11:52:18 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 10427770 logWriteOrderID: 1489850440523
17/03/18 11:52:48 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 11:52:48 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850440542, queueSize: 0, queueHead: 113
17/03/18 11:52:48 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 10451134 logWriteOrderID: 1489850440542
17/03/18 11:53:18 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 11:53:18 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850440559, queueSize: 0, queueHead: 113
17/03/18 11:53:18 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 10480718 logWriteOrderID: 1489850440559
17/03/18 11:53:48 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 11:53:48 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850440568, queueSize: 0, queueHead: 113
17/03/18 11:53:48 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 10492149 logWriteOrderID: 1489850440568
17/03/18 11:54:18 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 11:54:18 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850440584, queueSize: 0, queueHead: 113
17/03/18 11:54:18 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 10524862 logWriteOrderID: 1489850440584
17/03/18 11:54:48 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 11:54:48 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850440596, queueSize: 0, queueHead: 113
17/03/18 11:54:49 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 10537516 logWriteOrderID: 1489850440596
17/03/18 11:55:19 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 11:55:19 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850440609, queueSize: 0, queueHead: 113
17/03/18 11:55:19 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 10547973 logWriteOrderID: 1489850440609
17/03/18 11:55:49 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 3
17/03/18 11:55:49 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850440620, queueSize: 3, queueHead: 111
17/03/18 11:55:49 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 10568029 logWriteOrderID: 1489850440620
17/03/18 11:56:19 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 3
17/03/18 11:56:19 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850440643, queueSize: 1, queueHead: 113
17/03/18 11:56:19 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 10590543 logWriteOrderID: 1489850440643
17/03/18 11:56:49 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 11:56:49 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850440654, queueSize: 0, queueHead: 114
17/03/18 11:56:49 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 10599181 logWriteOrderID: 1489850440654
17/03/18 11:57:19 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 11:57:19 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850440671, queueSize: 0, queueHead: 114
17/03/18 11:57:19 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 10616553 logWriteOrderID: 1489850440671
17/03/18 11:57:49 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 3
17/03/18 11:57:49 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850440694, queueSize: 0, queueHead: 115
17/03/18 11:57:49 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 10639027 logWriteOrderID: 1489850440694
17/03/18 11:58:19 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 11:58:19 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850440710, queueSize: 0, queueHead: 115
17/03/18 11:58:19 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 10654833 logWriteOrderID: 1489850440710
17/03/18 11:58:49 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 11:58:49 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850440719, queueSize: 0, queueHead: 115
17/03/18 11:58:49 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 10664335 logWriteOrderID: 1489850440719
17/03/18 11:59:19 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 11:59:19 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850440731, queueSize: 0, queueHead: 115
17/03/18 11:59:19 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 10673604 logWriteOrderID: 1489850440731
17/03/18 12:00:02 INFO hdfs.HDFSDataStream: Serializer = TEXT, UseRawLocalFileSystem = false
17/03/18 12:00:02 INFO hdfs.BucketWriter: Creating /user/cloudera/output/handson_train/flume/techJobTweet/2017/03/18/12/job_tweet.1489863602517.txt.tmp
17/03/18 12:00:19 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 6
17/03/18 12:00:19 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850440789, queueSize: 0, queueHead: 119
17/03/18 12:00:19 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 10740192 logWriteOrderID: 1489850440789
17/03/18 12:00:34 INFO hdfs.BucketWriter: Closing /user/cloudera/output/handson_train/flume/techJobTweet/2017/03/18/11/job_tweet.1489863034020.txt.tmp
17/03/18 12:00:34 INFO hdfs.BucketWriter: Renaming /user/cloudera/output/handson_train/flume/techJobTweet/2017/03/18/11/job_tweet.1489863034020.txt.tmp to /user/cloudera/output/handson_train/flume/techJobTweet/2017/03/18/11/job_tweet.1489863034020.txt
17/03/18 12:00:34 INFO hdfs.HDFSEventSink: Writer callback called.
17/03/18 12:00:49 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 12:00:49 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850440810, queueSize: 0, queueHead: 119
17/03/18 12:00:49 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 10760604 logWriteOrderID: 1489850440810
17/03/18 12:01:19 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 12:01:19 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850440830, queueSize: 0, queueHead: 119
17/03/18 12:01:19 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 10778296 logWriteOrderID: 1489850440830
17/03/18 12:01:49 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 12:01:49 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850440849, queueSize: 0, queueHead: 119
17/03/18 12:01:49 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 10800765 logWriteOrderID: 1489850440849
17/03/18 12:02:19 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 12:02:19 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850440861, queueSize: 0, queueHead: 119
17/03/18 12:02:19 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 10816687 logWriteOrderID: 1489850440861
17/03/18 12:03:19 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 12:03:19 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850440869, queueSize: 0, queueHead: 119
17/03/18 12:03:19 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 10821400 logWriteOrderID: 1489850440869
17/03/18 12:03:49 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 2
17/03/18 12:03:49 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850440881, queueSize: 0, queueHead: 119
17/03/18 12:03:49 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 10834382 logWriteOrderID: 1489850440881
17/03/18 12:04:19 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 3
17/03/18 12:04:19 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850440908, queueSize: 0, queueHead: 120
17/03/18 12:04:19 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 10857977 logWriteOrderID: 1489850440908
17/03/18 12:04:49 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 1
17/03/18 12:04:49 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850440925, queueSize: 0, queueHead: 120
17/03/18 12:04:49 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 10876003 logWriteOrderID: 1489850440925
17/03/18 12:04:58 WARN hdfs.DFSClient: DFSOutputStream ResponseProcessor exception  for block BP-1288191314-127.0.0.1-1470859564889:blk_1073743289_2469
java.io.EOFException: Premature EOF: no length prefix available
	at org.apache.hadoop.hdfs.protocolPB.PBHelper.vintPrefixed(PBHelper.java:2243)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck.readFields(PipelineAck.java:235)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer$ResponseProcessor.run(DFSOutputStream.java:986)
17/03/18 12:05:00 ERROR hdfs.AbstractHDFSWriter: Unexpected error while checking replication factor
java.lang.reflect.InvocationTargetException
	at sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.flume.sink.hdfs.AbstractHDFSWriter.getNumCurrentReplicas(AbstractHDFSWriter.java:165)
	at org.apache.flume.sink.hdfs.AbstractHDFSWriter.isUnderReplicated(AbstractHDFSWriter.java:84)
	at org.apache.flume.sink.hdfs.BucketWriter.shouldRotate(BucketWriter.java:583)
	at org.apache.flume.sink.hdfs.BucketWriter.append(BucketWriter.java:518)
	at org.apache.flume.sink.hdfs.HDFSEventSink.process(HDFSEventSink.java:418)
	at org.apache.flume.sink.DefaultSinkProcessor.process(DefaultSinkProcessor.java:68)
	at org.apache.flume.SinkRunner$PollingRunner.run(SinkRunner.java:147)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: All datanodes DatanodeInfoWithStorage[10.0.2.15:50010,DS-5dd411ac-a868-4285-8cbf-bbac9879a2ed,DISK] are bad. Aborting...
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1386)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:1147)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:632)
17/03/18 12:05:00 WARN hdfs.BucketWriter: Caught IOException writing to HDFSWriter (All datanodes DatanodeInfoWithStorage[10.0.2.15:50010,DS-5dd411ac-a868-4285-8cbf-bbac9879a2ed,DISK] are bad. Aborting...). Closing file (/user/cloudera/output/handson_train/flume/techJobTweet/2017/03/18/12/job_tweet.1489863602517.txt.tmp) and rethrowing exception.
17/03/18 12:05:00 INFO hdfs.BucketWriter: Closing /user/cloudera/output/handson_train/flume/techJobTweet/2017/03/18/12/job_tweet.1489863602517.txt.tmp
17/03/18 12:05:00 ERROR hdfs.AbstractHDFSWriter: Error while trying to hflushOrSync!
17/03/18 12:05:00 WARN hdfs.BucketWriter: failed to close() HDFSWriter for file (/user/cloudera/output/handson_train/flume/techJobTweet/2017/03/18/12/job_tweet.1489863602517.txt.tmp). Exception follows.
java.io.IOException: All datanodes DatanodeInfoWithStorage[10.0.2.15:50010,DS-5dd411ac-a868-4285-8cbf-bbac9879a2ed,DISK] are bad. Aborting...
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1386)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:1147)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:632)
17/03/18 12:05:00 INFO hdfs.BucketWriter: Renaming /user/cloudera/output/handson_train/flume/techJobTweet/2017/03/18/12/job_tweet.1489863602517.txt.tmp to /user/cloudera/output/handson_train/flume/techJobTweet/2017/03/18/12/job_tweet.1489863602517.txt
17/03/18 12:05:00 INFO hdfs.HDFSEventSink: Writer callback called.
17/03/18 12:05:00 WARN hdfs.HDFSEventSink: HDFS IO error
java.io.IOException: All datanodes DatanodeInfoWithStorage[10.0.2.15:50010,DS-5dd411ac-a868-4285-8cbf-bbac9879a2ed,DISK] are bad. Aborting...
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1386)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:1147)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:632)
17/03/18 12:05:05 INFO hdfs.HDFSDataStream: Serializer = TEXT, UseRawLocalFileSystem = false
17/03/18 12:05:05 INFO hdfs.BucketWriter: Creating /user/cloudera/output/handson_train/flume/techJobTweet/2017/03/18/12/job_tweet.1489863905527.txt.tmp
17/03/18 12:05:05 INFO hdfs.DFSClient: Exception in createBlockOutputStream
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.hdfs.DFSOutputStream.createSocketForPipeline(DFSOutputStream.java:1845)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.createBlockOutputStream(DFSOutputStream.java:1588)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1541)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:683)
17/03/18 12:05:05 INFO hdfs.DFSClient: Abandoning BP-1288191314-127.0.0.1-1470859564889:blk_1073743292_2472
17/03/18 12:05:05 INFO hdfs.DFSClient: Excluding datanode DatanodeInfoWithStorage[10.0.2.15:50010,DS-5dd411ac-a868-4285-8cbf-bbac9879a2ed,DISK]
17/03/18 12:05:05 WARN hdfs.DFSClient: DataStreamer Exception
org.apache.hadoop.ipc.RemoteException(java.io.IOException): File /user/cloudera/output/handson_train/flume/techJobTweet/2017/03/18/12/job_tweet.1489863905527.txt.tmp could only be replicated to 0 nodes instead of minReplication (=1).  There are 1 datanode(s) running and 1 node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1610)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3315)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:679)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:214)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:489)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:617)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1073)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2086)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2082)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1693)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2080)

	at org.apache.hadoop.ipc.Client.call(Client.java:1471)
	at org.apache.hadoop.ipc.Client.call(Client.java:1408)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:230)
	at com.sun.proxy.$Proxy16.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:409)
	at sun.reflect.GeneratedMethodAccessor9.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:256)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:104)
	at com.sun.proxy.$Proxy17.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1733)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1529)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:683)
17/03/18 12:05:05 WARN hdfs.DFSClient: Error while syncing
org.apache.hadoop.ipc.RemoteException(java.io.IOException): File /user/cloudera/output/handson_train/flume/techJobTweet/2017/03/18/12/job_tweet.1489863905527.txt.tmp could only be replicated to 0 nodes instead of minReplication (=1).  There are 1 datanode(s) running and 1 node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1610)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3315)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:679)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:214)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:489)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:617)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1073)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2086)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2082)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1693)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2080)

	at org.apache.hadoop.ipc.Client.call(Client.java:1471)
	at org.apache.hadoop.ipc.Client.call(Client.java:1408)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:230)
	at com.sun.proxy.$Proxy16.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:409)
	at sun.reflect.GeneratedMethodAccessor9.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:256)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:104)
	at com.sun.proxy.$Proxy17.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1733)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1529)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:683)
17/03/18 12:05:05 ERROR hdfs.AbstractHDFSWriter: Error while trying to hflushOrSync!
17/03/18 12:05:05 WARN hdfs.HDFSEventSink: HDFS IO error
org.apache.hadoop.ipc.RemoteException(java.io.IOException): File /user/cloudera/output/handson_train/flume/techJobTweet/2017/03/18/12/job_tweet.1489863905527.txt.tmp could only be replicated to 0 nodes instead of minReplication (=1).  There are 1 datanode(s) running and 1 node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1610)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3315)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:679)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:214)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:489)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:617)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1073)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2086)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2082)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1693)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2080)

	at org.apache.hadoop.ipc.Client.call(Client.java:1471)
	at org.apache.hadoop.ipc.Client.call(Client.java:1408)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:230)
	at com.sun.proxy.$Proxy16.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:409)
	at sun.reflect.GeneratedMethodAccessor9.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:256)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:104)
	at com.sun.proxy.$Proxy17.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1733)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1529)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:683)
17/03/18 12:05:10 ERROR hdfs.AbstractHDFSWriter: Unexpected error while checking replication factor
java.lang.reflect.InvocationTargetException
	at sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.flume.sink.hdfs.AbstractHDFSWriter.getNumCurrentReplicas(AbstractHDFSWriter.java:165)
	at org.apache.flume.sink.hdfs.AbstractHDFSWriter.isUnderReplicated(AbstractHDFSWriter.java:84)
	at org.apache.flume.sink.hdfs.BucketWriter.shouldRotate(BucketWriter.java:583)
	at org.apache.flume.sink.hdfs.BucketWriter.append(BucketWriter.java:518)
	at org.apache.flume.sink.hdfs.HDFSEventSink.process(HDFSEventSink.java:418)
	at org.apache.flume.sink.DefaultSinkProcessor.process(DefaultSinkProcessor.java:68)
	at org.apache.flume.SinkRunner$PollingRunner.run(SinkRunner.java:147)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.hadoop.ipc.RemoteException(java.io.IOException): File /user/cloudera/output/handson_train/flume/techJobTweet/2017/03/18/12/job_tweet.1489863905527.txt.tmp could only be replicated to 0 nodes instead of minReplication (=1).  There are 1 datanode(s) running and 1 node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1610)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3315)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:679)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:214)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:489)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:617)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1073)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2086)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2082)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1693)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2080)

	at org.apache.hadoop.ipc.Client.call(Client.java:1471)
	at org.apache.hadoop.ipc.Client.call(Client.java:1408)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:230)
	at com.sun.proxy.$Proxy16.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:409)
	at sun.reflect.GeneratedMethodAccessor9.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:256)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:104)
	at com.sun.proxy.$Proxy17.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1733)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1529)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:683)
17/03/18 12:05:10 WARN hdfs.BucketWriter: Caught IOException writing to HDFSWriter (File /user/cloudera/output/handson_train/flume/techJobTweet/2017/03/18/12/job_tweet.1489863905527.txt.tmp could only be replicated to 0 nodes instead of minReplication (=1).  There are 1 datanode(s) running and 1 node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1610)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3315)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:679)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:214)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:489)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:617)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1073)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2086)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2082)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1693)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2080)
). Closing file (/user/cloudera/output/handson_train/flume/techJobTweet/2017/03/18/12/job_tweet.1489863905527.txt.tmp) and rethrowing exception.
17/03/18 12:05:10 ERROR hdfs.AbstractHDFSWriter: Error while trying to hflushOrSync!
17/03/18 12:05:10 WARN hdfs.BucketWriter: pre-close flush failed
org.apache.hadoop.ipc.RemoteException(java.io.IOException): File /user/cloudera/output/handson_train/flume/techJobTweet/2017/03/18/12/job_tweet.1489863905527.txt.tmp could only be replicated to 0 nodes instead of minReplication (=1).  There are 1 datanode(s) running and 1 node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1610)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3315)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:679)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:214)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:489)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:617)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1073)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2086)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2082)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1693)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2080)

	at org.apache.hadoop.ipc.Client.call(Client.java:1471)
	at org.apache.hadoop.ipc.Client.call(Client.java:1408)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:230)
	at com.sun.proxy.$Proxy16.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:409)
	at sun.reflect.GeneratedMethodAccessor9.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:256)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:104)
	at com.sun.proxy.$Proxy17.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1733)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1529)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:683)
17/03/18 12:05:10 INFO hdfs.BucketWriter: Closing /user/cloudera/output/handson_train/flume/techJobTweet/2017/03/18/12/job_tweet.1489863905527.txt.tmp
17/03/18 12:05:10 ERROR hdfs.AbstractHDFSWriter: Error while trying to hflushOrSync!
17/03/18 12:05:10 WARN hdfs.BucketWriter: failed to close() HDFSWriter for file (/user/cloudera/output/handson_train/flume/techJobTweet/2017/03/18/12/job_tweet.1489863905527.txt.tmp). Exception follows.
org.apache.hadoop.ipc.RemoteException(java.io.IOException): File /user/cloudera/output/handson_train/flume/techJobTweet/2017/03/18/12/job_tweet.1489863905527.txt.tmp could only be replicated to 0 nodes instead of minReplication (=1).  There are 1 datanode(s) running and 1 node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1610)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3315)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:679)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:214)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:489)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:617)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1073)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2086)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2082)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1693)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2080)

	at org.apache.hadoop.ipc.Client.call(Client.java:1471)
	at org.apache.hadoop.ipc.Client.call(Client.java:1408)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:230)
	at com.sun.proxy.$Proxy16.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:409)
	at sun.reflect.GeneratedMethodAccessor9.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:256)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:104)
	at com.sun.proxy.$Proxy17.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1733)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1529)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:683)
17/03/18 12:05:10 WARN hdfs.BucketWriter: failed to rename() file (/user/cloudera/output/handson_train/flume/techJobTweet/2017/03/18/12/job_tweet.1489863905527.txt.tmp). Exception follows.
java.io.EOFException: End of File Exception between local host is: "quickstart.cloudera/10.0.2.15"; destination host is: "quickstart.cloudera":8020; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:764)
	at org.apache.hadoop.ipc.Client.call(Client.java:1475)
	at org.apache.hadoop.ipc.Client.call(Client.java:1408)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:230)
	at com.sun.proxy.$Proxy16.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:762)
	at sun.reflect.GeneratedMethodAccessor12.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:256)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:104)
	at com.sun.proxy.$Proxy17.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:2102)
	at org.apache.hadoop.hdfs.DistributedFileSystem$19.doCall(DistributedFileSystem.java:1215)
	at org.apache.hadoop.hdfs.DistributedFileSystem$19.doCall(DistributedFileSystem.java:1211)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1211)
	at org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1412)
	at org.apache.flume.sink.hdfs.BucketWriter$8.call(BucketWriter.java:628)
	at org.apache.flume.sink.hdfs.BucketWriter$8.call(BucketWriter.java:625)
	at org.apache.flume.sink.hdfs.BucketWriter$9$1.run(BucketWriter.java:679)
	at org.apache.flume.auth.SimpleAuthenticator.execute(SimpleAuthenticator.java:50)
	at org.apache.flume.sink.hdfs.BucketWriter$9.call(BucketWriter.java:676)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1080)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:975)
17/03/18 12:05:10 INFO hdfs.HDFSEventSink: Writer callback called.
17/03/18 12:05:10 WARN hdfs.HDFSEventSink: HDFS IO error
org.apache.hadoop.ipc.RemoteException(java.io.IOException): File /user/cloudera/output/handson_train/flume/techJobTweet/2017/03/18/12/job_tweet.1489863905527.txt.tmp could only be replicated to 0 nodes instead of minReplication (=1).  There are 1 datanode(s) running and 1 node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1610)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3315)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:679)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:214)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:489)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:617)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1073)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2086)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2082)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1693)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2080)

	at org.apache.hadoop.ipc.Client.call(Client.java:1471)
	at org.apache.hadoop.ipc.Client.call(Client.java:1408)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:230)
	at com.sun.proxy.$Proxy16.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:409)
	at sun.reflect.GeneratedMethodAccessor9.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:256)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:104)
	at com.sun.proxy.$Proxy17.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1733)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1529)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:683)
17/03/18 12:05:15 INFO hdfs.HDFSDataStream: Serializer = TEXT, UseRawLocalFileSystem = false
17/03/18 12:05:16 INFO hdfs.BucketWriter: Creating /user/cloudera/output/handson_train/flume/techJobTweet/2017/03/18/12/job_tweet.1489863915970.txt.tmp
17/03/18 12:05:16 WARN hdfs.HDFSEventSink: HDFS IO error
java.net.ConnectException: Call From quickstart.cloudera/10.0.2.15 to quickstart.cloudera:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:731)
	at org.apache.hadoop.ipc.Client.call(Client.java:1475)
	at org.apache.hadoop.ipc.Client.call(Client.java:1408)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:230)
	at com.sun.proxy.$Proxy16.create(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.create(ClientNamenodeProtocolTranslatorPB.java:297)
	at sun.reflect.GeneratedMethodAccessor8.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:256)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:104)
	at com.sun.proxy.$Proxy17.create(Unknown Source)
	at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:1965)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1738)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1663)
	at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:405)
	at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:401)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:401)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:344)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:920)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:901)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:798)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:787)
	at org.apache.flume.sink.hdfs.HDFSDataStream.doOpen(HDFSDataStream.java:86)
	at org.apache.flume.sink.hdfs.HDFSDataStream.open(HDFSDataStream.java:113)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:246)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:235)
	at org.apache.flume.sink.hdfs.BucketWriter$9$1.run(BucketWriter.java:679)
	at org.apache.flume.auth.SimpleAuthenticator.execute(SimpleAuthenticator.java:50)
	at org.apache.flume.sink.hdfs.BucketWriter$9.call(BucketWriter.java:676)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:713)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1524)
	at org.apache.hadoop.ipc.Client.call(Client.java:1447)
	... 33 more
17/03/18 12:05:19 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 6
17/03/18 12:05:19 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850440947, queueSize: 6, queueHead: 118
17/03/18 12:05:19 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 10894215 logWriteOrderID: 1489850440947
17/03/18 12:05:21 INFO hdfs.BucketWriter: Creating /user/cloudera/output/handson_train/flume/techJobTweet/2017/03/18/12/job_tweet.1489863915971.txt.tmp
17/03/18 12:05:21 WARN hdfs.HDFSEventSink: HDFS IO error
java.net.ConnectException: Call From quickstart.cloudera/10.0.2.15 to quickstart.cloudera:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:731)
	at org.apache.hadoop.ipc.Client.call(Client.java:1475)
	at org.apache.hadoop.ipc.Client.call(Client.java:1408)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:230)
	at com.sun.proxy.$Proxy16.create(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.create(ClientNamenodeProtocolTranslatorPB.java:297)
	at sun.reflect.GeneratedMethodAccessor8.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:256)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:104)
	at com.sun.proxy.$Proxy17.create(Unknown Source)
	at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:1965)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1738)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1663)
	at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:405)
	at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:401)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:401)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:344)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:920)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:901)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:798)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:787)
	at org.apache.flume.sink.hdfs.HDFSDataStream.doOpen(HDFSDataStream.java:86)
	at org.apache.flume.sink.hdfs.HDFSDataStream.open(HDFSDataStream.java:113)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:246)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:235)
	at org.apache.flume.sink.hdfs.BucketWriter$9$1.run(BucketWriter.java:679)
	at org.apache.flume.auth.SimpleAuthenticator.execute(SimpleAuthenticator.java:50)
	at org.apache.flume.sink.hdfs.BucketWriter$9.call(BucketWriter.java:676)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:713)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1524)
	at org.apache.hadoop.ipc.Client.call(Client.java:1447)
	... 33 more
17/03/18 12:05:26 INFO hdfs.BucketWriter: Creating /user/cloudera/output/handson_train/flume/techJobTweet/2017/03/18/12/job_tweet.1489863915972.txt.tmp
17/03/18 12:05:26 WARN hdfs.HDFSEventSink: HDFS IO error
java.net.ConnectException: Call From quickstart.cloudera/10.0.2.15 to quickstart.cloudera:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:731)
	at org.apache.hadoop.ipc.Client.call(Client.java:1475)
	at org.apache.hadoop.ipc.Client.call(Client.java:1408)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:230)
	at com.sun.proxy.$Proxy16.create(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.create(ClientNamenodeProtocolTranslatorPB.java:297)
	at sun.reflect.GeneratedMethodAccessor8.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:256)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:104)
	at com.sun.proxy.$Proxy17.create(Unknown Source)
	at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:1965)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1738)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1663)
	at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:405)
	at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:401)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:401)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:344)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:920)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:901)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:798)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:787)
	at org.apache.flume.sink.hdfs.HDFSDataStream.doOpen(HDFSDataStream.java:86)
	at org.apache.flume.sink.hdfs.HDFSDataStream.open(HDFSDataStream.java:113)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:246)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:235)
	at org.apache.flume.sink.hdfs.BucketWriter$9$1.run(BucketWriter.java:679)
	at org.apache.flume.auth.SimpleAuthenticator.execute(SimpleAuthenticator.java:50)
	at org.apache.flume.sink.hdfs.BucketWriter$9.call(BucketWriter.java:676)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:713)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1524)
	at org.apache.hadoop.ipc.Client.call(Client.java:1447)
	... 33 more
17/03/18 12:05:31 WARN hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1243578604_29] for 30 seconds.  Will retry shortly ...
java.net.ConnectException: Call From quickstart.cloudera/10.0.2.15 to quickstart.cloudera:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:731)
	at org.apache.hadoop.ipc.Client.call(Client.java:1475)
	at org.apache.hadoop.ipc.Client.call(Client.java:1408)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:230)
	at com.sun.proxy.$Proxy16.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease(ClientNamenodeProtocolTranslatorPB.java:581)
	at sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:256)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:104)
	at com.sun.proxy.$Proxy17.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:922)
	at org.apache.hadoop.hdfs.LeaseRenewer.renew(LeaseRenewer.java:423)
	at org.apache.hadoop.hdfs.LeaseRenewer.run(LeaseRenewer.java:448)
	at org.apache.hadoop.hdfs.LeaseRenewer.access$700(LeaseRenewer.java:71)
	at org.apache.hadoop.hdfs.LeaseRenewer$1.run(LeaseRenewer.java:304)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:713)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1524)
	at org.apache.hadoop.ipc.Client.call(Client.java:1447)
	... 16 more
17/03/18 12:05:31 INFO hdfs.BucketWriter: Creating /user/cloudera/output/handson_train/flume/techJobTweet/2017/03/18/12/job_tweet.1489863915973.txt.tmp
17/03/18 12:05:32 WARN hdfs.HDFSEventSink: HDFS IO error
java.net.ConnectException: Call From quickstart.cloudera/10.0.2.15 to quickstart.cloudera:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:731)
	at org.apache.hadoop.ipc.Client.call(Client.java:1475)
	at org.apache.hadoop.ipc.Client.call(Client.java:1408)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:230)
	at com.sun.proxy.$Proxy16.create(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.create(ClientNamenodeProtocolTranslatorPB.java:297)
	at sun.reflect.GeneratedMethodAccessor8.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:256)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:104)
	at com.sun.proxy.$Proxy17.create(Unknown Source)
	at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:1965)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1738)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1663)
	at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:405)
	at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:401)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:401)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:344)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:920)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:901)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:798)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:787)
	at org.apache.flume.sink.hdfs.HDFSDataStream.doOpen(HDFSDataStream.java:86)
	at org.apache.flume.sink.hdfs.HDFSDataStream.open(HDFSDataStream.java:113)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:246)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:235)
	at org.apache.flume.sink.hdfs.BucketWriter$9$1.run(BucketWriter.java:679)
	at org.apache.flume.auth.SimpleAuthenticator.execute(SimpleAuthenticator.java:50)
	at org.apache.flume.sink.hdfs.BucketWriter$9.call(BucketWriter.java:676)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:713)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1524)
	at org.apache.hadoop.ipc.Client.call(Client.java:1447)
	... 33 more
17/03/18 12:05:32 WARN hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1243578604_29] for 31 seconds.  Will retry shortly ...
java.net.ConnectException: Call From quickstart.cloudera/10.0.2.15 to quickstart.cloudera:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:731)
	at org.apache.hadoop.ipc.Client.call(Client.java:1475)
	at org.apache.hadoop.ipc.Client.call(Client.java:1408)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:230)
	at com.sun.proxy.$Proxy16.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease(ClientNamenodeProtocolTranslatorPB.java:581)
	at sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:256)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:104)
	at com.sun.proxy.$Proxy17.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:922)
	at org.apache.hadoop.hdfs.LeaseRenewer.renew(LeaseRenewer.java:423)
	at org.apache.hadoop.hdfs.LeaseRenewer.run(LeaseRenewer.java:448)
	at org.apache.hadoop.hdfs.LeaseRenewer.access$700(LeaseRenewer.java:71)
	at org.apache.hadoop.hdfs.LeaseRenewer$1.run(LeaseRenewer.java:304)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:713)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1524)
	at org.apache.hadoop.ipc.Client.call(Client.java:1447)
	... 16 more
17/03/18 12:05:33 WARN hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1243578604_29] for 32 seconds.  Will retry shortly ...
java.net.ConnectException: Call From quickstart.cloudera/10.0.2.15 to quickstart.cloudera:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:731)
	at org.apache.hadoop.ipc.Client.call(Client.java:1475)
	at org.apache.hadoop.ipc.Client.call(Client.java:1408)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:230)
	at com.sun.proxy.$Proxy16.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease(ClientNamenodeProtocolTranslatorPB.java:581)
	at sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:256)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:104)
	at com.sun.proxy.$Proxy17.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:922)
	at org.apache.hadoop.hdfs.LeaseRenewer.renew(LeaseRenewer.java:423)
	at org.apache.hadoop.hdfs.LeaseRenewer.run(LeaseRenewer.java:448)
	at org.apache.hadoop.hdfs.LeaseRenewer.access$700(LeaseRenewer.java:71)
	at org.apache.hadoop.hdfs.LeaseRenewer$1.run(LeaseRenewer.java:304)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:713)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1524)
	at org.apache.hadoop.ipc.Client.call(Client.java:1447)
	... 16 more
17/03/18 12:05:34 WARN hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1243578604_29] for 33 seconds.  Will retry shortly ...
java.net.ConnectException: Call From quickstart.cloudera/10.0.2.15 to quickstart.cloudera:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:731)
	at org.apache.hadoop.ipc.Client.call(Client.java:1475)
	at org.apache.hadoop.ipc.Client.call(Client.java:1408)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:230)
	at com.sun.proxy.$Proxy16.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease(ClientNamenodeProtocolTranslatorPB.java:581)
	at sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:256)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:104)
	at com.sun.proxy.$Proxy17.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:922)
	at org.apache.hadoop.hdfs.LeaseRenewer.renew(LeaseRenewer.java:423)
	at org.apache.hadoop.hdfs.LeaseRenewer.run(LeaseRenewer.java:448)
	at org.apache.hadoop.hdfs.LeaseRenewer.access$700(LeaseRenewer.java:71)
	at org.apache.hadoop.hdfs.LeaseRenewer$1.run(LeaseRenewer.java:304)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:713)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1524)
	at org.apache.hadoop.ipc.Client.call(Client.java:1447)
	... 16 more
17/03/18 12:05:35 WARN hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1243578604_29] for 34 seconds.  Will retry shortly ...
java.net.ConnectException: Call From quickstart.cloudera/10.0.2.15 to quickstart.cloudera:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:731)
	at org.apache.hadoop.ipc.Client.call(Client.java:1475)
	at org.apache.hadoop.ipc.Client.call(Client.java:1408)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:230)
	at com.sun.proxy.$Proxy16.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease(ClientNamenodeProtocolTranslatorPB.java:581)
	at sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:256)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:104)
	at com.sun.proxy.$Proxy17.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:922)
	at org.apache.hadoop.hdfs.LeaseRenewer.renew(LeaseRenewer.java:423)
	at org.apache.hadoop.hdfs.LeaseRenewer.run(LeaseRenewer.java:448)
	at org.apache.hadoop.hdfs.LeaseRenewer.access$700(LeaseRenewer.java:71)
	at org.apache.hadoop.hdfs.LeaseRenewer$1.run(LeaseRenewer.java:304)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:713)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1524)
	at org.apache.hadoop.ipc.Client.call(Client.java:1447)
	... 16 more
17/03/18 12:05:36 WARN hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1243578604_29] for 35 seconds.  Will retry shortly ...
java.net.ConnectException: Call From quickstart.cloudera/10.0.2.15 to quickstart.cloudera:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:731)
	at org.apache.hadoop.ipc.Client.call(Client.java:1475)
	at org.apache.hadoop.ipc.Client.call(Client.java:1408)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:230)
	at com.sun.proxy.$Proxy16.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease(ClientNamenodeProtocolTranslatorPB.java:581)
	at sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:256)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:104)
	at com.sun.proxy.$Proxy17.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:922)
	at org.apache.hadoop.hdfs.LeaseRenewer.renew(LeaseRenewer.java:423)
	at org.apache.hadoop.hdfs.LeaseRenewer.run(LeaseRenewer.java:448)
	at org.apache.hadoop.hdfs.LeaseRenewer.access$700(LeaseRenewer.java:71)
	at org.apache.hadoop.hdfs.LeaseRenewer$1.run(LeaseRenewer.java:304)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:713)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1524)
	at org.apache.hadoop.ipc.Client.call(Client.java:1447)
	... 16 more
17/03/18 12:05:37 WARN hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1243578604_29] for 36 seconds.  Will retry shortly ...
java.net.ConnectException: Call From quickstart.cloudera/10.0.2.15 to quickstart.cloudera:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:731)
	at org.apache.hadoop.ipc.Client.call(Client.java:1475)
	at org.apache.hadoop.ipc.Client.call(Client.java:1408)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:230)
	at com.sun.proxy.$Proxy16.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease(ClientNamenodeProtocolTranslatorPB.java:581)
	at sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:256)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:104)
	at com.sun.proxy.$Proxy17.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:922)
	at org.apache.hadoop.hdfs.LeaseRenewer.renew(LeaseRenewer.java:423)
	at org.apache.hadoop.hdfs.LeaseRenewer.run(LeaseRenewer.java:448)
	at org.apache.hadoop.hdfs.LeaseRenewer.access$700(LeaseRenewer.java:71)
	at org.apache.hadoop.hdfs.LeaseRenewer$1.run(LeaseRenewer.java:304)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:713)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1524)
	at org.apache.hadoop.ipc.Client.call(Client.java:1447)
	... 16 more
17/03/18 12:05:37 INFO hdfs.BucketWriter: Creating /user/cloudera/output/handson_train/flume/techJobTweet/2017/03/18/12/job_tweet.1489863915974.txt.tmp
17/03/18 12:05:37 WARN hdfs.HDFSEventSink: HDFS IO error
java.net.ConnectException: Call From quickstart.cloudera/10.0.2.15 to quickstart.cloudera:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:731)
	at org.apache.hadoop.ipc.Client.call(Client.java:1475)
	at org.apache.hadoop.ipc.Client.call(Client.java:1408)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:230)
	at com.sun.proxy.$Proxy16.create(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.create(ClientNamenodeProtocolTranslatorPB.java:297)
	at sun.reflect.GeneratedMethodAccessor8.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:256)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:104)
	at com.sun.proxy.$Proxy17.create(Unknown Source)
	at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:1965)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1738)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1663)
	at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:405)
	at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:401)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:401)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:344)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:920)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:901)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:798)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:787)
	at org.apache.flume.sink.hdfs.HDFSDataStream.doOpen(HDFSDataStream.java:86)
	at org.apache.flume.sink.hdfs.HDFSDataStream.open(HDFSDataStream.java:113)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:246)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:235)
	at org.apache.flume.sink.hdfs.BucketWriter$9$1.run(BucketWriter.java:679)
	at org.apache.flume.auth.SimpleAuthenticator.execute(SimpleAuthenticator.java:50)
	at org.apache.flume.sink.hdfs.BucketWriter$9.call(BucketWriter.java:676)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:713)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1524)
	at org.apache.hadoop.ipc.Client.call(Client.java:1447)
	... 33 more
17/03/18 12:05:38 WARN hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1243578604_29] for 37 seconds.  Will retry shortly ...
java.net.ConnectException: Call From quickstart.cloudera/10.0.2.15 to quickstart.cloudera:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:731)
	at org.apache.hadoop.ipc.Client.call(Client.java:1475)
	at org.apache.hadoop.ipc.Client.call(Client.java:1408)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:230)
	at com.sun.proxy.$Proxy16.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease(ClientNamenodeProtocolTranslatorPB.java:581)
	at sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:256)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:104)
	at com.sun.proxy.$Proxy17.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:922)
	at org.apache.hadoop.hdfs.LeaseRenewer.renew(LeaseRenewer.java:423)
	at org.apache.hadoop.hdfs.LeaseRenewer.run(LeaseRenewer.java:448)
	at org.apache.hadoop.hdfs.LeaseRenewer.access$700(LeaseRenewer.java:71)
	at org.apache.hadoop.hdfs.LeaseRenewer$1.run(LeaseRenewer.java:304)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:713)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1524)
	at org.apache.hadoop.ipc.Client.call(Client.java:1447)
	... 16 more
17/03/18 12:05:39 WARN hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1243578604_29] for 38 seconds.  Will retry shortly ...
java.net.ConnectException: Call From quickstart.cloudera/10.0.2.15 to quickstart.cloudera:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:731)
	at org.apache.hadoop.ipc.Client.call(Client.java:1475)
	at org.apache.hadoop.ipc.Client.call(Client.java:1408)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:230)
	at com.sun.proxy.$Proxy16.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease(ClientNamenodeProtocolTranslatorPB.java:581)
	at sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:256)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:104)
	at com.sun.proxy.$Proxy17.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:922)
	at org.apache.hadoop.hdfs.LeaseRenewer.renew(LeaseRenewer.java:423)
	at org.apache.hadoop.hdfs.LeaseRenewer.run(LeaseRenewer.java:448)
	at org.apache.hadoop.hdfs.LeaseRenewer.access$700(LeaseRenewer.java:71)
	at org.apache.hadoop.hdfs.LeaseRenewer$1.run(LeaseRenewer.java:304)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:713)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1524)
	at org.apache.hadoop.ipc.Client.call(Client.java:1447)
	... 16 more
17/03/18 12:05:40 WARN hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1243578604_29] for 39 seconds.  Will retry shortly ...
java.net.ConnectException: Call From quickstart.cloudera/10.0.2.15 to quickstart.cloudera:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:731)
	at org.apache.hadoop.ipc.Client.call(Client.java:1475)
	at org.apache.hadoop.ipc.Client.call(Client.java:1408)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:230)
	at com.sun.proxy.$Proxy16.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease(ClientNamenodeProtocolTranslatorPB.java:581)
	at sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:256)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:104)
	at com.sun.proxy.$Proxy17.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:922)
	at org.apache.hadoop.hdfs.LeaseRenewer.renew(LeaseRenewer.java:423)
	at org.apache.hadoop.hdfs.LeaseRenewer.run(LeaseRenewer.java:448)
	at org.apache.hadoop.hdfs.LeaseRenewer.access$700(LeaseRenewer.java:71)
	at org.apache.hadoop.hdfs.LeaseRenewer$1.run(LeaseRenewer.java:304)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:713)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1524)
	at org.apache.hadoop.ipc.Client.call(Client.java:1447)
	... 16 more
17/03/18 12:05:41 WARN hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1243578604_29] for 40 seconds.  Will retry shortly ...
java.net.ConnectException: Call From quickstart.cloudera/10.0.2.15 to quickstart.cloudera:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:731)
	at org.apache.hadoop.ipc.Client.call(Client.java:1475)
	at org.apache.hadoop.ipc.Client.call(Client.java:1408)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:230)
	at com.sun.proxy.$Proxy16.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease(ClientNamenodeProtocolTranslatorPB.java:581)
	at sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:256)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:104)
	at com.sun.proxy.$Proxy17.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:922)
	at org.apache.hadoop.hdfs.LeaseRenewer.renew(LeaseRenewer.java:423)
	at org.apache.hadoop.hdfs.LeaseRenewer.run(LeaseRenewer.java:448)
	at org.apache.hadoop.hdfs.LeaseRenewer.access$700(LeaseRenewer.java:71)
	at org.apache.hadoop.hdfs.LeaseRenewer$1.run(LeaseRenewer.java:304)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:713)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1524)
	at org.apache.hadoop.ipc.Client.call(Client.java:1447)
	... 16 more
17/03/18 12:05:42 WARN hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1243578604_29] for 41 seconds.  Will retry shortly ...
java.net.ConnectException: Call From quickstart.cloudera/10.0.2.15 to quickstart.cloudera:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor19.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:731)
	at org.apache.hadoop.ipc.Client.call(Client.java:1475)
	at org.apache.hadoop.ipc.Client.call(Client.java:1408)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:230)
	at com.sun.proxy.$Proxy16.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease(ClientNamenodeProtocolTranslatorPB.java:581)
	at sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:256)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:104)
	at com.sun.proxy.$Proxy17.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:922)
	at org.apache.hadoop.hdfs.LeaseRenewer.renew(LeaseRenewer.java:423)
	at org.apache.hadoop.hdfs.LeaseRenewer.run(LeaseRenewer.java:448)
	at org.apache.hadoop.hdfs.LeaseRenewer.access$700(LeaseRenewer.java:71)
	at org.apache.hadoop.hdfs.LeaseRenewer$1.run(LeaseRenewer.java:304)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:713)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1524)
	at org.apache.hadoop.ipc.Client.call(Client.java:1447)
	... 16 more
17/03/18 12:05:42 INFO hdfs.BucketWriter: Creating /user/cloudera/output/handson_train/flume/techJobTweet/2017/03/18/12/job_tweet.1489863915975.txt.tmp
17/03/18 12:05:42 WARN hdfs.HDFSEventSink: HDFS IO error
java.net.ConnectException: Call From quickstart.cloudera/10.0.2.15 to quickstart.cloudera:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor19.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:731)
	at org.apache.hadoop.ipc.Client.call(Client.java:1475)
	at org.apache.hadoop.ipc.Client.call(Client.java:1408)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:230)
	at com.sun.proxy.$Proxy16.create(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.create(ClientNamenodeProtocolTranslatorPB.java:297)
	at sun.reflect.GeneratedMethodAccessor8.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:256)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:104)
	at com.sun.proxy.$Proxy17.create(Unknown Source)
	at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:1965)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1738)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1663)
	at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:405)
	at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:401)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:401)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:344)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:920)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:901)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:798)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:787)
	at org.apache.flume.sink.hdfs.HDFSDataStream.doOpen(HDFSDataStream.java:86)
	at org.apache.flume.sink.hdfs.HDFSDataStream.open(HDFSDataStream.java:113)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:246)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:235)
	at org.apache.flume.sink.hdfs.BucketWriter$9$1.run(BucketWriter.java:679)
	at org.apache.flume.auth.SimpleAuthenticator.execute(SimpleAuthenticator.java:50)
	at org.apache.flume.sink.hdfs.BucketWriter$9.call(BucketWriter.java:676)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:713)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1524)
	at org.apache.hadoop.ipc.Client.call(Client.java:1447)
	... 33 more
17/03/18 12:05:43 WARN hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1243578604_29] for 42 seconds.  Will retry shortly ...
java.net.ConnectException: Call From quickstart.cloudera/10.0.2.15 to quickstart.cloudera:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor19.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:731)
	at org.apache.hadoop.ipc.Client.call(Client.java:1475)
	at org.apache.hadoop.ipc.Client.call(Client.java:1408)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:230)
	at com.sun.proxy.$Proxy16.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease(ClientNamenodeProtocolTranslatorPB.java:581)
	at sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:256)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:104)
	at com.sun.proxy.$Proxy17.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:922)
	at org.apache.hadoop.hdfs.LeaseRenewer.renew(LeaseRenewer.java:423)
	at org.apache.hadoop.hdfs.LeaseRenewer.run(LeaseRenewer.java:448)
	at org.apache.hadoop.hdfs.LeaseRenewer.access$700(LeaseRenewer.java:71)
	at org.apache.hadoop.hdfs.LeaseRenewer$1.run(LeaseRenewer.java:304)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:713)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1524)
	at org.apache.hadoop.ipc.Client.call(Client.java:1447)
	... 16 more
17/03/18 12:05:44 WARN hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1243578604_29] for 43 seconds.  Will retry shortly ...
java.net.ConnectException: Call From quickstart.cloudera/10.0.2.15 to quickstart.cloudera:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor19.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:731)
	at org.apache.hadoop.ipc.Client.call(Client.java:1475)
	at org.apache.hadoop.ipc.Client.call(Client.java:1408)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:230)
	at com.sun.proxy.$Proxy16.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease(ClientNamenodeProtocolTranslatorPB.java:581)
	at sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:256)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:104)
	at com.sun.proxy.$Proxy17.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:922)
	at org.apache.hadoop.hdfs.LeaseRenewer.renew(LeaseRenewer.java:423)
	at org.apache.hadoop.hdfs.LeaseRenewer.run(LeaseRenewer.java:448)
	at org.apache.hadoop.hdfs.LeaseRenewer.access$700(LeaseRenewer.java:71)
	at org.apache.hadoop.hdfs.LeaseRenewer$1.run(LeaseRenewer.java:304)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:713)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1524)
	at org.apache.hadoop.ipc.Client.call(Client.java:1447)
	... 16 more
17/03/18 12:05:45 WARN hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1243578604_29] for 44 seconds.  Will retry shortly ...
java.net.ConnectException: Call From quickstart.cloudera/10.0.2.15 to quickstart.cloudera:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor19.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:731)
	at org.apache.hadoop.ipc.Client.call(Client.java:1475)
	at org.apache.hadoop.ipc.Client.call(Client.java:1408)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:230)
	at com.sun.proxy.$Proxy16.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease(ClientNamenodeProtocolTranslatorPB.java:581)
	at sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:256)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:104)
	at com.sun.proxy.$Proxy17.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:922)
	at org.apache.hadoop.hdfs.LeaseRenewer.renew(LeaseRenewer.java:423)
	at org.apache.hadoop.hdfs.LeaseRenewer.run(LeaseRenewer.java:448)
	at org.apache.hadoop.hdfs.LeaseRenewer.access$700(LeaseRenewer.java:71)
	at org.apache.hadoop.hdfs.LeaseRenewer$1.run(LeaseRenewer.java:304)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:713)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1524)
	at org.apache.hadoop.ipc.Client.call(Client.java:1447)
	... 16 more
17/03/18 12:05:46 WARN hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1243578604_29] for 45 seconds.  Will retry shortly ...
java.net.ConnectException: Call From quickstart.cloudera/10.0.2.15 to quickstart.cloudera:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor19.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:731)
	at org.apache.hadoop.ipc.Client.call(Client.java:1475)
	at org.apache.hadoop.ipc.Client.call(Client.java:1408)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:230)
	at com.sun.proxy.$Proxy16.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease(ClientNamenodeProtocolTranslatorPB.java:581)
	at sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:256)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:104)
	at com.sun.proxy.$Proxy17.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:922)
	at org.apache.hadoop.hdfs.LeaseRenewer.renew(LeaseRenewer.java:423)
	at org.apache.hadoop.hdfs.LeaseRenewer.run(LeaseRenewer.java:448)
	at org.apache.hadoop.hdfs.LeaseRenewer.access$700(LeaseRenewer.java:71)
	at org.apache.hadoop.hdfs.LeaseRenewer$1.run(LeaseRenewer.java:304)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:713)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1524)
	at org.apache.hadoop.ipc.Client.call(Client.java:1447)
	... 16 more
17/03/18 12:05:47 WARN hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1243578604_29] for 46 seconds.  Will retry shortly ...
java.net.ConnectException: Call From quickstart.cloudera/10.0.2.15 to quickstart.cloudera:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor19.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:731)
	at org.apache.hadoop.ipc.Client.call(Client.java:1475)
	at org.apache.hadoop.ipc.Client.call(Client.java:1408)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:230)
	at com.sun.proxy.$Proxy16.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease(ClientNamenodeProtocolTranslatorPB.java:581)
	at sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:256)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:104)
	at com.sun.proxy.$Proxy17.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:922)
	at org.apache.hadoop.hdfs.LeaseRenewer.renew(LeaseRenewer.java:423)
	at org.apache.hadoop.hdfs.LeaseRenewer.run(LeaseRenewer.java:448)
	at org.apache.hadoop.hdfs.LeaseRenewer.access$700(LeaseRenewer.java:71)
	at org.apache.hadoop.hdfs.LeaseRenewer$1.run(LeaseRenewer.java:304)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:713)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1524)
	at org.apache.hadoop.ipc.Client.call(Client.java:1447)
	... 16 more
17/03/18 12:05:48 INFO hdfs.BucketWriter: Creating /user/cloudera/output/handson_train/flume/techJobTweet/2017/03/18/12/job_tweet.1489863915976.txt.tmp
17/03/18 12:05:48 WARN hdfs.HDFSEventSink: HDFS IO error
java.net.ConnectException: Call From quickstart.cloudera/10.0.2.15 to quickstart.cloudera:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor19.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:731)
	at org.apache.hadoop.ipc.Client.call(Client.java:1475)
	at org.apache.hadoop.ipc.Client.call(Client.java:1408)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:230)
	at com.sun.proxy.$Proxy16.create(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.create(ClientNamenodeProtocolTranslatorPB.java:297)
	at sun.reflect.GeneratedMethodAccessor8.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:256)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:104)
	at com.sun.proxy.$Proxy17.create(Unknown Source)
	at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:1965)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1738)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1663)
	at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:405)
	at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:401)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:401)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:344)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:920)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:901)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:798)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:787)
	at org.apache.flume.sink.hdfs.HDFSDataStream.doOpen(HDFSDataStream.java:86)
	at org.apache.flume.sink.hdfs.HDFSDataStream.open(HDFSDataStream.java:113)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:246)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:235)
	at org.apache.flume.sink.hdfs.BucketWriter$9$1.run(BucketWriter.java:679)
	at org.apache.flume.auth.SimpleAuthenticator.execute(SimpleAuthenticator.java:50)
	at org.apache.flume.sink.hdfs.BucketWriter$9.call(BucketWriter.java:676)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:713)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1524)
	at org.apache.hadoop.ipc.Client.call(Client.java:1447)
	... 33 more
17/03/18 12:05:48 WARN hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1243578604_29] for 47 seconds.  Will retry shortly ...
java.net.ConnectException: Call From quickstart.cloudera/10.0.2.15 to quickstart.cloudera:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor19.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:731)
	at org.apache.hadoop.ipc.Client.call(Client.java:1475)
	at org.apache.hadoop.ipc.Client.call(Client.java:1408)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:230)
	at com.sun.proxy.$Proxy16.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease(ClientNamenodeProtocolTranslatorPB.java:581)
	at sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:256)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:104)
	at com.sun.proxy.$Proxy17.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:922)
	at org.apache.hadoop.hdfs.LeaseRenewer.renew(LeaseRenewer.java:423)
	at org.apache.hadoop.hdfs.LeaseRenewer.run(LeaseRenewer.java:448)
	at org.apache.hadoop.hdfs.LeaseRenewer.access$700(LeaseRenewer.java:71)
	at org.apache.hadoop.hdfs.LeaseRenewer$1.run(LeaseRenewer.java:304)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:713)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1524)
	at org.apache.hadoop.ipc.Client.call(Client.java:1447)
	... 16 more
17/03/18 12:05:49 WARN hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1243578604_29] for 48 seconds.  Will retry shortly ...
java.net.ConnectException: Call From quickstart.cloudera/10.0.2.15 to quickstart.cloudera:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor19.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:731)
	at org.apache.hadoop.ipc.Client.call(Client.java:1475)
	at org.apache.hadoop.ipc.Client.call(Client.java:1408)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:230)
	at com.sun.proxy.$Proxy16.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease(ClientNamenodeProtocolTranslatorPB.java:581)
	at sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:256)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:104)
	at com.sun.proxy.$Proxy17.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:922)
	at org.apache.hadoop.hdfs.LeaseRenewer.renew(LeaseRenewer.java:423)
	at org.apache.hadoop.hdfs.LeaseRenewer.run(LeaseRenewer.java:448)
	at org.apache.hadoop.hdfs.LeaseRenewer.access$700(LeaseRenewer.java:71)
	at org.apache.hadoop.hdfs.LeaseRenewer$1.run(LeaseRenewer.java:304)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:713)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1524)
	at org.apache.hadoop.ipc.Client.call(Client.java:1447)
	... 16 more
17/03/18 12:05:49 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/chkpint_dir/checkpoint, elements to sync = 5
17/03/18 12:05:49 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1489850440968, queueSize: 10, queueHead: 118
17/03/18 12:05:49 INFO file.Log: Updated checkpoint for file: /home/cloudera/classes/hadoop-training-projects/final_project/flume/file_channel_dir/datadir/log-2 position: 10906453 logWriteOrderID: 1489850440968
17/03/18 12:05:50 WARN hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1243578604_29] for 49 seconds.  Will retry shortly ...
java.net.ConnectException: Call From quickstart.cloudera/10.0.2.15 to quickstart.cloudera:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor19.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:731)
	at org.apache.hadoop.ipc.Client.call(Client.java:1475)
	at org.apache.hadoop.ipc.Client.call(Client.java:1408)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:230)
	at com.sun.proxy.$Proxy16.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease(ClientNamenodeProtocolTranslatorPB.java:581)
	at sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:256)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:104)
	at com.sun.proxy.$Proxy17.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:922)
	at org.apache.hadoop.hdfs.LeaseRenewer.renew(LeaseRenewer.java:423)
	at org.apache.hadoop.hdfs.LeaseRenewer.run(LeaseRenewer.java:448)
	at org.apache.hadoop.hdfs.LeaseRenewer.access$700(LeaseRenewer.java:71)
	at org.apache.hadoop.hdfs.LeaseRenewer$1.run(LeaseRenewer.java:304)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:713)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1524)
	at org.apache.hadoop.ipc.Client.call(Client.java:1447)
	... 16 more
17/03/18 12:05:51 WARN hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1243578604_29] for 50 seconds.  Will retry shortly ...
java.net.ConnectException: Call From quickstart.cloudera/10.0.2.15 to quickstart.cloudera:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor19.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:731)
	at org.apache.hadoop.ipc.Client.call(Client.java:1475)
	at org.apache.hadoop.ipc.Client.call(Client.java:1408)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:230)
	at com.sun.proxy.$Proxy16.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease(ClientNamenodeProtocolTranslatorPB.java:581)
	at sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:256)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:104)
	at com.sun.proxy.$Proxy17.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:922)
	at org.apache.hadoop.hdfs.LeaseRenewer.renew(LeaseRenewer.java:423)
	at org.apache.hadoop.hdfs.LeaseRenewer.run(LeaseRenewer.java:448)
	at org.apache.hadoop.hdfs.LeaseRenewer.access$700(LeaseRenewer.java:71)
	at org.apache.hadoop.hdfs.LeaseRenewer$1.run(LeaseRenewer.java:304)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:713)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1524)
	at org.apache.hadoop.ipc.Client.call(Client.java:1447)
	... 16 more
17/03/18 12:05:52 WARN hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1243578604_29] for 51 seconds.  Will retry shortly ...
java.net.ConnectException: Call From quickstart.cloudera/10.0.2.15 to quickstart.cloudera:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor19.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:731)
	at org.apache.hadoop.ipc.Client.call(Client.java:1475)
	at org.apache.hadoop.ipc.Client.call(Client.java:1408)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:230)
	at com.sun.proxy.$Proxy16.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease(ClientNamenodeProtocolTranslatorPB.java:581)
	at sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:256)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:104)
	at com.sun.proxy.$Proxy17.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:922)
	at org.apache.hadoop.hdfs.LeaseRenewer.renew(LeaseRenewer.java:423)
	at org.apache.hadoop.hdfs.LeaseRenewer.run(LeaseRenewer.java:448)
	at org.apache.hadoop.hdfs.LeaseRenewer.access$700(LeaseRenewer.java:71)
	at org.apache.hadoop.hdfs.LeaseRenewer$1.run(LeaseRenewer.java:304)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:713)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1524)
	at org.apache.hadoop.ipc.Client.call(Client.java:1447)
	... 16 more
17/03/18 12:05:53 INFO hdfs.BucketWriter: Creating /user/cloudera/output/handson_train/flume/techJobTweet/2017/03/18/12/job_tweet.1489863915977.txt.tmp
17/03/18 12:05:53 WARN hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1243578604_29] for 52 seconds.  Will retry shortly ...
java.net.ConnectException: Call From quickstart.cloudera/10.0.2.15 to quickstart.cloudera:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor19.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:731)
	at org.apache.hadoop.ipc.Client.call(Client.java:1475)
	at org.apache.hadoop.ipc.Client.call(Client.java:1408)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:230)
	at com.sun.proxy.$Proxy16.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease(ClientNamenodeProtocolTranslatorPB.java:581)
	at sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:256)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:104)
	at com.sun.proxy.$Proxy17.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:922)
	at org.apache.hadoop.hdfs.LeaseRenewer.renew(LeaseRenewer.java:423)
	at org.apache.hadoop.hdfs.LeaseRenewer.run(LeaseRenewer.java:448)
	at org.apache.hadoop.hdfs.LeaseRenewer.access$700(LeaseRenewer.java:71)
	at org.apache.hadoop.hdfs.LeaseRenewer$1.run(LeaseRenewer.java:304)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:713)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1524)
	at org.apache.hadoop.ipc.Client.call(Client.java:1447)
	... 16 more
17/03/18 12:05:53 WARN hdfs.HDFSEventSink: HDFS IO error
java.net.ConnectException: Call From quickstart.cloudera/10.0.2.15 to quickstart.cloudera:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor19.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:731)
	at org.apache.hadoop.ipc.Client.call(Client.java:1475)
	at org.apache.hadoop.ipc.Client.call(Client.java:1408)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:230)
	at com.sun.proxy.$Proxy16.create(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.create(ClientNamenodeProtocolTranslatorPB.java:297)
	at sun.reflect.GeneratedMethodAccessor8.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:256)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:104)
	at com.sun.proxy.$Proxy17.create(Unknown Source)
	at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:1965)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1738)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1663)
	at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:405)
	at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:401)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:401)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:344)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:920)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:901)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:798)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:787)
	at org.apache.flume.sink.hdfs.HDFSDataStream.doOpen(HDFSDataStream.java:86)
	at org.apache.flume.sink.hdfs.HDFSDataStream.open(HDFSDataStream.java:113)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:246)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:235)
	at org.apache.flume.sink.hdfs.BucketWriter$9$1.run(BucketWriter.java:679)
	at org.apache.flume.auth.SimpleAuthenticator.execute(SimpleAuthenticator.java:50)
	at org.apache.flume.sink.hdfs.BucketWriter$9.call(BucketWriter.java:676)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:713)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1524)
	at org.apache.hadoop.ipc.Client.call(Client.java:1447)
	... 33 more
17/03/18 12:05:54 WARN hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1243578604_29] for 53 seconds.  Will retry shortly ...
java.net.ConnectException: Call From quickstart.cloudera/10.0.2.15 to quickstart.cloudera:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor19.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:731)
	at org.apache.hadoop.ipc.Client.call(Client.java:1475)
	at org.apache.hadoop.ipc.Client.call(Client.java:1408)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:230)
	at com.sun.proxy.$Proxy16.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease(ClientNamenodeProtocolTranslatorPB.java:581)
	at sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:256)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:104)
	at com.sun.proxy.$Proxy17.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:922)
	at org.apache.hadoop.hdfs.LeaseRenewer.renew(LeaseRenewer.java:423)
	at org.apache.hadoop.hdfs.LeaseRenewer.run(LeaseRenewer.java:448)
	at org.apache.hadoop.hdfs.LeaseRenewer.access$700(LeaseRenewer.java:71)
	at org.apache.hadoop.hdfs.LeaseRenewer$1.run(LeaseRenewer.java:304)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:713)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1524)
	at org.apache.hadoop.ipc.Client.call(Client.java:1447)
	... 16 more
17/03/18 12:05:55 WARN hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1243578604_29] for 54 seconds.  Will retry shortly ...
java.net.ConnectException: Call From quickstart.cloudera/10.0.2.15 to quickstart.cloudera:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor19.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:731)
	at org.apache.hadoop.ipc.Client.call(Client.java:1475)
	at org.apache.hadoop.ipc.Client.call(Client.java:1408)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:230)
	at com.sun.proxy.$Proxy16.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease(ClientNamenodeProtocolTranslatorPB.java:581)
	at sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:256)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:104)
	at com.sun.proxy.$Proxy17.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:922)
	at org.apache.hadoop.hdfs.LeaseRenewer.renew(LeaseRenewer.java:423)
	at org.apache.hadoop.hdfs.LeaseRenewer.run(LeaseRenewer.java:448)
	at org.apache.hadoop.hdfs.LeaseRenewer.access$700(LeaseRenewer.java:71)
	at org.apache.hadoop.hdfs.LeaseRenewer$1.run(LeaseRenewer.java:304)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:713)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1524)
	at org.apache.hadoop.ipc.Client.call(Client.java:1447)
	... 16 more
17/03/18 12:05:56 WARN hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1243578604_29] for 55 seconds.  Will retry shortly ...
java.net.ConnectException: Call From quickstart.cloudera/10.0.2.15 to quickstart.cloudera:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor19.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:731)
	at org.apache.hadoop.ipc.Client.call(Client.java:1475)
	at org.apache.hadoop.ipc.Client.call(Client.java:1408)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:230)
	at com.sun.proxy.$Proxy16.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease(ClientNamenodeProtocolTranslatorPB.java:581)
	at sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:256)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:104)
	at com.sun.proxy.$Proxy17.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:922)
	at org.apache.hadoop.hdfs.LeaseRenewer.renew(LeaseRenewer.java:423)
	at org.apache.hadoop.hdfs.LeaseRenewer.run(LeaseRenewer.java:448)
	at org.apache.hadoop.hdfs.LeaseRenewer.access$700(LeaseRenewer.java:71)
	at org.apache.hadoop.hdfs.LeaseRenewer$1.run(LeaseRenewer.java:304)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:713)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1524)
	at org.apache.hadoop.ipc.Client.call(Client.java:1447)
	... 16 more
17/03/18 12:05:57 WARN hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1243578604_29] for 56 seconds.  Will retry shortly ...
java.net.ConnectException: Call From quickstart.cloudera/10.0.2.15 to quickstart.cloudera:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor19.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:731)
	at org.apache.hadoop.ipc.Client.call(Client.java:1475)
	at org.apache.hadoop.ipc.Client.call(Client.java:1408)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:230)
	at com.sun.proxy.$Proxy16.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease(ClientNamenodeProtocolTranslatorPB.java:581)
	at sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:256)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:104)
	at com.sun.proxy.$Proxy17.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:922)
	at org.apache.hadoop.hdfs.LeaseRenewer.renew(LeaseRenewer.java:423)
	at org.apache.hadoop.hdfs.LeaseRenewer.run(LeaseRenewer.java:448)
	at org.apache.hadoop.hdfs.LeaseRenewer.access$700(LeaseRenewer.java:71)
	at org.apache.hadoop.hdfs.LeaseRenewer$1.run(LeaseRenewer.java:304)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:713)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1524)
	at org.apache.hadoop.ipc.Client.call(Client.java:1447)
	... 16 more
17/03/18 12:05:58 WARN hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1243578604_29] for 57 seconds.  Will retry shortly ...
java.net.ConnectException: Call From quickstart.cloudera/10.0.2.15 to quickstart.cloudera:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor19.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:731)
	at org.apache.hadoop.ipc.Client.call(Client.java:1475)
	at org.apache.hadoop.ipc.Client.call(Client.java:1408)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:230)
	at com.sun.proxy.$Proxy16.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease(ClientNamenodeProtocolTranslatorPB.java:581)
	at sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:256)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:104)
	at com.sun.proxy.$Proxy17.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:922)
	at org.apache.hadoop.hdfs.LeaseRenewer.renew(LeaseRenewer.java:423)
	at org.apache.hadoop.hdfs.LeaseRenewer.run(LeaseRenewer.java:448)
	at org.apache.hadoop.hdfs.LeaseRenewer.access$700(LeaseRenewer.java:71)
	at org.apache.hadoop.hdfs.LeaseRenewer$1.run(LeaseRenewer.java:304)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:713)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1524)
	at org.apache.hadoop.ipc.Client.call(Client.java:1447)
	... 16 more
17/03/18 12:05:58 INFO hdfs.BucketWriter: Creating /user/cloudera/output/handson_train/flume/techJobTweet/2017/03/18/12/job_tweet.1489863915978.txt.tmp
17/03/18 12:05:58 WARN hdfs.HDFSEventSink: HDFS IO error
java.net.ConnectException: Call From quickstart.cloudera/10.0.2.15 to quickstart.cloudera:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor19.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:731)
	at org.apache.hadoop.ipc.Client.call(Client.java:1475)
	at org.apache.hadoop.ipc.Client.call(Client.java:1408)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:230)
	at com.sun.proxy.$Proxy16.create(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.create(ClientNamenodeProtocolTranslatorPB.java:297)
	at sun.reflect.GeneratedMethodAccessor8.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:256)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:104)
	at com.sun.proxy.$Proxy17.create(Unknown Source)
	at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:1965)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1738)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1663)
	at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:405)
	at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:401)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:401)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:344)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:920)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:901)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:798)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:787)
	at org.apache.flume.sink.hdfs.HDFSDataStream.doOpen(HDFSDataStream.java:86)
	at org.apache.flume.sink.hdfs.HDFSDataStream.open(HDFSDataStream.java:113)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:246)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:235)
	at org.apache.flume.sink.hdfs.BucketWriter$9$1.run(BucketWriter.java:679)
	at org.apache.flume.auth.SimpleAuthenticator.execute(SimpleAuthenticator.java:50)
	at org.apache.flume.sink.hdfs.BucketWriter$9.call(BucketWriter.java:676)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:713)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1524)
	at org.apache.hadoop.ipc.Client.call(Client.java:1447)
	... 33 more
17/03/18 12:05:59 WARN hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1243578604_29] for 58 seconds.  Will retry shortly ...
java.net.ConnectException: Call From quickstart.cloudera/10.0.2.15 to quickstart.cloudera:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor19.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:731)
	at org.apache.hadoop.ipc.Client.call(Client.java:1475)
	at org.apache.hadoop.ipc.Client.call(Client.java:1408)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:230)
	at com.sun.proxy.$Proxy16.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease(ClientNamenodeProtocolTranslatorPB.java:581)
	at sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:256)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:104)
	at com.sun.proxy.$Proxy17.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:922)
	at org.apache.hadoop.hdfs.LeaseRenewer.renew(LeaseRenewer.java:423)
	at org.apache.hadoop.hdfs.LeaseRenewer.run(LeaseRenewer.java:448)
	at org.apache.hadoop.hdfs.LeaseRenewer.access$700(LeaseRenewer.java:71)
	at org.apache.hadoop.hdfs.LeaseRenewer$1.run(LeaseRenewer.java:304)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:713)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1524)
	at org.apache.hadoop.ipc.Client.call(Client.java:1447)
	... 16 more
17/03/18 12:06:00 WARN hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1243578604_29] for 59 seconds.  Will retry shortly ...
java.net.ConnectException: Call From quickstart.cloudera/10.0.2.15 to quickstart.cloudera:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor19.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:731)
	at org.apache.hadoop.ipc.Client.call(Client.java:1475)
	at org.apache.hadoop.ipc.Client.call(Client.java:1408)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:230)
	at com.sun.proxy.$Proxy16.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease(ClientNamenodeProtocolTranslatorPB.java:581)
	at sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:256)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:104)
	at com.sun.proxy.$Proxy17.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:922)
	at org.apache.hadoop.hdfs.LeaseRenewer.renew(LeaseRenewer.java:423)
	at org.apache.hadoop.hdfs.LeaseRenewer.run(LeaseRenewer.java:448)
	at org.apache.hadoop.hdfs.LeaseRenewer.access$700(LeaseRenewer.java:71)
	at org.apache.hadoop.hdfs.LeaseRenewer$1.run(LeaseRenewer.java:304)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:713)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1524)
	at org.apache.hadoop.ipc.Client.call(Client.java:1447)
	... 16 more
17/03/18 12:06:01 WARN hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1243578604_29] for 60 seconds.  Will retry shortly ...
java.net.ConnectException: Call From quickstart.cloudera/10.0.2.15 to quickstart.cloudera:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor19.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:731)
	at org.apache.hadoop.ipc.Client.call(Client.java:1475)
	at org.apache.hadoop.ipc.Client.call(Client.java:1408)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:230)
	at com.sun.proxy.$Proxy16.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease(ClientNamenodeProtocolTranslatorPB.java:581)
	at sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:256)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:104)
	at com.sun.proxy.$Proxy17.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:922)
	at org.apache.hadoop.hdfs.LeaseRenewer.renew(LeaseRenewer.java:423)
	at org.apache.hadoop.hdfs.LeaseRenewer.run(LeaseRenewer.java:448)
	at org.apache.hadoop.hdfs.LeaseRenewer.access$700(LeaseRenewer.java:71)
	at org.apache.hadoop.hdfs.LeaseRenewer$1.run(LeaseRenewer.java:304)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:713)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1524)
	at org.apache.hadoop.ipc.Client.call(Client.java:1447)
	... 16 more
17/03/18 12:06:02 WARN hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1243578604_29] for 61 seconds.  Will retry shortly ...
java.net.ConnectException: Call From quickstart.cloudera/10.0.2.15 to quickstart.cloudera:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor19.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:731)
	at org.apache.hadoop.ipc.Client.call(Client.java:1475)
	at org.apache.hadoop.ipc.Client.call(Client.java:1408)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:230)
	at com.sun.proxy.$Proxy16.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease(ClientNamenodeProtocolTranslatorPB.java:581)
	at sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:256)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:104)
	at com.sun.proxy.$Proxy17.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:922)
	at org.apache.hadoop.hdfs.LeaseRenewer.renew(LeaseRenewer.java:423)
	at org.apache.hadoop.hdfs.LeaseRenewer.run(LeaseRenewer.java:448)
	at org.apache.hadoop.hdfs.LeaseRenewer.access$700(LeaseRenewer.java:71)
	at org.apache.hadoop.hdfs.LeaseRenewer$1.run(LeaseRenewer.java:304)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:713)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1524)
	at org.apache.hadoop.ipc.Client.call(Client.java:1447)
	... 16 more
17/03/18 12:06:03 WARN hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1243578604_29] for 62 seconds.  Will retry shortly ...
java.net.ConnectException: Call From quickstart.cloudera/10.0.2.15 to quickstart.cloudera:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor19.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:731)
	at org.apache.hadoop.ipc.Client.call(Client.java:1475)
	at org.apache.hadoop.ipc.Client.call(Client.java:1408)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:230)
	at com.sun.proxy.$Proxy16.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease(ClientNamenodeProtocolTranslatorPB.java:581)
	at sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:256)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:104)
	at com.sun.proxy.$Proxy17.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:922)
	at org.apache.hadoop.hdfs.LeaseRenewer.renew(LeaseRenewer.java:423)
	at org.apache.hadoop.hdfs.LeaseRenewer.run(LeaseRenewer.java:448)
	at org.apache.hadoop.hdfs.LeaseRenewer.access$700(LeaseRenewer.java:71)
	at org.apache.hadoop.hdfs.LeaseRenewer$1.run(LeaseRenewer.java:304)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:713)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1524)
	at org.apache.hadoop.ipc.Client.call(Client.java:1447)
	... 16 more
17/03/18 12:06:03 INFO hdfs.BucketWriter: Creating /user/cloudera/output/handson_train/flume/techJobTweet/2017/03/18/12/job_tweet.1489863915979.txt.tmp
17/03/18 12:06:04 WARN hdfs.HDFSEventSink: HDFS IO error
java.net.ConnectException: Call From quickstart.cloudera/10.0.2.15 to quickstart.cloudera:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor19.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:731)
	at org.apache.hadoop.ipc.Client.call(Client.java:1475)
	at org.apache.hadoop.ipc.Client.call(Client.java:1408)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:230)
	at com.sun.proxy.$Proxy16.create(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.create(ClientNamenodeProtocolTranslatorPB.java:297)
	at sun.reflect.GeneratedMethodAccessor8.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:256)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:104)
	at com.sun.proxy.$Proxy17.create(Unknown Source)
	at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:1965)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1738)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1663)
	at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:405)
	at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:401)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:401)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:344)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:920)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:901)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:798)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:787)
	at org.apache.flume.sink.hdfs.HDFSDataStream.doOpen(HDFSDataStream.java:86)
	at org.apache.flume.sink.hdfs.HDFSDataStream.open(HDFSDataStream.java:113)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:246)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:235)
	at org.apache.flume.sink.hdfs.BucketWriter$9$1.run(BucketWriter.java:679)
	at org.apache.flume.auth.SimpleAuthenticator.execute(SimpleAuthenticator.java:50)
	at org.apache.flume.sink.hdfs.BucketWriter$9.call(BucketWriter.java:676)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:713)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1524)
	at org.apache.hadoop.ipc.Client.call(Client.java:1447)
	... 33 more
17/03/18 12:06:04 WARN hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1243578604_29] for 63 seconds.  Will retry shortly ...
java.net.ConnectException: Call From quickstart.cloudera/10.0.2.15 to quickstart.cloudera:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor19.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:731)
	at org.apache.hadoop.ipc.Client.call(Client.java:1475)
	at org.apache.hadoop.ipc.Client.call(Client.java:1408)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:230)
	at com.sun.proxy.$Proxy16.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease(ClientNamenodeProtocolTranslatorPB.java:581)
	at sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:256)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:104)
	at com.sun.proxy.$Proxy17.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:922)
	at org.apache.hadoop.hdfs.LeaseRenewer.renew(LeaseRenewer.java:423)
	at org.apache.hadoop.hdfs.LeaseRenewer.run(LeaseRenewer.java:448)
	at org.apache.hadoop.hdfs.LeaseRenewer.access$700(LeaseRenewer.java:71)
	at org.apache.hadoop.hdfs.LeaseRenewer$1.run(LeaseRenewer.java:304)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:713)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1524)
	at org.apache.hadoop.ipc.Client.call(Client.java:1447)
	... 16 more
17/03/18 12:06:05 WARN hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1243578604_29] for 64 seconds.  Will retry shortly ...
java.net.ConnectException: Call From quickstart.cloudera/10.0.2.15 to quickstart.cloudera:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor19.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:731)
	at org.apache.hadoop.ipc.Client.call(Client.java:1475)
	at org.apache.hadoop.ipc.Client.call(Client.java:1408)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:230)
	at com.sun.proxy.$Proxy16.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease(ClientNamenodeProtocolTranslatorPB.java:581)
	at sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:256)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:104)
	at com.sun.proxy.$Proxy17.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:922)
	at org.apache.hadoop.hdfs.LeaseRenewer.renew(LeaseRenewer.java:423)
	at org.apache.hadoop.hdfs.LeaseRenewer.run(LeaseRenewer.java:448)
	at org.apache.hadoop.hdfs.LeaseRenewer.access$700(LeaseRenewer.java:71)
	at org.apache.hadoop.hdfs.LeaseRenewer$1.run(LeaseRenewer.java:304)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:713)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1524)
	at org.apache.hadoop.ipc.Client.call(Client.java:1447)
	... 16 more
17/03/18 12:06:06 WARN hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1243578604_29] for 65 seconds.  Will retry shortly ...
java.net.ConnectException: Call From quickstart.cloudera/10.0.2.15 to quickstart.cloudera:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor19.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:731)
	at org.apache.hadoop.ipc.Client.call(Client.java:1475)
	at org.apache.hadoop.ipc.Client.call(Client.java:1408)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:230)
	at com.sun.proxy.$Proxy16.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease(ClientNamenodeProtocolTranslatorPB.java:581)
	at sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:256)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:104)
	at com.sun.proxy.$Proxy17.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:922)
	at org.apache.hadoop.hdfs.LeaseRenewer.renew(LeaseRenewer.java:423)
	at org.apache.hadoop.hdfs.LeaseRenewer.run(LeaseRenewer.java:448)
	at org.apache.hadoop.hdfs.LeaseRenewer.access$700(LeaseRenewer.java:71)
	at org.apache.hadoop.hdfs.LeaseRenewer$1.run(LeaseRenewer.java:304)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:713)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1524)
	at org.apache.hadoop.ipc.Client.call(Client.java:1447)
	... 16 more
17/03/18 12:06:07 WARN hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1243578604_29] for 66 seconds.  Will retry shortly ...
java.net.ConnectException: Call From quickstart.cloudera/10.0.2.15 to quickstart.cloudera:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor19.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:731)
	at org.apache.hadoop.ipc.Client.call(Client.java:1475)
	at org.apache.hadoop.ipc.Client.call(Client.java:1408)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:230)
	at com.sun.proxy.$Proxy16.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease(ClientNamenodeProtocolTranslatorPB.java:581)
	at sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:256)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:104)
	at com.sun.proxy.$Proxy17.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:922)
	at org.apache.hadoop.hdfs.LeaseRenewer.renew(LeaseRenewer.java:423)
	at org.apache.hadoop.hdfs.LeaseRenewer.run(LeaseRenewer.java:448)
	at org.apache.hadoop.hdfs.LeaseRenewer.access$700(LeaseRenewer.java:71)
	at org.apache.hadoop.hdfs.LeaseRenewer$1.run(LeaseRenewer.java:304)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:713)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1524)
	at org.apache.hadoop.ipc.Client.call(Client.java:1447)
	... 16 more
17/03/18 12:06:08 WARN hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1243578604_29] for 67 seconds.  Will retry shortly ...
java.net.ConnectException: Call From quickstart.cloudera/10.0.2.15 to quickstart.cloudera:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor19.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:731)
	at org.apache.hadoop.ipc.Client.call(Client.java:1475)
	at org.apache.hadoop.ipc.Client.call(Client.java:1408)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:230)
	at com.sun.proxy.$Proxy16.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease(ClientNamenodeProtocolTranslatorPB.java:581)
	at sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:256)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:104)
	at com.sun.proxy.$Proxy17.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:922)
	at org.apache.hadoop.hdfs.LeaseRenewer.renew(LeaseRenewer.java:423)
	at org.apache.hadoop.hdfs.LeaseRenewer.run(LeaseRenewer.java:448)
	at org.apache.hadoop.hdfs.LeaseRenewer.access$700(LeaseRenewer.java:71)
	at org.apache.hadoop.hdfs.LeaseRenewer$1.run(LeaseRenewer.java:304)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:713)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1524)
	at org.apache.hadoop.ipc.Client.call(Client.java:1447)
	... 16 more
17/03/18 12:06:09 INFO hdfs.BucketWriter: Creating /user/cloudera/output/handson_train/flume/techJobTweet/2017/03/18/12/job_tweet.1489863915980.txt.tmp
17/03/18 12:06:09 WARN hdfs.HDFSEventSink: HDFS IO error
java.net.ConnectException: Call From quickstart.cloudera/10.0.2.15 to quickstart.cloudera:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor19.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:731)
	at org.apache.hadoop.ipc.Client.call(Client.java:1475)
	at org.apache.hadoop.ipc.Client.call(Client.java:1408)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:230)
	at com.sun.proxy.$Proxy16.create(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.create(ClientNamenodeProtocolTranslatorPB.java:297)
	at sun.reflect.GeneratedMethodAccessor8.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:256)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:104)
	at com.sun.proxy.$Proxy17.create(Unknown Source)
	at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:1965)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1738)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1663)
	at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:405)
	at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:401)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:401)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:344)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:920)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:901)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:798)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:787)
	at org.apache.flume.sink.hdfs.HDFSDataStream.doOpen(HDFSDataStream.java:86)
	at org.apache.flume.sink.hdfs.HDFSDataStream.open(HDFSDataStream.java:113)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:246)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:235)
	at org.apache.flume.sink.hdfs.BucketWriter$9$1.run(BucketWriter.java:679)
	at org.apache.flume.auth.SimpleAuthenticator.execute(SimpleAuthenticator.java:50)
	at org.apache.flume.sink.hdfs.BucketWriter$9.call(BucketWriter.java:676)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:713)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1524)
	at org.apache.hadoop.ipc.Client.call(Client.java:1447)
	... 33 more
17/03/18 12:06:09 WARN hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1243578604_29] for 68 seconds.  Will retry shortly ...
java.net.ConnectException: Call From quickstart.cloudera/10.0.2.15 to quickstart.cloudera:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor19.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:731)
	at org.apache.hadoop.ipc.Client.call(Client.java:1475)
	at org.apache.hadoop.ipc.Client.call(Client.java:1408)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:230)
	at com.sun.proxy.$Proxy16.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease(ClientNamenodeProtocolTranslatorPB.java:581)
	at sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:256)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:104)
	at com.sun.proxy.$Proxy17.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:922)
	at org.apache.hadoop.hdfs.LeaseRenewer.renew(LeaseRenewer.java:423)
	at org.apache.hadoop.hdfs.LeaseRenewer.run(LeaseRenewer.java:448)
	at org.apache.hadoop.hdfs.LeaseRenewer.access$700(LeaseRenewer.java:71)
	at org.apache.hadoop.hdfs.LeaseRenewer$1.run(LeaseRenewer.java:304)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:713)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1524)
	at org.apache.hadoop.ipc.Client.call(Client.java:1447)
	... 16 more
17/03/18 12:06:10 WARN hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1243578604_29] for 69 seconds.  Will retry shortly ...
java.net.ConnectException: Call From quickstart.cloudera/10.0.2.15 to quickstart.cloudera:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor19.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:731)
	at org.apache.hadoop.ipc.Client.call(Client.java:1475)
	at org.apache.hadoop.ipc.Client.call(Client.java:1408)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:230)
	at com.sun.proxy.$Proxy16.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease(ClientNamenodeProtocolTranslatorPB.java:581)
	at sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:256)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:104)
	at com.sun.proxy.$Proxy17.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:922)
	at org.apache.hadoop.hdfs.LeaseRenewer.renew(LeaseRenewer.java:423)
	at org.apache.hadoop.hdfs.LeaseRenewer.run(LeaseRenewer.java:448)
	at org.apache.hadoop.hdfs.LeaseRenewer.access$700(LeaseRenewer.java:71)
	at org.apache.hadoop.hdfs.LeaseRenewer$1.run(LeaseRenewer.java:304)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:713)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1524)
	at org.apache.hadoop.ipc.Client.call(Client.java:1447)
	... 16 more
17/03/18 12:06:11 WARN hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1243578604_29] for 70 seconds.  Will retry shortly ...
java.net.ConnectException: Call From quickstart.cloudera/10.0.2.15 to quickstart.cloudera:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor19.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:731)
	at org.apache.hadoop.ipc.Client.call(Client.java:1475)
	at org.apache.hadoop.ipc.Client.call(Client.java:1408)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:230)
	at com.sun.proxy.$Proxy16.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease(ClientNamenodeProtocolTranslatorPB.java:581)
	at sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:256)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:104)
	at com.sun.proxy.$Proxy17.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:922)
	at org.apache.hadoop.hdfs.LeaseRenewer.renew(LeaseRenewer.java:423)
	at org.apache.hadoop.hdfs.LeaseRenewer.run(LeaseRenewer.java:448)
	at org.apache.hadoop.hdfs.LeaseRenewer.access$700(LeaseRenewer.java:71)
	at org.apache.hadoop.hdfs.LeaseRenewer$1.run(LeaseRenewer.java:304)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:713)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1524)
	at org.apache.hadoop.ipc.Client.call(Client.java:1447)
	... 16 more
17/03/18 12:06:12 WARN hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1243578604_29] for 71 seconds.  Will retry shortly ...
java.net.ConnectException: Call From quickstart.cloudera/10.0.2.15 to quickstart.cloudera:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor19.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:731)
	at org.apache.hadoop.ipc.Client.call(Client.java:1475)
	at org.apache.hadoop.ipc.Client.call(Client.java:1408)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:230)
	at com.sun.proxy.$Proxy16.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease(ClientNamenodeProtocolTranslatorPB.java:581)
	at sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:256)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:104)
	at com.sun.proxy.$Proxy17.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:922)
	at org.apache.hadoop.hdfs.LeaseRenewer.renew(LeaseRenewer.java:423)
	at org.apache.hadoop.hdfs.LeaseRenewer.run(LeaseRenewer.java:448)
	at org.apache.hadoop.hdfs.LeaseRenewer.access$700(LeaseRenewer.java:71)
	at org.apache.hadoop.hdfs.LeaseRenewer$1.run(LeaseRenewer.java:304)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:713)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1524)
	at org.apache.hadoop.ipc.Client.call(Client.java:1447)
	... 16 more
17/03/18 12:06:13 WARN hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1243578604_29] for 72 seconds.  Will retry shortly ...
java.io.IOException: Failed on local exception: java.net.SocketException: Network is unreachable; Host Details : local host is: "quickstart.cloudera/10.0.2.15"; destination host is: "quickstart.cloudera":8020; 
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:772)
	at org.apache.hadoop.ipc.Client.call(Client.java:1475)
	at org.apache.hadoop.ipc.Client.call(Client.java:1408)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:230)
	at com.sun.proxy.$Proxy16.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease(ClientNamenodeProtocolTranslatorPB.java:581)
	at sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:256)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:104)
	at com.sun.proxy.$Proxy17.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:922)
	at org.apache.hadoop.hdfs.LeaseRenewer.renew(LeaseRenewer.java:423)
	at org.apache.hadoop.hdfs.LeaseRenewer.run(LeaseRenewer.java:448)
	at org.apache.hadoop.hdfs.LeaseRenewer.access$700(LeaseRenewer.java:71)
	at org.apache.hadoop.hdfs.LeaseRenewer$1.run(LeaseRenewer.java:304)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.SocketException: Network is unreachable
	at sun.nio.ch.Net.connect0(Native Method)
	at sun.nio.ch.Net.connect(Net.java:465)
	at sun.nio.ch.Net.connect(Net.java:457)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:670)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:713)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1524)
	at org.apache.hadoop.ipc.Client.call(Client.java:1447)
	... 16 more
17/03/18 12:06:14 INFO hdfs.BucketWriter: Creating /user/cloudera/output/handson_train/flume/techJobTweet/2017/03/18/12/job_tweet.1489863915981.txt.tmp
17/03/18 12:06:14 INFO lifecycle.LifecycleSupervisor: Stopping lifecycle supervisor 10
17/03/18 12:06:14 WARN hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1243578604_29] for 73 seconds.  Will retry shortly ...
java.io.IOException: Failed on local exception: java.net.SocketException: Network is unreachable; Host Details : local host is: "quickstart.cloudera/10.0.2.15"; destination host is: "quickstart.cloudera":8020; 
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:772)
	at org.apache.hadoop.ipc.Client.call(Client.java:1475)
	at org.apache.hadoop.ipc.Client.call(Client.java:1408)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:230)
	at com.sun.proxy.$Proxy16.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease(ClientNamenodeProtocolTranslatorPB.java:581)
	at sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:256)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:104)
	at com.sun.proxy.$Proxy17.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:922)
	at org.apache.hadoop.hdfs.LeaseRenewer.renew(LeaseRenewer.java:423)
	at org.apache.hadoop.hdfs.LeaseRenewer.run(LeaseRenewer.java:448)
	at org.apache.hadoop.hdfs.LeaseRenewer.access$700(LeaseRenewer.java:71)
	at org.apache.hadoop.hdfs.LeaseRenewer$1.run(LeaseRenewer.java:304)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.SocketException: Network is unreachable
	at sun.nio.ch.Net.connect0(Native Method)
	at sun.nio.ch.Net.connect(Net.java:465)
	at sun.nio.ch.Net.connect(Net.java:457)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:670)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:713)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1524)
	at org.apache.hadoop.ipc.Client.call(Client.java:1447)
	... 16 more
17/03/18 12:06:14 WARN hdfs.BucketWriter: Unexpected Exception null
java.lang.InterruptedException
	at java.util.concurrent.FutureTask.awaitDone(FutureTask.java:400)
	at java.util.concurrent.FutureTask.get(FutureTask.java:199)
	at org.apache.flume.sink.hdfs.BucketWriter.callWithTimeout(BucketWriter.java:686)
	at org.apache.flume.sink.hdfs.BucketWriter.open(BucketWriter.java:235)
	at org.apache.flume.sink.hdfs.BucketWriter.append(BucketWriter.java:514)
	at org.apache.flume.sink.hdfs.HDFSEventSink.process(HDFSEventSink.java:418)
	at org.apache.flume.sink.DefaultSinkProcessor.process(DefaultSinkProcessor.java:68)
	at org.apache.flume.SinkRunner$PollingRunner.run(SinkRunner.java:147)
	at java.lang.Thread.run(Thread.java:745)
17/03/18 12:06:14 ERROR hdfs.HDFSEventSink: process failed
java.lang.RuntimeException: java.lang.InterruptedException
	at com.google.common.base.Throwables.propagate(Throwables.java:156)
	at org.apache.flume.sink.hdfs.BucketWriter.open(BucketWriter.java:264)
	at org.apache.flume.sink.hdfs.BucketWriter.append(BucketWriter.java:514)
	at org.apache.flume.sink.hdfs.HDFSEventSink.process(HDFSEventSink.java:418)
	at org.apache.flume.sink.DefaultSinkProcessor.process(DefaultSinkProcessor.java:68)
	at org.apache.flume.SinkRunner$PollingRunner.run(SinkRunner.java:147)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.InterruptedException
	at java.util.concurrent.FutureTask.awaitDone(FutureTask.java:400)
	at java.util.concurrent.FutureTask.get(FutureTask.java:199)
	at org.apache.flume.sink.hdfs.BucketWriter.callWithTimeout(BucketWriter.java:686)
	at org.apache.flume.sink.hdfs.BucketWriter.open(BucketWriter.java:235)
	... 5 more
17/03/18 12:06:14 ERROR flume.SinkRunner: Unable to deliver event. Exception follows.
org.apache.flume.EventDeliveryException: java.lang.RuntimeException: java.lang.InterruptedException
	at org.apache.flume.sink.hdfs.HDFSEventSink.process(HDFSEventSink.java:463)
	at org.apache.flume.sink.DefaultSinkProcessor.process(DefaultSinkProcessor.java:68)
	at org.apache.flume.SinkRunner$PollingRunner.run(SinkRunner.java:147)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.RuntimeException: java.lang.InterruptedException
	at com.google.common.base.Throwables.propagate(Throwables.java:156)
	at org.apache.flume.sink.hdfs.BucketWriter.open(BucketWriter.java:264)
	at org.apache.flume.sink.hdfs.BucketWriter.append(BucketWriter.java:514)
	at org.apache.flume.sink.hdfs.HDFSEventSink.process(HDFSEventSink.java:418)
	... 3 more
Caused by: java.lang.InterruptedException
	at java.util.concurrent.FutureTask.awaitDone(FutureTask.java:400)
	at java.util.concurrent.FutureTask.get(FutureTask.java:199)
	at org.apache.flume.sink.hdfs.BucketWriter.callWithTimeout(BucketWriter.java:686)
	at org.apache.flume.sink.hdfs.BucketWriter.open(BucketWriter.java:235)
	... 5 more
17/03/18 12:06:15 WARN hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1243578604_29] for 74 seconds.  Will retry shortly ...
java.io.IOException: Failed on local exception: java.net.SocketException: Network is unreachable; Host Details : local host is: "quickstart.cloudera/10.0.2.15"; destination host is: "quickstart.cloudera":8020; 
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:772)
	at org.apache.hadoop.ipc.Client.call(Client.java:1475)
	at org.apache.hadoop.ipc.Client.call(Client.java:1408)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:230)
	at com.sun.proxy.$Proxy16.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease(ClientNamenodeProtocolTranslatorPB.java:581)
	at sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:256)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:104)
	at com.sun.proxy.$Proxy17.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:922)
	at org.apache.hadoop.hdfs.LeaseRenewer.renew(LeaseRenewer.java:423)
	at org.apache.hadoop.hdfs.LeaseRenewer.run(LeaseRenewer.java:448)
	at org.apache.hadoop.hdfs.LeaseRenewer.access$700(LeaseRenewer.java:71)
	at org.apache.hadoop.hdfs.LeaseRenewer$1.run(LeaseRenewer.java:304)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.SocketException: Network is unreachable
	at sun.nio.ch.Net.connect0(Native Method)
	at sun.nio.ch.Net.connect(Net.java:465)
	at sun.nio.ch.Net.connect(Net.java:457)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:670)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:713)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1524)
	at org.apache.hadoop.ipc.Client.call(Client.java:1447)
	... 16 more
